['validator.py', '../annotated_dataset/Post-1.ann', '../tokenizer/Post-1.ann', '../annotated_dataset/Post-2.ann', '../tokenizer/Post-2.ann', '../annotated_dataset/Post-3.ann', '../tokenizer/Post-3.ann', '../annotated_dataset/Post-4.ann', '../tokenizer/Post-4.ann']
------ Model performance on Post 1-------
Total ground truth tokens: 2815
Total model tokens: 2861
--------------------------------
True positive: 2764 (96.61%)
False positive: 61 (2.13%)
False negative: 36 (1.26%)
--------------------------------
Precision: 0.978
Recall: 0.987
F1 score: 0.983
--------------------------------

------ Model performance on Post 2-------
Total ground truth tokens: 2283
Total model tokens: 2274
--------------------------------
True positive: 2250 (98.94%)
False positive: 4 (0.18%)
False negative: 20 (0.88%)
--------------------------------
Precision: 0.998
Recall: 0.991
F1 score: 0.995
--------------------------------

------ Model performance on Post 3-------
Total ground truth tokens: 1878
Total model tokens: 1892
--------------------------------
True positive: 1845 (97.52%)
False positive: 25 (1.32%)
False negative: 22 (1.16%)
--------------------------------
Precision: 0.987
Recall: 0.988
F1 score: 0.987
--------------------------------

------ Model performance on Post 4-------
Total ground truth tokens: 2491
Total model tokens: 2482
--------------------------------
True positive: 2412 (97.18%)
False positive: 27 (1.09%)
False negative: 43 (1.73%)
--------------------------------
Precision: 0.989
Recall: 0.982
F1 score: 0.986
--------------------------------

------ Overall model performance %d-------
Total ground truth tokens: 9467
Total model tokens: 9509
--------------------------------
True positive: 9271 (97.50%)
False positive: 117 (1.23%)
False negative: 121 (1.27%)
--------------------------------
Precision: 0.988
Recall: 0.987
F1 score: 0.987
--------------------------------
