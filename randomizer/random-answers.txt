Answer-133, Question-123
This may be too basic or limited of a solution, but couldn't you do a String.split() on each line of the file, remembering the result array of the first line to generate the XML, and just spit each line's array data out with the proper XML elements padding each iteration of a loop?

Answer-183, Question-123
Maybe this might help: JSefa You can read CSV file with this tool and serialize it to XML.

Answer-190, Question-123
I don't understand why you would want to do this. It sounds almost like cargo cult coding. Converting a CSV file to XML doesn't add any value. Your program is already reading the CSV file, so arguing that you need XML doesn't work. On the other hand, reading the CSV file, doing something with the values, and then serializing to XML does make sense (well, as much as using XML can make sense... ;)) but you would supposedly already have a means of serializing to XML.

Answer-540, Question-123
As far as I know, there's no ready-made library to do this for you, but producing a tool capable of translating from CSV to XML should only require you to write a crude CSV parser and hook up JDOM (or your XML Java library of choice) with some glue code.

Answer-939, Question-123
There is nothing I know of that can do this without you at least writing a little bit of code... You will need 2 separate library: A CSV Parser Framework  An XML Serialization Framework The CSV parser I would recommend (unless you want to have a little bit of fun to write your own CSV Parser) is OpenCSV (A SourceForge Project for parsing CSV Data) The XML Serialization Framework should be something that can scale in case you want to transform large (or huge) CSV file to XML: My recommendation is the Sun Java Streaming XML Parser Framework (See here) which allows pull-parsing AND serialization.

Answer-6726, Question-123
I know you asked for Java, but this strikes me as a task well suited to a scripting language. Here is a quick (very simple) solution written in Groovy. test.csv csvtoxml.groovy Writes the following XML to stdout: However, the code does very simple parsing (not taking into account quoted or escaped commas) and it does not account for possible absent data.

Answer-21413, Question-123
This solution does not need any CSV or XML libraries and, I know, it does not handle any illegal characters and encoding issues, but you might be interested in it as well, provided your CSV input does not break the above mentioned rules. Attention: You should not use this code unless you know what you do or don't have the chance to use a further library (possible in some bureaucratic projects)... Use a StringBuffer for older Runtime Environments... So here we go: The input test.csv (stolen from another answer on this page): The resulting output:

Answer-53547, Question-123
As the others above, I don't know any one-step way to do that, but if you are ready to use very simple external libraries, I would suggest: OpenCsv for parsing CSV (small, simple, reliable and easy to use) Xstream to parse/serialize XML (very very easy to use, and creating fully human readable xml) Using the same sample data as above, code would look like: Producing the following result: (Xstream allows very fine tuning of the result...)

Answer-74018, Question-123
For the CSV Part, you may use my little open source library

Answer-144768, Question-123
I have an opensource framework for working with CSV and flat files in general. Maybe it's worth looking: JFileHelpers. With that toolkit you can write code using beans, like: and then just parse your text files using: And you'll have a collection of parsed objects. Hope that helps!

Answer-155146, Question-123
There is also good library ServingXML by Daniel Parker, which is able to convert almost any plain text format to XML and back. The example for your case can be found here: It uses heading of field in CSV file as the XML element name.

Answer-161058, Question-123
You can do this exceptionally easily using Groovy, and the code is very readable.  Basically, the text variable will be written to contacts.xml for each line in the contactData.csv, and the fields array contains each column.

Answer-208799, Question-123
You could use XSLT.  Google it and you will find a few examples e.g. CSV to XML If you use XSLT you can then convert the XML to whatever format you want.

Answer-3455575, Question-123
The big difference is that JSefa brings in is that it can serialize your java objects to  CSV/XML/etc files and can deserialize back to java objects. And it's driven by annotations which gives you lot of control over the output.  JFileHelpers also looks interesting.

Answer-23096745, Question-123
I had the same problem and needed an application to convert a CSV file to a XML file for one of my projects, but didn't find anything free and good enough on the net, so I coded my own Java Swing CSVtoXML application. It's available from my website HERE. Hope it will help you.  If not, you can easily code your own like I did; The source code is inside the jar file so modify it as you need if it doesn't fill your requirement.

Answer-29953381, Question-123
Jackson processor family has backends for multiple data formats, not just JSON. This includes both XML (https://github.com/FasterXML/jackson-dataformat-xml) and CSV (https://github.com/FasterXML/jackson-dataformat-csv/) backends. Conversion would rely on reading input with CSV backend, write using XML backend. This is easiest to do if you have (or can define) a POJO for per-row (CSV) entries. This is not a strict requirement, as content from CSV may be read "untyped" as well (a sequence of String arrays), but requires bit more work on XML output. For XML side, you would need a wrapper root object to contain array or List of objects to serialize.

Answer-384, Question-382
This warning is there because Java is not actually storing type information at run-time in an object that uses generics.  Thus, if object is actually a List<String>, there will be no ClassCastException at run-time except until an item is accessed from the list that doesn't match the generic type defined in the variable.   This can cause further complications if items are added to the list, with this incorrect generic type information.  Any code still holding a reference to the list but with the correct generic type information will now have an inconsistent list.   To remove the warning, try: However, note that you will not be able to use certain methods such as add because the compiler doesn't know if you are trying to add an object of incorrect type.  The above will work in a lot of situations, but if you have to use add, or some similarly restricted method, you will just have to suffer the yellow underline in Eclipse (or a SuppressWarning annotation).

Answer-8307, Question-2931
The META-INF.services stuff is known by its class name in the API: ServiceLoader. A Google search for ServiceLoader yields some information. I am not really familiar with it, but sometimes it's all about knowing the right search keywords.

Answer-6356, Question-6222
I've been successfully using it on Vista x64 for some light Java work.  Nothing too involved and no extra plugins, but basic Java coding has been working without any issues.  I'm using the 3.4M7 build but it looks like the 3.4 stable build supports Vista x64 now.

Answer-7877, Question-6222
I'm using Eclipse with a 64bit VM. However I have to use Java 1.5, because with Java 1.6, even 1.6.0_10ea, Eclipse crashed when changing the .classpath-file. On Linux I had the same problems and could only get the 64bit Eclipse to work with 64bit Java 1.5. The problem seems to be with the just in time compilation, since with vmparam -Xint eclipse works -- but this is not a sollution, because it's slow then. Edit: With 1.6.0_11 it seems to work.  1.6_10 final might work as well as mentioned in the comment, but I've not tested that.

Answer-7717, Question-7269
If I understand you correctly, the idea is to get a different "singleton" object for each caller object or "context". One thing you can do is to create a thread-local global variable where you write the ID of the current context. (This can be done with AOP.) Then in the singleton getter, the context ID is fetched from the thread-local to use as a key to the correct "singleton" instance for the calling context.  Regarding AOP there should be no problem using it in applets since, depending on your point-cuts, the advices are woven at compile time and a JAR is added to the runtime dependencies. Hence, no special evidence of AOP should remain at run time.

Answer-8163, Question-7269
@Hugo regarding threadlocal: I thought about that solution. However, from experiments I found two problems with that approach: Shared thread (server connections, etc) are problematic. This can be solved though by paying special attention to these thread (they're all under my control and are pretty much isolated from the legacy code). The EDT thread is shared across all applets. I failed to find a way to force the creation of a new EDT thread for each applet. This means that the threadlocal for the EDT would be shared across the applets. This one I have no idea how to solve. Suggestions?

Answer-46902, Question-7269
Singletons are evil, what do you expect? ;) Perhaps the most comprehensive approach would be to load the bulk of the applet in a different class loader (use java.net.URLClassLoader.newInstance). Then use a WeakHashMap to associate class loader with an applet. If you could split most of the code into a common class loader (as a parent of each per-applet class loader) and into the normal applet codebase, that would be faster but more work. Other hacks: If you have access to any component, you can use Component.getParent repeatedly or SwingUtilities.getRoot. If you are in a per-applet instance thread, then you can set up a ThreadLocal. From the EDT, you can read the current event from the queue (java.awt.EventQueue.getCurrentEvent()), and possibly find a component from that. Alternatively push an EventQueue with a overridden dispatchEvent method.

Answer-7747, Question-7720
Install4J. Not free, but worth it. Give the trial a shot

Answer-7759, Question-7720
I went through the same and found that all of the free options weren't very good. Looks like you'll be writing your own. I'd be interested to see if someone has a free/cheap option that works

Answer-7796, Question-7720
Have you thought about Java Web Start?  Here is a tutorial specifically for deploying an SWT application with Java Web Start.

Answer-8598, Question-7720
Maybe you should take a look at IzPack. I created a very nice installer some years ago and I'd bet that they are still improving it. It allows the installation of docs, binaries and a clickable link to start the application IIRC.

Answer-8602, Question-7720
Have you considered writing a small program in C/C++ that just calls CreateProcess to start up the java VM with the jar (or class) file? You could get Visual C++ Express and put together the startup program pretty easily.  This would make it easy to add a friendly icon as well.

Answer-8620, Question-7720
Another option I was considering: rather than writing a native launcher from scratch, Eclipse comes with the source code for its own launcher, and this could perhaps be repurposed for my app. It's a shame that Sun never included anything similar in the JDK.

Answer-26942, Question-7720
I've used the free Launch4J to create a custom launcher for my Java programs on Windows. Combined with the free NSIS Installer you can build a nice package for your Windows users. Edit: Did not see that you use SWT. Don't know if it works with SWT as well, because I used only Swing in my apps.

Answer-74597, Question-7720
In my company we use Launch4J to create the exe file, and NSIS to create the installer, with SWT applications.  We have used it for years in several commercial applications and the pair works fine.

Answer-75908, Question-7720
Another vote for Launch4J, just wrote an ant task this morning to integrate with one of my projects.  Seems to work really well

Answer-76548, Question-7720
Consider converting your application to Eclipse RCP.  It is written in SWT, and the Eclipse IDE contains packaging tools that generate executables for all major platforms.  For windows, it can generate a zip or a folder containing your code.  For a common installation experience, I'd using NSIS.  There is actually a packages generator project at eclipse to create common installers for all platforms eclipse supports.

Answer-149971, Question-7720
To follow up on pauxu's answer, I'm using launch4j and NSIS on a project of mine and thought it would be helpful to show just how I'm using them.  Here's what I'm doing for Windows.  BTW, I'm creating .app and .dmg for Mac, but haven't figured out what to do for Linux yet. Project Copies of launch4j and NSIS In my project I have a "vendor" directory and underneath it I have a directory for "launch4j" and "nsis".  Within each is a copy of the install for each application.  I find it easier to have a copy local to the project rather than forcing others to install both products and set up some kind of environment variable to point to each. Script Files I also have a "scripts" directory in my project that holds various configuration/script files for my project.  First there is the launch4j.xml file: And then there's the NSIS script rpgam-setup.nsis.  It can take a VERSION argument to help name the file. Ant Integration I have some targets in my Ant buildfile (build.xml) to handle the above.  First I tel Ant to import launch4j's Ant tasks: I then have a simple target for creating the wrapper executable: And another target for making the installer: The top portion of that just copies the necessary files for the installer to a temporary location and the second half executes the script that uses all of it to make the installer.

Answer-242507, Question-7720
I have used JSmooth in the past, and still have luck with it.  The UI is pretty buggy, but I only use that for building the config file once, and then I build from Ant after that. What issues are you having with JSmooth?

Answer-438536, Question-7720
JSMooth has worked very well for us in a production environment, where I first generated a single jar using one-jar (fat jar plugin to eclipse) and then wrapped it with JSmooth. (Please note that I wanted a no-install distribution of a single file, which could promt for installing the JRE if needed). It has worked so well that I thought nobody was using it :)

Answer-455074, Question-7720
You may want to try our tool, BitRock InstallBuilder. Although it is a native application, a lot of our customers use it to package desktop Java applications. If you bundle the JRE and create launcher, etc. the user does not even need to know they are installing a Java application. It is cross platform, so you can generate installers for both Windows and Mac (and Linux, Solaris, etc.) Like install4j tool mentioned in another post, it is a commercial tool, but we have free licenses for open source projects and special discounts for microISVs / small business, etc. just drop us an email.  Also wanted to emphasize that this is an installer tool, so it will not address your needs if you are looking only for a single file executable.

Answer-1214727, Question-7720
In my company we use launch4J and NSIS for the windows distribution, and jdeb for the Debian distribution, and Java Web Start for the general operating system. This works quite fine.

Answer-9884607, Question-7720
Please try InstallJammer.The best one I have used ever. Free and powerful.And sufficient for personal and commercial use.

Answer-23582843, Question-7720
You can now do this through Netbeans! It's really easy and works perfectly. Check out this tutorial on the Netbeans website.

Answer-25479992, Question-7720
Have you considered Advanced Installer? I have used it severally especially for Windows and Mac. No scripting or Ant required. All GUI. Very simple and understandable. Ain't free but worth every penny. - Lauch as Administrator - File Association - Custom Install Themes + In built Themes - Package with JRE - Install location - Native Splash screen implementation - You can event create services and installation events - Prerequisites - JRE minimum version and maximum version And a lot more. And don't get it twisted, i have no connections with the dudes...their App is just awesome.

Answer-9404, Question-8968
If you only need some simple process orchestration, Spring's own Web Flow, despite its name can serve as a orchestration task manager.   If you need to preserve state for several days then you will need to become an 'early adopter' of one of the open-source projects.  You may want to look at Eclipse's BPEL project.   My hunch is that once a clearer picture of the BPEL/BPM/Workflow space emerges you will see Spring provide an abstraction layer the same way they have for JDBC, Transactions, ORM frameworks etc...

Answer-9698, Question-8968
We're looking at Drools/Guvnor, possibly integrated with jBPM (as in this presentation), to add a workflow engine to our Spring/Java EE app, but we're still in the very early phases of trying it out.

Answer-17820, Question-8968
I second Spring Web Flow. Depending on how complex the process is, Web Flow is great for managing various states and I've found that it's pretty easy to pick up and there's a good amount of documentation out there for it.

Answer-182346, Question-8968
Like Brian said if you're doing anything of great complexity you might look at using BPEL.  There are a number of open source BPEL engines, one that comes to mind is Apache Orchestration Director Engine

Answer-427596, Question-8968
ActiveVOS is by far the best BPEL engine in my opinion. Download the evaluation version and give it a go. JBoss have even adopted their open source offering.

Answer-9137, Question-9044
I'm not sure how the annotation inheritance works for TestNG but this article may be of some use. http://beust.com/weblog/archives/000170.html.  Actually, this may help better http://testng.org/doc/documentation-main.html#annotations, look at inheritGroups.

Answer-75209, Question-9044
TestNG will run all the public methods from a class with a @Test annotation. Maybe you could change the methods you don't want TestNG to run to be non-public

Answer-154080, Question-9044
You can specify the @Test annotation at method level that allows for maximum flexibility. Does this works for you or I am missing something from your question.

Answer-275342, Question-9044
It would seem to me as the following code-challenge (community wiki post): How to be able to execute all test methods of Extended class from the group 'aGlobalGroup' without: specifying the 'aGlobalGroup' group on the Extended class itself ? testing non-annotated public methods of Extended class ? The first answer is easy: add a class TestNG(groups = { "aGlobalGroup" }) on the Base class level That group will apply to all public methods of both Base class and Extended class. BUT: even non-testng public methods (with no TestNG annotation) will be included in that group. CHALLENGE: avoid including those non-TestNG methods.

Answer-275353, Question-9044
The answer is through a custom org.testng.IMethodSelector: Its includeMethod() can exclude any method we want, like a public not-annotated method. However, to register a custom Java MethodSelector, you must add it to the XMLTest instance managed by any TestRunner, which means you need your own custom TestRunner. But, to build a custom TestRunner, you need to register a TestRunnerFactory, through the -testrunfactory option. BUT that -testrunfactory is NEVER taken into account by TestNG class... so you need also to define a custom TestNG class : in order to override the configure(Map) method,  so you can actually set the TestRunnerFactory TestRunnerFactory which will build you a custom TestRunner, TestRunner which will set to the XMLTest instance a custom XMLMethodSelector XMLMethodSelector which will build a custom IMethodSelector IMethodSelector which will exclude any TestNG methods of your choosing! Ok... it's a nightmare. But it is also a code-challenge, so it must be a little challenging ;) All the code is available at DZone snippets. As usual for a code challenge: one java class (and quite a few inner classes) copy-paste the class in a 'source/test' directory (since the package is 'test') run it (no arguments needed) Update from Mike Stone: I'm going to accept this because it sounds pretty close to what I ended up doing, but I figured I would add what I did as well. Basically, I created a Groups annotation that behaves like the groups property of the Test (and other) annotations. Then, I created a GroupsAnnotationTransformer, which uses IAnnotationTransformer to look at all tests and test classes being defined, then modifies the test to add the groups, which works perfectly with group exclusion and inclusion. Modify the build to use the new annotation transformer, and it all works perfectly! Well... the one caveat is that it doesn't add the groups to non-test methods... because at the time I did this, there was another annotation transformer that lets you transform ANYTHING, but it somehow wasn't included in the TestNG I was using for some reason... so it is a good idea to make your before/after annotated methods to alwaysRun=true... which is sufficient for me. The end result is I can do: And I made the transformer work with subclassing and everything.

Answer-16650, Question-9361
I believe that the real answer is that you can't.  The file path won't be sent by the browser for security reasons.  The file name will be sent, however I don't believe it gets sent without an actual upload. The closest you could come, afaik, would be to forcibly kill the connection just when the upload starts.  That would net you the filename with little actual transferred data, but it doesn't sound like it would be useful to you. Alternatively, a signed Java applet might get you closer to a solution that you'd want.

Answer-11399, Question-11341
I'm not familiar with WebObjects, but I see you have java listed in there. iText is a java api for building pdfs. If you can access a java api from WebObjects you should be able to build pdfs that way.

Answer-13423, Question-11341
ScArcher2>> I have looked into different routes for creating PDFs on the fly including FOP and a few Java libraries. I think what I am really asking is if anyone has already done this in the WebObjects framework. My hope is that someone familiar with WebObjects might have done this already and have some insight that would save me some time.

Answer-13431, Question-11341
The canonical response when asked about PDFs from WebObjects has generally been ReportMill.    It's a PDF document generating framework that works a lot like WebObjects, and includes its own graphical PDF builder tool similar to WebObjects Builder and Interface Builder.  You can bind elements in your generated PDFs to dynamic data in your application just as you would for a WOComponent. They have couple of tutorial videos on the ReportMill product page that should give you an idea of how the tool works.  It'll probably be a lot easier than trying to work with FOP programmatically.

Answer-1289373, Question-11341
You can use ReportMill or Jasper Reports. Compared with ReportMill Jasper Reports is Free but requires learning huge library. You can use IReport or Jasper Assistant eclipse plugin(If you are using WOLips) for building report templates. My experiance both are good.

Answer-2867327, Question-11341
ERPDFWrapper component in Project Wonder: http://webobjects.mdimension.com/hudson/job/Wonder/javadoc/er/pdf/ERPDFWrapper.html

Answer-3705685, Question-11341
Jasper Reports support have been added to Project Wonder a week ago : http://www.wocommunity.org/podcasts/wowodc/2010/IntegratingJasperReports.mov A talk about that new framework was done at WOWODC 2010, and it was recorded. Check wocommunity.org and the mailing list about that in October.

Answer-13239, Question-13225
Perhaps:

Answer-13250, Question-13225
One approach that comes to mind is that you could store the translated replacement parameters i.e. "address" and "contact information" in a separate properties file, one per locale. Then have your Action class (or probably some helper class) look up the values from the correct ResourceBundle for the current locale and pass them to the message tag.

Answer-13359, Question-13225
 The message message tag API allows   only 5 parametric arguments Ah! I blame my complete ignorance of the Struts API. To quote the manual:  Some of the features in this taglib   are also available in the JavaServer   Pages Standard Tag Library (JSTL). The   Struts team encourages the use of the   standard tags over the Struts specific   tags when possible. You could probably do this with the http://java.sun.com/jsp/jstl/fmt taglib. The downside is that this isn't valid XML and yanking the values to variables involves more indirection, lookups and verbosity. This is not a good solution. I don't know Struts, but if it is anything like JavaServer Faces (same architect), then there is probably support for configuring a replacement control. I would either replace the existing control with a more flexible one or add a new one. Anytime I receive newly-translated   text, I must decide what to surround   with the <a>...</a> markup. There is no way you should be doing this and I see this as a fault in your translation process (I am an ex-localization engineer and ex-developer of localization tools). The {0} characters should be included in the files that are sent to the translators. The localization guidelines should explain the string's context and the meaning of any variables. You can programmatically validate the property bundles on return. String-specific regex's might do the trick. It isn't outside the realms of possibility that "address" and "contact information" would swap order during translation. The simplest solution is to redesign the messages to render: I accept that this might not be a solution in all cases and may have your UI designer spitting teeth.

Answer-13381, Question-13225
 Avoid creating links within long   blocks of text. Prefer shorter text   that can act as a logically complete   and independent link. Generally, it will lead to fewer problems. Sometimes you have to compromise your UI design to accommodate localization; sometimes you need to compromise your localization process to accommodate the UI. Any time a developer manually manipulates post-translation strings is a source of potentially expensive bugs. Cutting/pasting or string editing can result in character corruption, misplaced strings, etc. A translation defect needs the participation of outside parties to fix which involves cost and takes time. Thinking on it, something like this might be less ugly: ...but I'm no UI designer.

Answer-17779, Question-16487
The Maven SourceForge plug-in does not work with Maven 2. Also I believe this plug-in uses FTP which is no longer supported.

Answer-47220, Question-16487
It looks like I am going to have to write this myself. https://sourceforge.net/projects/wagon-sf/

Answer-187030, Question-16487
I found that CruiseControl can upload releases to SFEE and also works with Maven and Maven2

Answer-1449595, Question-16487
I'm not able to test this to confirm, but I believe it is possible without writing any plugins. You can deploy to SourceForge using SCP, and the maven-deploy-plugin can be configured to use SCP so it should work. You can also deploy your site to SourceForge via SCP. You would configure the SourceForge server in your settings.xml to use a "combined" username with a comma separator. With these credentials: The server element would look like this: And the distributionManagement section in your POM would look like this: Finally declare that ssh-external is to be used: If this doesn't work, you may be able to use the recommended approach in the site reference above, i.e. create a shell on shell.sourceforge.net with your username and project group: Then use shell.sourceforge.net (instead of web.sourceforge.net) in your site URL in the diestributionManagement section:

Answer-2187533, Question-16487
After trying this a number of times, I finally got it to work -- with sftp not scp.  This should work from a unix box (or Mac) -- I'm not sure about sftp clients for Windoze.  I am using mvn version 2.2.0 and I don't think I have any special plugins installed.  This deploys the various mvn packages to the Files section of my project page. You'll need to change the following in your settings to get it to work: user -- replace with your sourceforce username secret -- replace with your password ormlite -- replace with your project name /o/or/ -- replace with the first char and first 2 chars of your project name In my $HOME/.m2/settings.xml file I have the following for the SF server: I don't specify the username in the settings.xml file because it needs to be username,project and I want to deploy multiple packages to SF.  Then, in my pom.xml file for the ormlite package I have the following: Obviously the /releases and /snapshots directory suffixes can be changed depending on your file hierarchy.

Answer-2330852, Question-16487
Where timp = user and webmacro = project scp url does not work: sftp url works:  or for project release artifacts:  scp will work to shell.sourceforge.net, but you have to create the shell before use with

Answer-2337482, Question-16487
I have uploaded an example to sourceforge.net at: http://sf-mvn-plugins.sourceforge.net/example-1jar-thinlet/ You can check out it via svn - so you can see how to use plugins for upload and download of and to sourceforge.net file system area and web site. The main points to upload are to use sftp: Add this similar code to your pom.xml Add similar code to settings.xml The main point for download is to use the wagon-http-sourceforge maven plugin - please see at: sf-mvn-plugins.sourceforge.net/wagon-http-sourceforge/FAQ.html Please add the following code to your pom.xml

Answer-4110138, Question-16487
This really did not turn out to be that hard. First up I had the mvn site:deploy working following the instructions at this sourceforge site. Basically you start the sourceforge shell with  That will create the shell at their end with a folder mounted to your project on a path such as (depending on your projects name): In that shell I on the sourceforge server I created a folder for my repo under the project apache folder "htdocs" with  In my settings.xml I set the username and password to that shell server so that maven can login:  In the pom.xml you just need your distibutionManagement section setup to name the server by ID that you set the password for in your settings file:  There the repository entry is the one for the mvn deploy command and the site entry is for the mvn site:deploy command. Then all I have to do is start the shell connection to bring up the server side then on my local side just run:  which uploads the jar, pom and sources and the like onto my sourceforge projects website. If you try to hit the /maven2 folder on your project website sourceforge kindly tell you that directory listing is off by default and how to fix it. To do this on the server shell you create a .htaccess file in your htdocs/maven2 folder containing the following apache options  Then bingo, you have a maven repo which looks like:  http://chex4j.sourceforge.net/maven2/net/sf/chex4j/chex4j-core/1.0.0/ Your sf.net shell it shuts down after a number of hours to not hog resources; so you run the "ssh -t ... create" when you want to deploy the site or your build artifacts.  You can browse all my maven project code under sourceforge to see my working settings:  http://chex4j.svn.sourceforge.net/viewvc/chex4j/branches/1.0.x/chex4j-core/

Answer-4402466, Question-16487
SCP URL does work. But do not use ":" after server name. MVN tries to read the following test as integer (port number). You do not need to establish tunnels as simbo1905 did.

Answer-20154, Question-19838
The concept of closures requires the concept of contexts. C's context is based on the stack and the registers of the CPU, so to create a block/closure, you need to be able to manipulate the stack pointer in a correct (and reentrant) way, and store/restore registers as needed. The way this is done by interpreters or virtual machines is to have a context structure or something similar, and not use the stack and registers directly. This structure keeps track of a stack and optionally some registers, if you're designing a register based VM. At least, that's the simplest way to do it (though slightly less performant than actually mapping things correctly).

Answer-22164, Question-19838
There's a good set of slides on Ruby Blocks as part of the "Rails with Passion" course: Ruby_Blocks.pdf This covers representing a block, how they get passed arguments and executed, and even further into things like Proc objects.  It's very clearly explained. It might then be of interest to look at how the JRuby guys handled these in their parsing to Java.  Take a look at the source at codehaus.

Answer-38105, Question-19838
I haven't actually implemented any of this, so take it with a sack of salt. There are two parts to a closure: the data environment and the code environment. Like you said, you can probably pass a void* to handle references to data. You could probably use setjmp and longjmp to implement the non-linear control flow jumps that the Ruby break requires.  If you want closures you should probably be programming in a language that actually supports them. :-) UPDATE: Interesting things are happening in Clang. They've prototyped a closure for C. http://lists.cs.uiuc.edu/pipermail/cfe-dev/2008-August/002670.html might prove to be interesting reading.

Answer-23121, Question-23106
You will want to use org.xml.sax.XMLReader (http://docs.oracle.com/javase/7/docs/api/org/xml/sax/XMLReader.html).

Answer-23300, Question-23106
I would use Stax to parse XML, it's fast and easy to use. I've been using it on my last project to parse XML files up to 24MB. There's a nice introduction on java.net, which tells you everything you need to know to get started.

Answer-4377373, Question-23106
Basically, you have two main XML parsing methods in Java : SAX, where you use an handler to only grab what you want in your XML and ditch the rest DOM, which parses your file all along, and allows you to grab all elements in a more tree-like fashion. Another very useful XML parsing method, albeit a little more recent than these ones, and included in the JRE only since Java6, is StAX. StAX was conceived as a medial method between the tree-based of DOM and event-based approach of SAX. It is quite similar to SAX in the fact that parsing very large documents is easy, but in this case the application "pulls" info from the parser, instead of the parsing "pushing" events to the application. You can find more explanation on this subject here. So, depending on what you want to achieve, you can use one of these approaches.

Answer-5299126, Question-23106
If you only need to parse then I would recommend using XPath library. Here is a nice reference: http://www.ibm.com/developerworks/library/x-javaxpathapi.html But you may want to consider turning XMLs to objects and then the sky is the limit.  For that you may use XStream, this is a great library which i use alot

Answer-5299265, Question-23106
Use the dom4j library First read the document Then use XPATH to get to the values you need

Answer-36190454, Question-23106
Below is the code of extracting some value value using vtd-xml.

Answer-23795, Question-23763
Let Y = 0.3*R + 0.59*G + 0.11*B for each pixel in the image, then set them to be ((R1+Y)/2,(G1+Y)/2,(B1+Y)/2) if (R1,G1,B1) is what you are colorizing with.

Answer-27185, Question-23763
I have never used GIMP's colorize command. However, if your getting the RGB value of each pixel and adding RGB value to it you should really use a LookupOp. Here is some code that I wrote to apply a BufferedImageOp to a BufferedImage. Using Nicks example from above heres how I would do it. Let Y = 0.3*R + 0.59*G + 0.11*B for   each pixel (R1,G1,B1) is what you are colorizing   with It creates a BufferedImageOp that will mask out each color if the mask boolean is true. Its simple to call too. If this is not what your looking for I suggest you look more into BufferedImageOp's. This is would also be more efficient since you would not need to do the calculations multiple times on different images. Or do the calculations over again on different BufferedImages as long as the R1,G1,B1 values don't change.

Answer-4720882, Question-23763
I wanted to do the exact same thing as the question poster wanted to do but the above conversion did not remove colors like the GIMP does (ie green with a red overlay made an unpleasant brown color etc). So I downloaded the source code for GIMP and converted the c code over to Java. Posting it in this thread just in case anyone else wants to do the same (since it is the first thread that comes up in Google). The conversion still changes the white color when it should not, it's probably a casting issue from double to int. The class converts a BufferedImage in-place.

Answer-20411373, Question-23763
This works exactly like the Colorize function in GIMP and it preserves the transparency. I've also added a few things like Contrast and Brightness, Hue, Sat, and Luminosity - 0circle0 Google Me --> ' Sprite Creator 3'

Answer-23879, Question-23853
You can mark variables as "silent" like this: If $variable is null, nothing will be rendered. If it is not null, its value will render as it normally would.

Answer-63970, Question-23853
You will also need to be sure and use the proper syntax.  Your example is missing the dollar before the variable.  It should be $!{person.age}, not just {person.age}.

Answer-24741, Question-24723
The regex shown in your example, should work regardless of language. So is it the regex you want, or the java code to put this logic around the regex?

Answer-27649, Question-24723
The Open Web Application Security Project (OWASP) have a few suggestion for sanitizing your input. See for instance: Java Table of Contents How to perform HTML entity encoding in Java How to add validation logic to HttpServletRequest

Answer-27705, Question-24723
The biggest problem by using jeffs code is the @ which currently isnt available.  I would probably just take the "raw" regexp from jeffs code if i needed it and paste it into  http://www.cis.upenn.edu/~matuszek/General/RegexTester/regex-tester.html  and see the things needing escape get escaped and then use it. Taking the usage of this regex in mind I would personally make sure I understood exactly what I was doing, why and what consequences would be if I didnt succeed, before copy/pasting anything, like the other answers try to help you with. (Thats propbably pretty sound advice for any copy/paste)

Answer-533695, Question-24723
I'm not to convinced that using a regular expression is the best way for finding all suspect code. Regular expressions are quite easy to trick specially when dealing with broken HTML. For example, the regular expression listed in the Sanitize HTML link will fail to remove all 'a' elements that have an attribute between the element name and the attribute 'href': < a alt="xss injection" href="http://www.malicous.com/bad.php" > A more robust way of removing malicious code is to rely on a XML Parser that can handle all kind of HTML documents (Tidy, TagSoup, etc) and to select the elements to remove with an XPath expression. Once the HTML document is parsed into a DOM document the elements to revome can be found easily and safely. This is even easy to do with XSLT.

Answer-535022, Question-24723
Don't do this with regular expressions. Remember, you're not protecting just against valid HTML; you're protecting against the DOM that web browsers create. Browsers can be tricked into producing valid DOM from invalid HTML quite easily.  For example, see this list of obfuscated XSS attacks. Are you prepared to tailor a regex to prevent this real world attack on Yahoo and Hotmail on IE6/7/8? How about this attack that works on IE6?  How about attacks that are not listed on this site? The problem with Jeff's approach is that it's not a whitelist, as claimed. As someone on that page adeptly notes: The problem with it, is that the html   must be clean. There are cases where   you can pass in hacked html, and it   won't match it, in which case it'll   return the hacked html string as it   won't match anything to replace. This   isn't strictly whitelisting. I would suggest a purpose built tool like AntiSamy. It works by actually parsing the HTML, and then traversing the DOM and removing anything that's not in the configurable whitelist. The major difference is the ability to gracefully handle malformed HTML.  The best part is that it actually unit tests for all the XSS attacks on the above site. Besides, what could be easier than this API call:

Answer-538379, Question-24723
[\s\w\.]*.  If it doesn't match, you've got XSS.  Maybe.  Take note that this expression only allows letters, numbers,  and periods.  It avoids all symbols, even useful ones, out of fear of XSS.  Once you allow &, you've got worries.  And merely replacing all instances of & with &amp; is not sufficient.  Too complicated to trust :P.  Obviously this will disallow a lot of legitimate text (You can just replace all nonmatching characters with a ! or something), but I think it will kill XSS. The idea to just parse it as html and generate new html is probably better.

Answer-937158, Question-24723
 This  will validate characters, digits, whitespaces and also the <br>  tag.  If you want more risk you can add more tags like

Answer-24094764, Question-24723
I extracted from NoScript best Anti-XSS addon, here is its Regex: Work flawless: Test: http://regex101.com/r/rV7zK8 I think it block 99% XSS because it is a part of NoScript, a addon that get updated regularly

Answer-25745835, Question-24723
An old thread but maybe this will be useful for other users. There is a maintained security layer tool for php: https://github.com/PHPIDS/ It is based on a set of regex which you can find here: https://github.com/PHPIDS/PHPIDS/blob/master/lib/IDS/default_filter.xml

Answer-25701, Question-25614
I like XOM, because I like the way Elliotte Rusty Harold thinks. Of the ones you mention I belive it's the one that strays away from the DOM standard API:s the most, but I consider that a benefit. I once implemented a DOM library for Cocoa, and XOM was my inspiration. I've worked with JDOM as well, and there's absolutely nothing wrong with it, although I do prefer XOM.

Answer-25983, Question-25614
It all depends on the feature set. If you want to benefit from an XSL Transformation Engine (Like Xalan) or an XPath Engine (Like Jaxen or Saxon) I would recommend sticking to the more popular framework available like Apache Xerces, JDOM. After that, it's all a matter of taste. I personnally use a W3C compliant ( org.w3c.* ) like Apache Xerces because they are common enough, reasonably fast and well supported by the Java Community. Of course, if you need blinding speed and do not care about XPath, XQuery or XSL, you can surely find yourself something that is much faster and/or resource-hungry. (i.e. A StAX Implementation)

Answer-575759, Question-25614
While dom4j is an old player, we have been using it for a while and haven't regret it yet. Strong features: simplicity, xpath support and others.  Weak sides: yet to support java 5.0, but version 2.0 has been finally announced.

Answer-26311, Question-26305
A bad example:

Answer-26318, Question-26305
I wrote the following code that works fine. But I think it only works with .wav format.

Answer-575078, Question-26305
The Sound Trail of the Java Tutorial is worth being the starting point.

Answer-2084262, Question-26305
There is an alternative to importing the sound files which works in both applets and applications: convert the audio files into .java files and simply use them in your code. I have developed a tool which makes this process a lot easier.  It simplifies the Java Sound API quite a bit. http://stephengware.com/projects/soundtoclass/

Answer-15694770, Question-26305
I created a game framework sometime ago to work on Android and Desktop, the desktop part that handle sound maybe can be used as inspiration to what you need. https://github.com/hamilton-lima/jaga/blob/master/jaga%20desktop/src-desktop/com/athanazio/jaga/desktop/sound/Sound.java Here is the code for reference.

Answer-20514020, Question-26305
For playing sound in java, you can refer to the following code.

Answer-35162134, Question-26305
I didn't want to have so many lines of code just to play a simple damn sound. This can work if you have the JavaFX package (already included in my jdk 8). Notice : You need to initialize JavaFX. A quick way to do that, is to call the constructor of JFXPanel() once in your app :

Answer-37693420, Question-26305
For whatever reason, the top answer by wchargin was giving me a null pointer error when I was calling this.getClass().getResourceAsStream(). What worked for me was the following: And I would play the sound with: sounds/effects/sheep1.wav was located in the base directory of my project in Eclipse (so not inside the src folder).

Answer-39965540, Question-26305
This thread is rather old but I have determined an option that could prove useful. Instead of using the Java AudioStream library you could use an external program like Windows Media Player or VLC and run it with a console command through Java. This will also create a separate process that can be controlled it the program.  Of course this will take longer to execute than using an internal library but there may be programs that can start up faster and possibly without a GUI given certain console commands.  If time is not of the essence then this is useful.

Answer-28684, Question-28675
I would say don't create a huge ByteBuffer that contains ALL of the data at once.  Create a much smaller ByteBuffer, fill it with data, then write this data to the FileChannel.  Then reset the ByteBuffer and continue until all the data is written.

Answer-28690, Question-28675
If you access files in a random fashion (read here, skip, write there, move back) then you have a problem ;-) But if you only write big files, you should seriously consider using streams. java.io.FileOutputStream can be used directly to write file byte after byte or wrapped in any other stream (i.e. DataOutputStream, ObjectOutputStream) for convenience of writing floats, ints, Strings or even serializeable objects. Similar classes exist for reading files. Streams offer you convenience of manipulating arbitrarily large files in (almost) arbitrarily small memory. They are preferred way of accessing file system in vast majority of cases.

Answer-28743, Question-28675
Check out Java's Mapped Byte Buffers, also known as 'direct buffers'.  Basically, this mechanism uses the OS's virtual memory paging system to 'map' your buffer directly to disk.  The OS will manage moving the bytes to/from disk and memory auto-magically, very quickly, and you won't have to worry about changing your virtual machine options.  This will also allow you to take advantage of NIO's improved performance over traditional java stream-based i/o, without any weird hacks.   The only two catches that I can think of are:  On 32-bit system, you are limited to  just under 4GB total for all mapped byte buffers.  (That is actually a limit for my application, and I now run on 64-bit architectures.) Implementation is JVM specific and not a requirement.  I use Sun's JVM and there are no problems, but YMMV. Kirk Pepperdine (a somewhat famous Java performance guru) is involved with a website, www.JavaPerformanceTuning.com, that has some more MBB details: NIO Performance Tips

Answer-28747, Question-28675
The previous two responses seem pretty reasonable. As for whether the commandline switch will work, it depends how quickly your memory usage hits the limit. If you don't have enough ram and virtual memory available to at least triple the memory available, then you will need to use one of the alternate suggestions given.

Answer-28829, Question-28675
Using the transferFrom method should help with this, assuming you write to the channel incrementally and not all at once as previous answers also point out.

Answer-140145, Question-28675
This can depend on the particular JDK vendor and version. There is a bug in GC in some Sun JVMs.  Shortages of direct memory will not trigger a GC in the main heap, but the direct memory is pinned down by garbage direct ByteBuffers in the main heap.  If the main heap is mostly empty they many not be collected for a long time. This can burn you even if you aren't using direct buffers on your own, because the JVM may be creating direct buffers on your behalf.  For instance, writing a non-direct ByteBuffer to a SocketChannel creates a direct buffer under the covers to use for the actual I/O operation. The workaround is to use a small number of direct buffers yourself, and keep them around for reuse.

Answer-28856, Question-28832
The finalize method is never invoked more than once by a JVM for any given object. You shouldn't be relying on finalize anyway because there's no guarantee that it will be invoked. If you're calling finalize because you need to execute clean up code then better to put it into a separate method and make it explicit, e.g:

Answer-28906, Question-28832
According to this simple test program, the JVM will still make its call to finalize() even if you explicitly called it: The output is: start   finalizing!   finalizing!   done Every resource out there says to never call finalize() explicitly, and pretty much never even implement the method because there are no guarantees as to if and when it will be called.  You're better off just closing all of your resources manually.

Answer-20706792, Question-28832
One must understand the Garbage Collector(GC) Workflow to understand the function of finalize. calling .finalize() will not invoke the garbage collector, nor calling system.gc. Actually, What finalize will help the coder is to declare the reference of the object as "unreferenced". GC forces a suspension on the running operation of JVM, which creates a dent on the performance. During operation, GC will traverse all referenced objects, starting from the root object(your main method call). This suspension time can be decreased by declaring the objects as unreferenced manually, because it will cut down the operation costs to declare the object reference obsolete by the automated run. By declaring finalize(), coder sets the reference to the object obsolete, thus on the next run of GC operation, GC run will sweep the objects without using operation time. Quote: "After the finalize method has been invoked for an object, no further action is taken until the Java virtual machine has again determined that there is no longer any means by which this object can be accessed by any thread that has not yet died, including possible actions by other objects or classes which are ready to be finalized, at which point the object may be discarded. " from Java API Doc on java.Object.finalize(); For detailed explanation, you can also check: javabook.computerware

Answer-28960, Question-28949
The GC does a lot of optimization on when to properly finalize things. So unless you're familiar with how the GC actually works and how it tags generations, manually calling finalize or start GC'ing will probably hurt performance than help.

Answer-28976, Question-28949
Assuming finalizers are similar to their .NET namesake then you only really need to call these when you have resources such as file handles that can leak. Most of the time your objects don't have these references so they don't need to be called. It's bad to try to collect the garbage because it's not really your garbage. You have told the VM to allocate some memory when you created objects, and the garbage collector is hiding information about those objects. Internally the GC is performing optimisations on the memory allocations it makes. When you manually try to collect the garbage you have no knowledge about what the GC wants to hold onto and get rid of, you are just forcing it's hand. As a result you mess up internal calculations. If you knew more about what the GC was holding internally then you might be able to make more informed decisions, but then you've missed the benefits of GC.

Answer-29038, Question-28949
The short answer: Java garbage collection is a very finely tuned tool. System.gc() is a sledge-hammer. Java's heap is divided into different generations, each of which is collected using a different strategy. If you attach a profiler to a healthy app, you'll see that it very rarely has to run the most expensive kinds of collections because most objects are caught by the faster copying collector in the young generation. Calling System.gc() directly, while technically not guaranteed to do anything, in practice will trigger an expensive, stop-the-world full heap collection. This is almost always the wrong thing to do. You think you're saving resources, but you're actually wasting them for no good reason, forcing Java to recheck all your live objects just in case. If you are having problems with GC pauses during critical moments, you're better off configuring the JVM to use the concurrent mark/sweep collector, which was designed specifically to minimise time spent paused, than trying to take a sledgehammer to the problem and just breaking it further. The Sun document you were thinking of is here: Java SE 6 HotSpot Virtual Machine Garbage Collection Tuning (Another thing you might not know: implementing a finalize() method on your object makes garbage collection slower. Firstly, it will take two GC runs to collect the object: one to run finalize() and the next to ensure that the object wasn't resurrected during finalization. Secondly, objects with finalize() methods have to be treated as special cases by the GC because they have to be collected individually, they can't just be thrown away in bulk.)

Answer-29138, Question-28949
Don't bother with finalizers.   Switch to incremental garbage collection. If you want to help the garbage collector, null off references to objects you no longer need. Less path to follow= more explicitly garbage. Don't forget that (non-static) inner class instances keep references to their parent class instance. So an inner class thread keeps a lot more baggage than you might expect. In a very related vein, if you're using serialization, and you've serialized temporary objects, you're going to need to clear the serialization caches, by calling ObjectOutputStream.reset()  or your process will leak memory and eventually die. Downside is that non-transient objects are going to get re-serialized. Serializing temporary result objects can be a bit more messy than you might think! Consider using soft references. If you don't know what soft references are, have a read of the javadoc for java.lang.ref.SoftReference Steer clear of Phantom references and Weak references unless you really get excitable. Finally, if you really can't tolerate the GC use Realtime Java.  No, I'm not joking. The reference implementation is free to download and Peter Dibbles book from SUN is really good reading.

Answer-30477, Question-28949
Avoid finalizers.  There is no guarantee that they will be called in a timely fashion.  It could take quite a long time before the Memory Management system (i.e., the garbage collector) decides to collect an object with a finalizer.   Many people use finalizers to do things like close socket connections or delete temporary files.  By doing so you make your application behaviour unpredictable and tied to when the JVM is going to GC your object.  This can lead to "out of memory" scenarios, not due to the Java Heap being exhausted, but rather due to the system running out of handles for a particular resource. One other thing to keep in mind is that introducing the calls to System.gc() or such hammers may show good results in your environment, but they won't necessarily translate to other systems.  Not everyone runs the same JVM, there are many, SUN, IBM J9, BEA JRockit, Harmony, OpenJDK, etc... This JVM all conform to the JCK (those that have been officially tested that is), but have a lot of freedom when it comes to making things fast.  GC is one of those areas that everyone invests in heavily.  Using a hammer will often times destroy that effort.

Answer-30567, Question-28949
As far as finalizers go: They are virtually useless.  They aren't guaranteed to be called in a timely fashion, or indeed, at all (if the GC never runs, neither will any finalizers).  This means you generally shouldn't rely on them. Finalizers are not guaranteed to be idempotent.  The garbage collector takes great care to guarantee that it will never call finalize() more than once on the same object.  With well-written objects, it won't matter, but with poorly written objects, calling finalize multiple times can cause problems (e.g. double release of a native resource ... crash). Every object that has a finalize() method should also provide a close() (or similar) method.  This is the function you should be calling.  e.g., FileInputStream.close().  There's no reason to be calling finalize() when you have a more appropriate method that is intended to be called by you.

Answer-1072434, Question-28949
you dont call finalize method at all.

Answer-5388006, Question-28949
The real problem with closing OS handles in finalize is that the finalize are executed in no guaranteed order. But if you have handles to the things that block (think e.g. sockets) potentially your code can get into deadlock situation (not trivial at all). So I'm for explicitly closing handles in a predictable orderly manner. Basically code for dealing with resources should follow the pattern: It gets even more complicated if you write your own classes that work via JNI and open handles. You need to make sure handles are closed (released) and that it will happen only once. Frequently overlooked OS handle in Desktop J2SE is Graphics[2D]. Even BufferedImage.getGrpahics() can potentially return you the handle that points into a video driver (actually holding the resource on GPU). If you won't release it yourself and leave it  garbage collector to do the work - you may find strange OutOfMemory and alike situation when you ran out of video card mapped bitmaps but still have plenty of memory. In my experience it happens rather frequently in tight loops working with graphics objects (extracting thumbnails, scaling, sharpening you name it). Basically GC does not take care of programmers responsibility of correct resource management. It only takes care of memory and nothing else. The Stream.finalize calling close() IMHO would be better implemented throwing exception new RuntimeError("garbage collecting the stream that is still open"). It will save hours and days of debugging and cleaning code after the sloppy amateurs left the ends lose. Happy coding. Peace.

Answer-29524, Question-29505
Generally, no. The stream format for Java serialization is defined in this document, but you need access to the original class definitions (and a Java runtime to load them into) to turn the stream data back into something approaching the original objects. For example, classes may define writeObject() and readObject() methods to customise their own serialized form. (edit: lubos hasko suggests having a little java program to deserialize the objects in front of Python, but the problem is that for this to work, your "little java program" needs to load the same versions of all the same classes that it might deserialize. Which is tricky if you're receiving log messages from one app, and really tricky if you're multiplexing more than one log stream. Either way, it's not going to be a little program any more. edit2: I could be wrong here, I don't know what gets serialized. If it's just log4j classes you should be fine. On the other hand, it's possible to log arbitrary exceptions, and if they get put in the stream as well my point stands.) It would be much easier to customise the log4j network adapter and replace the raw serialization with some more easily-deserialized form (for example you could use XStream to turn the object into an XML representation)

Answer-29529, Question-29505
In theory it's possible. Now how difficult in practice it might be depends on whether Java serialization format is documented or not. I guess, it's not. edit: oops, I was wrong, thanks Charles. Anyway, this is what I suggest you to do capture from log4j & deserialize Java object in your own little Java program. now when you have the object again, serialize it using your own custom formatter.  Tip: Maybe you don't even have to write your own custom formatter. for example, JSON (scroll down for libs) has libraries for Python and Java, so you could in theory use Java library to serialize your objects and Python equivalent library to deserialize it send output stream to your python application and deserialize it Charles wrote: the problem is that for this   to work, your "little java program"   needs to load the same versions of all   the same classes that it might   deserialize. Which is tricky if you're   receiving log messages from one app,   and really tricky if you're   multiplexing more than one log stream.   Either way, it's not going to be a   little program any more. Can't you just simply reference Java log4j libraries in your own java process? I'm just giving general advice here that is applicable to any pair of languages (name of the question is pretty language agnostic so I just provided one of the generic solutions). Anyway, I'm not familiar with log4j and don't know whether you can "inject" your own serializer into it. If you can, then of course your suggestion is much better and cleaner.

Answer-29535, Question-29505
I would recommend moving to a third-party format (by creating your own log4j adapters etc) that both languages understand and can easily marshal / unmarshal, e.g. XML.

Answer-29564, Question-29505
Theoretically, it's possible. The Java Serialization, like pretty much everything in Javaland, is standardized. So, you could implement a deserializer according to that standard in Python. However, the Java Serialization format is not designed for cross-language use, the serialization format is closely tied to the way objects are represented inside the JVM. While implementing a JVM in Python is surely a fun exercise, it's probably not what you're looking for (-: There are other (data) serialization formats that are specifically designed to be language agnostic. They usually work by stripping the data formats down to the bare minimum (number, string, sequence, dictionary and that's it) and thus requiring a bit of work on both ends to represent a rich object as a graph of dumb data structures (and vice versa). Two examples are JSON (JavaScript Object Notation) and YAML (YAML Ain't Markup Language). ASN.1 (Abstract Syntax Notation One) is another data serialization format. Instead of dumbing the format down to a point where it can be easily understood, ASN.1 is self-describing, meaning all the information needed to decode a stream is encoded within the stream itself. And, of course, XML (eXtensible Markup Language), will work too, provided that it is not just used to provide textual representation of a "memory dump" of a Java object, but an actual abstract, language-agnostic encoding. So, to make a long story short: your best bet is to either try to coerce log4j into logging in one of the above-mentioned formats, replace log4j with something that does that or try to somehow intercept the objects before they are sent over the wire and convert them before leaving Javaland. Libraries that implement JSON, YAML, ASN.1 and XML are available for both Java and Python (and pretty much every programming language known to man).

Answer-29582, Question-29505
Well I am not Python expert so I can't comment on how to solve your problem but if you have program in .NET you may use IKVM.NET to deserialize Java objects easily. I have experimented this by creating .NET Client for Log4J log messages written to Socket appender and it worked really well. I am sorry, if this answer does not make sense here.

Answer-570540, Question-29505
If you can have a JVM on the receiving side and the class definitions for the serialized data, and you only want to use Python and no other language, then you may use Jython: you would deserialize what you received using the correct Java methods and then you process what you get with you Python code

Answer-29756, Question-29751
have you tried adding the following before the call : The options are explained here. contentType : When sending data to the server, use this content-type. Default is "application/x-www-form-urlencoded", which is fine for most cases. scriptCharset : Only for requests with 'jsonp' or 'script' dataType and GET type. Forces the request to be interpreted as a certain charset. Only needed for charset differences between the remote and local content.

Answer-323518, Question-29751
I had the same problem and fixed it by downgrading to mysql-connector-odbc-3.51.16.

Answer-732713, Question-29751
I had the same problem also and I fixed it in this way: In PHP, before storing the data in the database, I used the htmlentities() function. And during showing the data, I used the html_entity_decode() function. This worked. I strongly hope this will work for you too.

Answer-2048911, Question-29751
I'm having the same problem. I saw InternetExplorer8 sends this header: while Firefox sends this: My solution was just forcing in jQuery to use the Firefox content-type:

Answer-3129545, Question-29751
I see this problem a lot. The meta doesn't work always in your PHP data operations, so just type this at the beginning:

Answer-3656547, Question-29751
As the exception is a jdbc error, your best approach is to capture the input before it is sent to the database. java.sql.BatchUpdateException: Incorrect string value: '\xF1a' for column 'body' at row 1 A single character is causing the exception. It may be the case that you will need to override some characters manually.  You will find, when working with non-latin-alphabet languages (as I do), that this is a common pain.

Answer-30402, Question-30160
DrJava is your best bet. It also has an Eclipse plugin to use the interactions pane like GroovyConsole.

Answer-30408, Question-30160
try beanshell. its a scripting wrapper over java. http://www.beanshell.org/

Answer-190089, Question-30160
Why not use the GroovyConsole ? Groovy accepts the vast majority of Java syntax

Answer-1717633, Question-30160
One good reason for not using something like the Groovy Console (wonderful though it is) is when you want to test what something would be like in Java, without actually going to the trouble of a boilerplate class and main method to run snippets of code. There are some differences between what Groovy does and what Java does. It'd be nice to have a simple way to test something and know for sure it will work when you put it in Java.

Answer-30172, Question-30171
Here's what I ended up with.  I have never found another solution out there for this, so if you have something better, by all means, contribute. First, the long array definition in the wsdl:types area: Next, we create a SoapExtensionAttribute that will perform the fix.  It seems that the problem was that .NET wasn't following the multiref id to the element containing the double value.  So, we process the array item, go find the value, and then insert it the value into the element: Finally, we tag all methods in the Reference.cs file that will be deserializing a long array with our attribute: This fix is long-specific, but it could probably be generalized to handle any primitive type having this problem.

Answer-570459, Question-30171
Found this link that may offer a better alternative:  http://www.tomergabel.com/GettingWCFAndApacheAxisToBeFriendly.aspx

Answer-650260, Question-30171
Here's a more or less copy-pasted version of a blog post I wrote on the subject. Executive summary: You can either change the way .NET deserializes the result set (see Chris's solution above), or you can reconfigure Axis to serialize its results in a way that's compatible with the .NET SOAP implementation. If you go the latter route, here's how: ... the generated   classes look and appear to function   normally, but if you'll look at the   deserialized array on the client   (.NET/WCF) side you'll find that the   array has been deserialized   incorrectly, and all values in the   array are 0. You'll have to manually   look at the SOAP response returned by   Axis to figure out what's wrong;   here's a sample response (again,   edited for clarity): You'll notice that Axis does not   generate values directly in the   returned element, but instead   references external elements for   values. This might make sense when   there are many references to   relatively few discrete values, but   whatever the case this is not properly   handled by the WCF basicHttpBinding   provider (and reportedly by gSOAP and   classic .NET web references as well). It took me a while to find a solution:   edit your Axis deployment's   server-config.wsdd file and find the   following parameter: Change it to false,   then redeploy via the command line,   which looks (under Windows) something   like this: The web service's   response should now be deserializable   by your .NET client.

Answer-30416, Question-30337
I'm not aware of anyone who keeps track of this publicly on a regular basis (unlike Adobe who pushes it every chance they get).  The closest that I could come was this article from last November.  Based upon his site, this data could be skewed a bit, but I think we fairly similar numbers as well.

Answer-30424, Question-30337
There is a very rough percentage of browsers with some JRE available at The Counter, though I wouldn't trust it. Sun has a few useful stats from 2007, but their stats from 2008 are much less detailed. They suggest that in 2007 "92%...of JRE installs...are now Java SE 6", but who knows what highly technical site they surveyed to get that number.

Answer-31146, Question-31127
You can try something like: In your JAR file, you might have a directory structure of: MyJAR.jar   - com (class files in here)   - images   ----image.jpg

Answer-45580, Question-31127
To create an ImageIcon from an image file within the same jars your code is loaded: Class.getResource returns a URL of a resource (or null!). ImageIcon has a constructors that load from a URL. To construct a URL for a resource in a jar not on your "classpath", see the documentation for java.net.JarURLConnection.

Answer-15353743, Question-31127
This is working for me to load and set the content pane background image: jar (or build path) contains: java contains: Tested and working in both jar and unjarred (is that the technical term) execution. BTW getClass().getClassLoader().getResourceAsStream("/img/bg.png") - which I tried first - returned me a null InputStream.

Answer-34138083, Question-31127
Load image in from Jar file during run time is the same as loading image when executed from IDE e.g netbeans the difference is that when loading image from JAR file the path must be correct and its case sensitive (very important). This works for me if p in "/Pictures/firstgame/habitat1.jpg" is in lower case it wont work. check spaces, cases and spelling

Answer-42084902, Question-31127
In netbeans 8.1 what I've done is to include the folder of icons and other images called Resources inside the src folder in the project file. So whenever i build Jar file the folder is included there.The file tree should be like this: src (Java files in source packges are here) ** PACKAGE YOU NAMED IN PROJECT** file.java Resources image.jpg The code should be like:

Answer-31463, Question-31462
I'm currently using this: But not sure if there's a better way.

Answer-31471, Question-31462
I just left this post in your other thread, though what you have above might work as well.  I don't think either would be any easier than the other.  The Apache packages can be accessed by just using import org.apache.commons.HttpClient at the top of your code. Edit: Forgot the link ;)

Answer-33986, Question-31462
This has worked well for me: Not sure at to whether the other solution(s) provided are any more efficient or not.

Answer-15219927, Question-31462
Whilst not vanilla-Java, I'll offer up a simpler solution. Use Groovy ;-)

Answer-31589, Question-31584
I wouldn't do it myself, because to me it muddies what a particular method does, and the method-chaining is of limited use to me over doing it longhand. It isn't going to send me into a quivering ball of rage and psychosis, though, which is always a good thing. :') I wouldn't be concerned about performance; just ask Knuth.

Answer-31604, Question-31584
This is called a Fluent Interface, for reference. Personally, I think it's a pretty neat idea, but a matter of taste really. I think jQuery works this way.

Answer-31606, Question-31584
I agree with @Bernard that method chaining like this muddles the purpose of the setters. Instead I would suggest that if you are always creating chains of setters like this that you create a custom Constructor for your class so instead of You do This makes it more readable and you can use this to make your class immutable if you chose to.

Answer-31614, Question-31584
This idea is seen a lot in c++, it allows operations to be cascaded... for example is where the stream method << returns an instance of itself (*this). see this: http://www.java2s.com/Tutorial/Cpp/0180__Class/Cascadingmemberfunctioncallswiththethispointer.htm

Answer-31621, Question-31584
I find this to be in poor style when used in setters. Immutable classes are usually a better fit for chaining, such as: where myObject is of this class: The builder pattern is also useful, since it allows the final object to be immutable while preventing the intermediate instances you would normally have to create when using this technique.

Answer-31625, Question-31584
I use to be a fan of the Java (and worse C#) practice of making getters and setters (get set properties) throughout an object.  This use to be what I considered object oriented, but really this leads us just to exposing the guts and implementation of the object and not really taking advantage of encapsulation.  There are times you can't get away from this (OR/M comes to mind), but in general the object should be set up and then perform its function.  My dream objects tend to have one or two constructors, and maybe a half dozen functions that do work. The reason for this is that once I started developing API's there is a real need to keep things simple.  You really only want to add as much complexity as is required to get the job done, and getters and setters, while simple in themselves, add complexity in heaps when added in mass.  What happens when I load setters i na different order?  Anythign different? Are you sure?

Answer-31639, Question-31584
@Dan again, for more complex situations (immutability comes in mind) the Builder Pattern is a great solution. Also, I agree with you mostly in getters. I believe what you are saying is to mostly follow the "Tell don't ask" paradigm and I greatly agree. But that is oriented mostly at getters. Lastly, all of the above are for classes that have a great deal of attributes. I don't see a reason for any if you only have less than, say, 7.

Answer-37005, Question-31584
@pek Chained invocation is one of proposals for Java 7. It says that if a method return type is void, it should implicitly return this. If you're interested in this topic, there is a bunch of links and a simple example on Alex Miller's Java 7 page.

Answer-39948, Question-31584
I ended up doing this a lot when working with the Apache POI excel library; I ended up writing helper methods that chained so I could apply formatting, data types, internal cell data, formulas, and cell positioning. For stuff with lots of little tiny flyweight elements that need to have finicky little tweaks applied it works pretty well.

Answer-46858, Question-31584
It makes sense for builders, where all you are going to do is set a load of stuff, create the real object and throw the builder away. For builders, you might as well get rid of the "set" part of the method name. Similarly, immutable types don't really need the "get". A useful trick (if you don't mind a ~2K runtime overhead per class) is to use the double brace idiom:

Answer-3741052, Question-31584
How To Use

Answer-31696, Question-31693
Wikipedia has great write-ups comparing both Java/C# generics and Java generics/C++ templates. The main article on Generics seems a bit cluttered but it does have some good info in it.

Answer-31697, Question-31693
Anders Hejlsberg himself described the differences here "Generics in C#, Java, and C++".

Answer-31698, Question-31693
The biggest complaint is type erasure.  In that, generics are not enforced at runtime.  Here's a link to some Sun docs on the subject. Generics are implemented by type   erasure: generic type information is   present only at compile time, after   which it is erased by the compiler.

Answer-31721, Question-31693
C++ templates are actually much more powerful than their C# and Java counterparts as they are evaluated at compile time and support specialization. This allows for Template Meta-Programming and makes the C++ compiler equivalent to a Turing machine (i.e. during the compilation process you can compute anything that is computable with a Turing machine).

Answer-31758, Question-31693
C++ rarely uses the generics terminology. Instead, the word templates is used and is more accurate. Templates describes one technique to achieve a generic design. C++ templates is very different from what both C# and Java implement for two main reasons. The first reason is that C++ templates don't only allow compile-time type arguments but also compile-time const-value arguments: templates can be given as integers or even function signatures. This means that you can do some quite funky stuff at compile time, e.g. calculations: This code also uses the other distinguished feature of C++ templates, namely template specialization. The code defines one class template, product that has one value argument. It also defines a specialization for that template that is used whenever the argument evaluates to 1. This allows me to define a recursion over template definitions. I believe that this was first discovered by Andrei Alexandrescu. Template specialization is important for C++ because it allows for structural differences in data structures. Templates as a whole is a means of unifying an interface across types. However, although this is desirable, all types cannot be treated equally inside the implementation. C++ templates takes this into account. This is very much the same difference that OOP makes between interface and implementation with the overriding of virtual methods. C++ templates are essential for its algorithmic programming paradigm. For example, almost all algorithms for containers are defined as functions that accept the container type as a template type and treat them uniformly. Actually, that's not quite right: C++ doesn't work on containers but rather on ranges that are defined by two iterators, pointing to the beginning and behind the end of the container. Thus, the whole content is circumscribed by the iterators: begin <= elements < end. Using iterators instead of containers is useful because it allows to operate on parts of a container instead of on the whole. Another distinguishing feature of C++ is the possibility of partial specialization for class templates. This is somewhat related to pattern matching on arguments in Haskell and other functional languages. For example, let's consider a class that stores elements: This works for any element type. But let's say that we can store pointers more effciently than other types by applying some special trick. We can do this by partially specializing for all pointer types: Now, whenever we instance a container template for one type, the appropriate definition is used:

Answer-31778, Question-31693
In Java, generics are compiler level only, so you get: Note that the type of 'a' is an array list, not a list of strings. So the type of a list of bananas would equal() a list of monkeys. So to speak.

Answer-31821, Question-31693
Follow-up to my previous posting. Templates are one of the main reasons why C++ fails so abysmally at intellisense, regardless of the IDE used. Because of template specialization, the IDE can never be really sure if a given member exists or not. Consider: Now, the cursor is at the indicated position and it's damn hard for the IDE to say at that point if, and what, members a has. For other languages the parsing would be straightforward but for C++, quite a bit of evaluation is needed beforehand. It gets worse. What if my_int_type were defined inside a class template as well? Now its type would depend on another type argument. And here, even compilers fail. After a bit of thinking, a programmer would conclude that this code is the same as the above: Y<int>::my_type resolves to int, therefore b should be the same type as a, right? Wrong. At the point where the compiler tries to resolve this statement, it doesn't actually know Y<int>::my_type yet! Therefore, it doesn't know that this is a type. It could be something else, e.g. a member function or a field. This might give rise to ambiguities (though not in the present case), therefore the compiler fails. We have to tell it explicitly that we refer to a type name: Now, the code compiles. To see how ambiguities arise from this situation, consider the following code: This code statement is perfectly valid and tells C++ to execute the function call to Y<int>::my_type. However, if my_type is not a function but rather a type, this statement would still be valid and perform a special cast (the function-style cast) which is often a constructor invocation. The compiler can't tell which we mean so we have to disambiguate here.

Answer-31866, Question-31693
Both Java and C# introduced generics after their first language release.  However, there are differences in how the core libraries changed when generics was introduced.  C#'s generics are not just compiler magic and so it was not possible to generify existing library classes without breaking backwards compatibility. For example, in Java the existing Collections Framework was completely genericised.  Java does not have both a generic and legacy non-generic version of the collections classes.  In some ways this is much cleaner - if you need to use a collection in C# there is really very little reason to go with the non-generic version, but those legacy classes remain in place, cluttering up the landscape.  Another notable difference is the Enum classes in Java and C#.  Java's Enum has this somewhat tortuous looking definition: (see Angelika Langer's very clear explanation of exactly why this is so.  Essentially, this means Java can give type safe access from a string to its Enum value: Compare this to C#'s version: As Enum already existed in C# before generics was introduced to the language, the definition could not change without breaking existing code.  So, like collections, it remains in the core libraries in this legacy state.

Answer-31929, Question-31693
I'll add my voice to the noise and take a stab at making things clear: C# Generics allow you to declare something like this. and then the compiler will prevent you from putting things that aren't Person into the list. Behind the scenes the C# compiler is just putting List<Person> into the .NET dll file, but at runtime the JIT compiler  goes and builds a new set of code, as if you had written a special list class just for containing people - something like ListOfPerson. The benefit of this is that it makes it really fast. There's no casting or any other stuff, and because the dll contains the information that this is a List of Person, other code that looks at it later on using reflection can tell that it contains Person objects (so you get intellisense and so on). The downside of this is that old C# 1.0 and 1.1 code (before they added generics) doesn't understand these new List<something>, so you have to manually convert things back to plain old List to interoperate with them. This is not that big of a problem, because C# 2.0 binary code is not backwards compatible. The only time this will ever happen is if you're upgrading some old C# 1.0/1.1 code to C# 2.0 Java Generics allow you to declare something like this. On the surface it looks the same, and it sort-of is. The compiler will also prevent you from putting things that aren't Person into the list. The difference is what happens behind the scenes. Unlike C#, Java does not go and build a special ListOfPerson - it just uses the plain old ArrayList which has always been in Java. When you get things out of the array, the usual Person p = (Person)foo.get(1); casting-dance still has to be done. The compiler is saving you the key-presses, but the speed hit/casting is still incurred just like it always was. When people mention "Type Erasure" this is what they're talking about. The compiler inserts the casts for you, and then 'erases' the fact that it's meant to be a list of Person not just Object The benefit of this approach is that old code which doesn't understand generics doesn't have to care. It's still dealing with the same old ArrayList as it always has. This is more important in the java world because they wanted to support compiling code using Java 5 with generics, and having it run on old 1.4 or previous JVM's, which microsoft deliberately decided not to bother with. The downside is the speed hit I mentioned previously, and also because there is no ListOfPerson pseudo-class or anything like that going into the .class files, code that looks at it later on (with reflection, or if you pull it out of another collection where it's been converted into Object or so on) can't tell in any way that it's meant to be a list containing only Person and not just any other array list. C++ Templates allow you to declare something like this It looks like C# and Java generics, and it will do what you think it should do, but behind the scenes different things are happening. It has the most in common with C# generics in that it builds special pseudo-classes rather than just throwing the type information away like java does, but it's a whole different kettle of fish. Both C# and Java produce output which is designed for virtual machines. If you write some code which has a Person class in it, in both cases some information about a Person class will go into the .dll or .class file, and the JVM/CLR will do stuff with this. C++ produces raw x86 binary code. Everything is not an object, and there's no underlying virtual machine which needs to know about a Person class. There's no boxing or unboxing, and functions don't have to belong to classes, or indeed anything.  Because of this, the C++ compiler places no restrictions on what you can do with templates - basically any code you could write manually, you can get templates to write for you. The most obvious example is adding things: In C# and Java, the generics system needs to know what methods are available for a class, and it needs to pass this down to the virtual machine. The only way to tell it this is by either hard-coding the actual class in, or using interfaces. For example: That code won't compile in C# or Java, because it doesn't know that the type T actually provides a method called Name(). You have to tell it - in C# like this: And then you have to make sure the things you pass to addNames implement the IHasName interface and so on. The java syntax is different (<T extends IHasName>), but it suffers from the same problems. The 'classic' case for this problem is trying to write a function which does this You can't actually write this code because there are no ways to declare an interface with the + method in it. You fail. C++ suffers from none of these problems. The compiler doesn't care about passing types down to any VM's - if both your objects have a .Name() function, it will compile. If they don't, it won't. Simple. So, there you have it :-)

Answer-33729, Question-31693
There are already a lot of good answers on what the differences are, so let me give a slightly different perspective and add the why. As was already explained, the main difference is type erasure, i.e. the fact that the Java compiler erases the generic types and they don't end up in the generated bytecode. However, the question is: why would anyone do that? It doesn't make sense! Or does it? Well, what's the alternative? If you don't implement generics in the language, where do you implement them? And the answer is: in the Virtual Machine. Which breaks backwards compatibility. Type erasure, on the other hand, allows you to mix generic clients with non-generic libraries. In other words: code that was compiled on Java 5 can still be deployed to Java 1.4. Microsoft, however, decided to break backwards compatibility for generics. That's why .NET Generics are "better" than Java Generics. Of course, Sun aren't idiots or cowards. The reason why they "chickened out", was that Java was significantly older and more widespread than .NET when they introduced generics. (They were introduced roughly at the same time in both worlds.) Breaking backwards compatibility would have been a huge pain. Put yet another way: in Java, Generics are a part of the Language (which means they apply only to Java, not to other languages), in .NET they are part of the Virtual Machine (which means they apply to all languages, not just C# and Visual Basic.NET). Compare this with .NET features like LINQ, lambda expressions, local variable type inference, anonymous types and expression trees: these are all language features. That's why there are subtle differences between VB.NET and C#: if those features were part of the VM, they would be the same in all languages. But the CLR hasn't changed: it's still the same in .NET 3.5 SP1 as it was in .NET 2.0. You can compile a C# program that uses LINQ with the .NET 3.5 compiler and still run it on .NET 2.0, provided that you don't use any .NET 3.5 libraries. That would not work with generics and .NET 1.1, but it would work with Java and Java 1.4.

Answer-38286, Question-31693
Looks like, among other very interesting proposals, there is one about refining generics and breaking backwards compatibility: Currently, generics are implemented   using erasure, which means that the   generic type information is not   available at runtime, which makes some   kind of code hard to write. Generics   were implemented this way to support   backwards compatibility with older   non-generic code. Reified generics   would make the generic type   information available at runtime,   which would break legacy non-generic   code. However, Neal Gafter has   proposed making types reifiable only   if specified, so as to not break   backward compatibility. at Alex Miller's article about Java 7 Proposals

Answer-1109644, Question-31693
11 months late, but I think this question is ready for some Java Wildcard stuff. This is a syntactical feature of Java. Suppose you have a method: And suppose you don't need to refer to the type T in the method body. You're declaring a name T and then only using it once, so why should you have to think of a name for it? Instead, you can write: The question-mark asks the the compiler to pretend that you declared a normal named type parameter that only needs to appear once in that spot. There's nothing you can do with wildcards that you can't also do with a named type parameter (which is how these things are always done in C++ and C#).

Answer-3398108, Question-31693
NB: I don't have enough point to comment, so feel free to move this as a comment to appropriate answer. Contrary to popular believe, which I never understand where it came from, .net implemented true generics without breaking backward compatibility, and they spent explicit effort for that. You don't have to change your non-generic .net 1.0 code into generics just to be used in .net 2.0. Both the generic and non-generic lists are still available in .Net framework 2.0 even until 4.0, exactly for nothing else but backward compatibility reason. Therefore old codes that still used non-generic ArrayList will still work, and use the same ArrayList class as before. Backward code compatibility is always maintained since 1.0 till now... So even in .net 4.0, you still have to option to use any non-generics class from 1.0 BCL if you choose to do so. So I don't think java has to break backward compatibility to support true generics.

Answer-32008, Question-32001
Do you need to schedule a recurring task? In that case I recommend you consider using Quartz.

Answer-32047, Question-32001
I don't think it's possible to do it with Timer/TimerTask, but depending on what exactly you want to achieve you might be happy with using java.util.concurrent.ScheduledThreadPoolExecutor.

Answer-32057, Question-32001
According to the Timer documentation, in Java 1.5 onwards, you should prefer the ScheduledThreadPoolExecutor instead. (You may like to create this executor using Executors.newSingleThreadScheduledExecutor() for ease of use; it creates something much like a Timer.) The cool thing is, when you schedule a task (by calling schedule()), it returns a ScheduledFuture object. You can use this to cancel the scheduled task. You're then free to submit a new task with a different triggering time. ETA: The Timer documentation linked to doesn't say anything about ScheduledThreadPoolExecutor, however the OpenJDK version had this to say: Java 5.0 introduced the java.util.concurrent package and   one of the concurrency utilities therein is the    ScheduledThreadPoolExecutor which is a thread pool for repeatedly   executing tasks at a given rate or delay.  It is effectively a more   versatile replacement for the Timer/TimerTask   combination, as it allows multiple service threads, accepts various   time units, and doesn't require subclassing TimerTask (just   implement Runnable).  Configuring   ScheduledThreadPoolExecutor with one thread makes it equivalent to   Timer.

Answer-32073, Question-32001
If your Timer is only ever going to have one task to execute then I would suggest subclassing it: You will need to work on the code to add checks for mis-use, but it  should achieve what you want. The ScheduledThreadPoolExecutor does not seem to have built in support for rescheduling existing tasks either, but a similar approach should work there as well.

Answer-756285, Question-32001
this is what I'm trying out.  I have a class that polls a database every 60 seconds using a TimerTask. in my main class, I keep the instance of the Timer, and an instance of my local subclass of TimerTask.  the main class has a method to set the polling interval (say going from 60 to 30).  in it, i cancel my TimerTask (which is my subclass, where I overwrote the cancel() method to do some cleanup, but that shouldn't matter) and then make it null.  i recreate a new instance of it, and schedule the new instance at the new interval in the existing Timer. since the Timer itself isn't canceled, the thread it was using stays active (and so would any other TimerTasks inside it), and the old TimerTask is replaced with a new one, which happens to be the same, but VIRGIN (since the old one would have been executed or scheduled, it is no longer VIRGIN, as required for scheduling). when i want to shutdown the entire timer, i cancel and null the TimerTask (same as i did when changing the timing, again, for cleaning up resources in my subclass of TimerTask), and then i cancel and null the Timer itself.

Answer-13583445, Question-32001
The whole Code snippet goes like this .... I hope it will be help full

Answer-13720407, Question-32001
Here is the example for Resetable Timer . Try to change it for your convinence... This example prints the current date and time for every 5 seconds...But if you give any input in console the timer will be delayed to perform the given input task...

Answer-32916, Question-32897
Eclipse is correct. There is no interpretation context inside a comment (no escaping, etc). See JLS 3.7.

Answer-32918, Question-32897
Yes, I am commenting the code out just to do a quick test. I've already tested what I needed to by commenting the code out another way; I was just curious about what appears to be an odd misfeature of Java and/or Eclipse.

Answer-32927, Question-32897
A simple test shows Eclipse is correct: This fails to compile with:

Answer-32931, Question-32897
I may be helpful to just do a "batch" multiline comment so that it comments each line with "//". It is Ctrl+"/" in Idea for commenting and uncommenting the selected lines, Eclipse should have a similar feature.

Answer-32982, Question-32897
In Eclipse you can highlight the part of the source code you want to comment out and use the Ctrl+/ to single-line comment every line in the highlighted section - puts a "//" at the beginning of the lines. Or if you really want to block-comment the selection use the Ctrl+Shift+/ combination. It will detect the block comments in your selection. However undoing this is harder than single-line comments.

Answer-143915, Question-32897
I often use only // for inline commments, and use /* */ only for commenting out large blocks the way you have. A lot of developers will still use /*  */ for inline comments, because that's what they're familiar with, but they all run into problems like this one, in C it didn't matter as much because you could #if 0 the stuff away.

Answer-33456, Question-33412
For session cookies it doesn't seem to be supported in Tomcat yet. See the bug report Need to add support for HTTPOnly session cookie parameter. A somewhat involved work-around for now can be found here, which basically boils down to manually patching Tomcat. Can't really find an easy way to do it at this moment at this point I'm affraid.  To summarize the work-around, it involves downloading the 5.5 source, and then change the source in the following places: org.apache.catalina.connector.Request.java org.apache.catalina.connectorResponse.addCookieInternal

Answer-33461, Question-33412
 Update: The JSESSIONID stuff here is   only for older containers. Please use   jt's currently accepted answer unless   you are using < Tomcat 6.0.19 or < Tomcat   5.5.28 or another container that does not support HttpOnly JSESSIONID cookies as a config option. When setting cookies in your app, use However, in many webapps, the most important cookie is the session identifier, which is automatically set by the container as the JSESSIONID cookie. If you only use this cookie, you can write a ServletFilter to re-set the cookies on the way out, forcing JSESSIONID to HttpOnly. The page at http://keepitlocked.net/archive/2007/11/05/java-and-httponly.aspx http://alexsmolen.com/blog/?p=16 suggests adding the following in a filter. but note that this will overwrite all cookies and only set what you state here in this filter.  If you use additional cookies to the JSESSIONID cookie, then you'll need to extend this code to set all the cookies in the filter. This is not a great solution in the case of multiple-cookies, but is a perhaps an acceptable quick-fix for the JSESSIONID-only setup.  Please note that as your code evolves over time, there's a nasty hidden bug waiting for you when you forget about this filter and try and set another cookie somewhere else in your code. Of course, it won't get set. This really is a hack though. If you do use Tomcat and can compile it, then take a look at Shabaz's excellent suggestion to patch HttpOnly support into Tomcat.

Answer-1088009, Question-33412
httpOnly is supported as of Tomcat 6.0.19 and Tomcat 5.5.28. See the changelog entry for bug 44382.  The last comment for bug 44382 states, "this has been applied to 5.5.x and will be included in 5.5.28 onwards." However, it does not appear that 5.5.28 has been released. The httpOnly functionality can be enabled for all webapps in conf/context.xml: My interpretation is that it also works for an individual context by setting it on the desired Context entry in conf/server.xml (in the same manner as above).

Answer-1465092, Question-33412
Please be careful not to overwrite the ";secure" cookie flag in https-sessions. This flag prevents the browser from sending the cookie over an unencrypted http connection, basically rendering the use of https for legit requests pointless.

Answer-3516432, Question-33412
also it should be noted that turning on HttpOnly will break applets that require stateful access back to the jvm. the Applet http requests will not use the jsessionid cookie and may get assigned to a different tomcat.

Answer-14610452, Question-33412
For cookies that I am explicitly setting, I switched to use SimpleCookie provided by Apache Shiro. It does not inherit from javax.servlet.http.Cookie so it takes a bit more juggling to get everything to work correctly however it does provide a property set HttpOnly and it works with Servlet 2.5. For setting a cookie on a response, rather than doing response.addCookie(cookie) you need to do cookie.saveTo(request, response).

Answer-30995999, Question-33412
If your Serlvet 3.0 web server, like tomcat 7.0+, you can use below in web.xml as: As mentioned in docs: HttpOnly: Specifies whether any session tracking cookies created by   this web application will be marked as HttpOnly  Secure: Specifies   whether any session tracking cookies created by this web application   will be marked as secure even if   the request that initiated the   corresponding session is using plain HTTP instead of HTTPS Please refer to how to set httponly and session cookie for java web appliaction

Answer-34147407, Question-33412
In Tomcat6, You can conditionally enable from your HTTP Listener Class: Using this class

Answer-34027, Question-33956
There may well be a better Solr-specific answer, but I work with straight Lucene, and since you're not getting much traction I'll take a stab. There, I'd create a populate a Filter with a FilteredQuery wrapping the original Query. Then I'd get a FieldCache for the field of interest. Enumerate the hits in the filter's bitset, and for each hit, you get the value of the field from the field cache, and add it to a SortedSet. When you've got all of the hits, divide the size of the set into the number of ranges you want (five to seven is a good number according the user interface guys), and rather than a single-valued constraint, your facets will be a range query with the lower and upper bounds of each of those subsets. I'd recommend using some special-case logic for a small number of values; obviously, if you only have four distinct values, it doesn't make sense to try and make 5 range refinements out of them. Below a certain threshold (say 3*your ideal number of ranges), you just show the facets normally rather than ranges.

Answer-170477, Question-33956
To answer your first question, you can get facet ranges by using the the generic facet query support. Here's an example: As for your second question (automatically suggesting facet ranges), that's not yet implemented. Some argue that this kind of querying would be best implemented on your application rather that letting Solr "guess" the best facet ranges. Here are some discussions on the topic: (Archived) https://web.archive.org/web/20100416235126/http://old.nabble.com/Re:-faceted-browsing-p3753053.html (Archived) https://web.archive.org/web/20090430160232/http://www.nabble.com/Re:-Sorting-p6803791.html (Archived) https://web.archive.org/web/20090504020754/http://www.nabble.com/Dynamically-calculated-range-facet-td11314725.html

Answer-10599043, Question-33956
You can use solr facet ranges http://wiki.apache.org/solr/SimpleFacetParameters#Facet_by_Range

Answer-11329853, Question-33956
I have worked out how to calculate sensible dynamic facets for product price ranges.  The solution involves some pre-processing of documents and some post-processing of the query results, but it requires only one query to Solr, and should even work on old version of Solr like 1.4. Round up prices before submission First, before submitting the document, round up the the price to the nearest "nice round facet boundary" and store it in a "rounded_price" field.  Users like their facets to look like "250-500" not "247-483", and rounding also means you get back hundreds of price facets not millions of them.  With some effort the following code can be generalised to round nicely at any price scale: Permissible prices go 1,2,3,...,24,25,30,35,...,95,100,110,...,240,250,275,300,325,...,975,1000 and so forth.   Get all facets on rounded prices Second, when submitting the query, request all facets on rounded prices sorted by price: facet.field=rounded_price.  Thanks to the rounding, you'll get at most a few hundred facets back. Combine adjacent facets into larger facets Third, after you have the results, the user wants see only 3 to 7 facets, not hundreds of facets.  So, combine adjacent facets into a few large facets (called "segments") trying to get a roughly equal number of documents in each segment. The following rather more complicated code does this, returning tuples of (start, end, count) suitable for performing range queries.  The counts returned will be correct provided prices were been rounded up to the nearest boundary: Filter results by selected facet Fourth, suppose ("250","500",38) was one of the resulting segments.  If the user selects "$250 to $500" as a filter, simply do a filter query fq=price:[250 TO 500]

Answer-34156, Question-34144
Using the Maven Dependency Plugin:

Answer-34162, Question-34144
If you run maven with "-x" switch, it will print out plenty of diagnostics, I guess the relevant dependency path can be picked up from there.

Answer-34167, Question-34144
To add to @David Crow, here's a dependency:tree example from the Maven site: might output

Answer-219325, Question-34144
The dependency information is also included in the Project Information/Dependencies report if you have maven generate a site for the project, using mvn site.

Answer-274901, Question-34144
If you use eclipse and the m2eclipse plugin (http://m2eclipse.sonatype.org) then there is a graphical version of dependency tree where you can filter by scope etc.

Answer-4233244, Question-34144
You can have many reports by  mvn site One of them is the dependency report.

Answer-34743, Question-34726
This is probably going to be difficult to resolve unless you post either the error message from the log file or the list of steps that you took so far. I have JRun 3.1 configured on my machine so maybe I can duplicate your issue if you give us more information.

Answer-34876, Question-34726
Jrun development has pretty much stopped. You should look into running another application server. Jboss or Glassfish are good alternatives.

Answer-35550, Question-34726
I didn't know JRun was even still in existence since 1999 or something like that. Anyway, Tomcat or Jetty would be my easy replacements. Tomcat for its scriptability from ANT etc and Jetty for its pure simplicity (start an instance in 5 lines of code!). Glassfish is a huge system with many components, if you just want to host vanilla servlets and JSPs etc. then I would go for one of the above.

Answer-35187, Question-35186
This is usually caused when using a build system like Apache Ant that only compiles java files when the java file is newer than the class file.  If a method signature changes and classes were using the old version things may not be compiled correctly.  The usual fix is to do a full rebuild (usually "ant clean" then "ant").   Sometimes this can also be caused when compiling against one version of a library but running against a different version.

Answer-35210, Question-35186
Without any more information it is difficult to pinpoint the problem, but the root cause is that you most likely have compiled a class against a different version of the class that is missing a method, than the one you are using when running it. Look at the stack trace ... If the exception appears when calling a method on an object in a library, you are most likely using separate versions of the library when compiling and running. Make sure you have the right version both places. If the exception appears when calling a method on objects instantiated by classes you made, then your build process seems to be faulty. Make sure the class files that you are actually running are updated when you compile.

Answer-35350, Question-35186
This can also be the result of using reflection.  If you have code that reflects on a class and extracts a method by name (eg: with Class.getDeclaredMethod("someMethodName", .....)) then any time that method name changes, such as during a refactor, you will need to remember to update the parameters to the reflection method to match the new method signature, or the getDeclaredMethod call will throw a NoSuchMethodException. If this is the reason, then the stack trace should show the point that the reflection method is invoked, and you'll just need to update the parameters to match the actual method signature.   In my experience, this comes up occasionally when unit testing private methods/fields, and using a TestUtilities class to extract fields for test verification.  (Generally with legacy code that wasn't designed with unit testing in mind.)

Answer-36617, Question-35186
Note that in the case of reflection, you get an NoSuchMethodException, while with non-reflective code, you get NoSuchMethodError. I tend to go looking in very different places when confronted with one versus the other.

Answer-38656, Question-35186
If you are writing a webapp, ensure that you don't have conflicting versions of a jar in your container's global library directory and also in your app. You may not necessarily know which jar is being used by the classloader. e.g.   tomcat/common/lib mywebapp/WEB-INF/lib

Answer-42097, Question-35186
If you have access to change the JVM parameters, adding verbose output should allow you to see what classes are being loaded from what JARs. When your program is run, the JVM should dump to standard out information such as: ... [Loaded junit.framework.Assert from file:/C:/Program%20Files/junit3.8.2/junit.jar] ...

Answer-2985338, Question-35186
I feel your pain.  You can learn programming out of a book, but when it comes to working with Eclipse or Visual Studio, its a ^&^&'n nightmare to do something simple like add a library.  Everybody expects you to know how to use it and if you don't they downvote your question.  The problem is, if you don't work in an office or know anyone who you can ask these questions, then its almost impossible to figure this stuff out.  Anyway... I was having your problem, and this is how I fixed it.  The following steps are a working way to add a library.  I had done the first two steps right, but I hadn't done the last one by dragging the ".jar" file direct from the file system into the "lib" folder on my eclipse project.  Additionally, I had to remove the previous version of the library from both the build path and the "lib" folder. If anyone knows of a more proper way to add/update a library, please chime in. Step 1 - Add .jar to build path Step 2 - Associate sources and javadocs (optional) Step 3 - Actually drag .jar file into "lib" folder (not optional)

Answer-14017856, Question-35186
To answer the original question. According to java docs here "NoSuchMethodError" Thrown if an application tries to call a specified method of a class (either static or instance), and that class no longer has a definition of that method. Normally, this error is caught by the compiler; this error can only occur at run time if the definition of a class has incompatibly changed.  !) if it happens in the run time, check the class containing the method is in class path. 2) Check if you have added new version of jar and the method is compatible.

Answer-18336764, Question-35186
I ran into a similar problem when I was changing method signatures in my application.  Cleaning and rebuilding my project resolved the "NoSuchMethodError".

Answer-27896288, Question-35186
If your file name is different than the class name which contain main method then it may be the possibility that this error may cause.

Answer-31451884, Question-35186
These problems are caused by the use of the same object at the same two classes. Objects used does not contain new method has been added that the new object class contains. ex: These problems are caused by the concomitant 02 similar class (1 in src, 1 in jar file here is gateway.jar)

Answer-33004790, Question-35186
If using maven or another framework, and you get this error randomly almost, try "clean install", especially if you wrote the object and you know it has the method.  Worked for me.

Answer-38221921, Question-35186
I've had the same problem. This is also caused when there is an ambiguity in classes. My program was trying to invoke a method which was present in two jar files present in the same location / class path. Delete one jar file or execute your code such that only one jar file is used. Check that you are not using same jar or different versions of the same jar that contain the same class. DISP_E_EXCEPTION [step] [] [Z-JAVA-105  Java exception java.lang.NoSuchMethodError(com.example.yourmethod)]

Answer-40418906, Question-35186
It means the respective method is not present in the class:  If you are using jar then decompile and check if the respective version of jar have proper class.  Check if you have compiled proper class from your source.

Answer-40696954, Question-35186
Above answer explains very well ..just to add one thing If you are using using eclipse use ctrl+shift+T and enter package structure of class (e.g. : gateway.smpp.PDUEventListener ), you will find all jars/projects where it's present. Remove unnecessary jars from classpath or add above in class path. Now it will pick up correct one.

Answer-45825973, Question-35186
For me it happened because I changed argument type in function, from Object a, to String a. I could resolve it with clean and build again

Answer-37632, Question-37628
The name reflection is used to describe code which is able to inspect other code in the same system (or itself). For example, say you have an object of an unknown type in Java, and you would like to call a 'doSomething' method on it if one exists. Java's static typing system isn't really designed to support this unless the object conforms to a known interface, but using reflection, your code can look at the object and find out if it has a method called 'doSomething' and then call it if you want to. So, to give you a code example of this in Java (imagine the object in question is foo) : One very common use case in Java is the usage with annotations. JUnit 4, for example, will use reflection to look through your classes for methods tagged with the @Test annotation, and will then call them when running the unit test. There are some good reflection examples to get you started at http://docs.oracle.com/javase/tutorial/reflect/index.html And finally, yes, the concepts are pretty much similar in other statically types languages which support reflection (like C#). In dynamically typed languages, the use case described above is less necessary (since the compiler will allow any method to be called on any object, failing at runtime if it does not exist), but the second case of looking for methods which are marked or work in a certain way is still common.

Answer-37634, Question-37628
Reflection is a set of functions which allows you to access the runtime information of your program and modify it behavior (with some limitations). It's useful because it allows you to change the runtime behaivour depending on the meta information of your program, that is, you can check the return type of a function and change the way you handle the situation. In C# for example you can load an assembly (a .dll) in runtime an examine it, navigating through the classes and taking actions according to what you found. It also let you create an instance of a class on runtime, invoke its method, etc. Where can it be useful? Is not useful everytime but for concrete situations. For example you can use it to get the name of the class for loggin purposes, to dinamically create handlers for events according to what's specified on a configuration file and so on...

Answer-37635, Question-37628
Not every language supports reflection but the principles are usually the same in languages that support it. Reflection is the ability to "reflect" on the structure of your program. Or more concrete. To look at the objects and classes you have and programmatically get back information on the methods, fields, and interfaces they implement. You can also look at things like annotations. It's usefull in a lot of situations. Everywhere you want to be able to dynamically plug in classes into your code. Lot's of object relational mappers use reflection to be able to instantiate objects from databases without knowing in advance what objects they're going to use. Plug-in architectures is another place where reflection is usefull. Being able to dynamically load code and determine if there are types there that implement the right interface to use as a plugin is important in those situations.

Answer-37638, Question-37628
 Reflection is a language's ability to inspect and dynamically call classes, methods, attributes, etc. at runtime. For example, all objects in Java have the method getClass(), which lets you determine the object's class even if you don't know it at compile time (e.g. if you declared it as an Object) - this might seem trivial, but such reflection is not possible in less dynamic languages such as C++. More advanced uses lets you list and call methods, constructors, etc. Reflection is important since it lets you write programs that do not have to "know" everything at compile time, making them more dynamic, since they can be tied together at runtime. The code can be written against known interfaces, but the actual classes to be used can be instantiated using reflection from configuration files. Lots of modern frameworks use reflection extensively for this very reason. Most other modern languages use reflection as well, and in scripting languages (such as Python) they are even more tightly integrated, since it feels more natural within the general programming model of those languages.

Answer-37659, Question-37628
Reflection is a key mechanism to allow an application or framework to work with code that might not have even been written yet! Take for example your typical web.xml file. This will contain a list of servlet elements, which contain nested servlet-class elements. The servlet container will process the web.xml file, and create new a new instance of each servlet class through reflection. Another example would be the Java API for XML Parsing (JAXP). Where an XML parser provider is 'plugged-in' via well-known system properties, which are used to construct new instances through reflection. And finally, the most comprehensive example is Spring which uses reflection to create its beans, and for its heavy use of proxies

Answer-39918, Question-37628
One of my favorite uses of reflection is the below Java dump method. It takes any object as a parameter and uses the Java reflection API print out every field name and value.

Answer-9155976, Question-37628
As per my understanding: Reflection allows programmer to access entities in program dynamically. i.e. while coding an application if programmer is unaware about a class or its methods, he can make use of such class dynamically (at run time) by using reflection. It is frequently used in scenarios where a class name changes frequently. If such a situation arises, then it is complicated for the programmer to rewrite the application and change the name of the class again and again. Instead, by using reflection, there is need to worry about a possibly changing class name.

Answer-11159447, Question-37628
Example : Take for example a remote application which gives your application an object which you obtain using their API Methods . Now based on the object you might need to perform some sort of computation . The provider guarantees that object can be of 3 types and we need to perform computation based on what type of object . So we might implement in 3 classes each containing a different logic .Obviously the object  information is available in runtime so you cannot statically code to perform computation hence reflection is used to instantiate the object of the class that you require to perform the computation based on the object received from the provider .

Answer-14277235, Question-37628
Java Reflection makes it possible to inspect classes, interfaces, fields and methods at runtime, without knowing the names of the classes, methods etc. at compile time. Mostly at framework level the maximum benefits of reflection can be achieved. The byte code that is compiled if needs extra modification at run time for examination, modification, addition of more byte code within itself, or another program or another framework at method level, instance variable level, constructor level, annotation level reflection can be useful. Suppose you have a method add(Int a,int b). The equivalent byte code is suppose B1. If suppose you have 1000 methods named add in your system. Now you want to check the value of parameter a and b before method add is called. So, you can glue your code to another program or framework that uses reflection to dynamically examine the byte code value using Object.getClass.getMethod(). There are several classes for examining. It can add more operation before method add is called. But, the program itself or another program or framework does not know about the object that has a method named add. Mostly in dependency injection, aspect oriented programming use of reflection is mostly used.

Answer-16406534, Question-37628
Reflection has many uses. The one I am more familiar with, is to be able to create code on the fly.  IE: dynamic classes, functions, constructors - based on any data   (xml/array/sql results/hardcoded/etc..)

Answer-17531269, Question-37628
Reflection allows instantiation of new objects, invocation of methods, and get/set operations on class variables dynamically at run time without having prior knowledge of its implementation. In above example the null parameter is the object you want to invoke the method on. If the method is static you supply null. If the method is not static, then while invoking you need to supply a valid MyObject instance instead of null. Reflection also allows you to access private member/methods of a class: . For inspection of classes (also know as introspection) you don't need to import the reflection package (java.lang.reflect). Class metadata can be accessed through java.lang.Class. Reflection is a very powerful API but it may slow down the application if used in excess, as it resolves all the types at runtime.

Answer-25721335, Question-37628
Java Reflection is quite powerful and can be very useful. Java Reflection makes it possible to inspect classes, interfaces, fields and methods at runtime, without knowing the names of the classes, methods etc. at compile time. It is also possible to instantiate new objects, invoke methods and get/set field values using reflection. A quick Java Reflection example to show you what using reflection looks like: This example obtains the Class object from the class called MyObject. Using the class object the example gets a list of the methods in that class, iterates the methods and print out their names. Exactly how all this works is explained here Edit: After almost 1 year I am editing this answer as while reading about reflection I got few more uses of Reflection. Spring uses bean configuration such as: When the Spring context processes this < bean > element, it will use Class.forName(String) with the argument "com.example.Foo" to instantiate that Class.  It will then again use reflection to get the appropriate setter for the < property > element and set its value to the specified value. Junit uses Reflection especially for testing Private/Protected methods. For Private methods, For private fields,

Answer-25953852, Question-37628
I just want to add some point to all that was listed. With Reflection API you can write universal toString() method for any object. It is useful at debugging. Here is some example:

Answer-26424561, Question-37628
Uses of Reflection Reflection is commonly used by programs which require the ability to examine or modify the runtime behavior of applications running in the Java virtual machine. This is a relatively advanced feature and should be used only by developers who have a strong grasp of the fundamentals of the language. With that caveat in mind, reflection is a powerful technique and can enable applications to perform operations which would otherwise be impossible. Extensibility Features An application may make use of external, user-defined classes by creating instances of extensibility objects using their fully-qualified names. Class Browsers and Visual Development Environments A class browser needs to be able to enumerate the members of classes. Visual development environments can benefit from making use of type information available in reflection to aid the developer in writing correct code. Debuggers and Test Tools Debuggers need to be able to examine private members on classes. Test harnesses can make use of reflection to systematically call a discoverable set APIs defined on a class, to insure a high level of code coverage in a test suite. Drawbacks of Reflection Reflection is powerful, but should not be used indiscriminately. If it is possible to perform an operation without using reflection, then it is preferable to avoid using it. The following concerns should be kept in mind when accessing code via reflection. Performance Overhead Because reflection involves types that are dynamically resolved, certain Java virtual machine optimizations can not be performed. Consequently, reflective operations have slower performance than their non-reflective counterparts, and should be avoided in sections of code which are called frequently in performance-sensitive applications. Security Restrictions Reflection requires a runtime permission which may not be present when running under a security manager. This is in an important consideration for code which has to run in a restricted security context, such as in an Applet. Exposure of Internals Since reflection allows code to perform operations that would be illegal in non-reflective code, such as accessing private fields and methods, the use of reflection can result in unexpected side-effects, which may render code dysfunctional and may destroy portability. Reflective code breaks abstractions and therefore may change behavior with upgrades of the platform. source: The Reflection API

Answer-26771874, Question-37628
simple example for reflection. In a Chess Game, you do not know what will be moved by the user at run time. reflection can be used to call methods which are already implemented at run time.

Answer-34149864, Question-37628
Reflection is to let object to see their appearance. This argument seems nothing to do with reflection. In fact, this is the "self-identify" ability. Reflection itself is a word for such languages that lack capability of self-knowledge and self-sensing as Java and C#. Because they do not have the capability of self-knowledge, when we want to observe how it looks like, we must have another thing to reflect how it looks like. Excellent dynamic languages such as Ruby and Python can perceive the reflection of their own without the help of other individuals. We can say that the object of Java cannot perceive how it looks like without a mirror, which is an object of the reflection class, but object in Python can perceive it without a mirror. So that's why we need reflection in Java.

Answer-35379881, Question-37628
From java documentation page java.lang.reflect package provides classes and interfaces for obtaining reflective information about classes and objects. Reflection allows programmatic access to information about the fields, methods and constructors of loaded classes, and the use of reflected fields, methods, and constructors to operate on their underlying counterparts, within security restrictions. AccessibleObject allows suppression of access checks if the necessary ReflectPermission is available. Classes in this package, along with java.lang.Class accommodate applications such as debuggers, interpreters, object inspectors, class browsers, and services such as Object Serialization and JavaBeans that need access to either the public members of a target object (based on its runtime class) or the members declared by a given class It includes following functionality. Obtaining Class objects, Examining properties of a class (fields, methods, constructors), Setting and getting field values, Invoking methods, Creating new instances of objects. Have a look at this documentation link for the methods exposed by Class class. From this article (by Dennis Sosnoski, President, Sosnoski Software Solutions, Inc)  and this article (security-explorations pdf): I can see considerable drawbacks than uses of using Reflection User of Reflection: It provides very versatile way of dynamically linking program components It is useful for creating libraries that work with objects in very general ways Drawbacks of Reflection: Reflection is much slower than direct code when used for field and method access. It can obscure what's actually going on inside your code It bypasses the source code can create maintenance problems Reflection code is also more complex than the corresponding direct code It  allows violation of key Java security constraints such as data access protection and type safety General abuses: Loading of restricted classes, Obtaining references to constructors, methods or fields of a restricted class, Creation of new object instances, methods invocation, getting or setting field values of a restricted class. Have a look at this SE question regarding abuse of reflection feature: How do I read a private field in Java? Summary: Insecure use of its functions conducted from within a system code can also easily lead to the compromise of a Java security model. So use this feature sparingly

Answer-36877237, Question-37628
Reflection gives you ability to write more generic code . It allows you to create object at runtime and call its method at runtime. Hence the program can be made higly parameterised. It also allow to introspect the object and class to detect its variables and method exposed to outer world.

Answer-42945244, Question-37628
Reflection is an API which is used to examine or modify the behaviour of methods, classes, interfaces at runtime. The required classes for reflection are provided under java.lang.reflect package. Reflection gives us information about the class to which an object belongs and also the methods of that class which can be executed by using the object. Through reflection we can invoke methods at runtime irrespective of the access specifier used with them. The java.lang and java.lang.reflect packages provide classes for java reflection. Reflection can be used to get information about  Class The getClass() method is used to get the name of the class to which an object belongs. Constructors The getConstructors() method is used to get the public constructors of the class to which an object belongs. Methods The getMethods() method is used to get the public methods of the class to which an objects belongs. The Reflection API is mainly used in: IDE (Integrated Development Environment) e.g. Eclipse, MyEclipse, NetBeans etc. Debugger and Test Tools etc. Advantages of Using Reflection: Extensibility Features: An application may make use of external, user-defined classes by creating instances of extensibility objects using their fully-qualified names. Debugging and testing tools: Debuggers use the property of reflection to examine private members on classes. Drawbacks: Performance Overhead: Reflective operations have slower performance than their non-reflective counterparts, and should be avoided in sections of code which are called frequently in performance-sensitive applications. Exposure of Internals: Reflective code breaks abstractions and therefore may change behaviour with upgrades of the platform. Ref: Java Reflection javarevisited.blogspot.in

Answer-43302409, Question-37628
As name itself suggest it reflects what it holds for example class method,etc apart from providing feature to invoke method creating instance dynamically at runtime. It is used by many frameworks and application under the wood to invoke services without actually knowing the code.

Answer-39989, Question-37692
You should take a look at ICProject, especially the getOutputEntries and getAllSourceRoots operations. This tutorial has some brief examples too. I work with JDT so thats pretty much what I can do. Hope it helps :)

Answer-38224, Question-37976
Sure there is. This is all you need to do: Go to Disable the checkbox next to 'Use same settings for all file types'   The 'XML' tab should become enabled. Click it and set the 'tab' (and probably 'indent') size to 2.

Answer-6286495, Question-37976
In IntelliJ IDEA 10.0.3 it's  File > Settings > Code Style > General

Answer-26929608, Question-37976
Note: make sure to not use the same file in two project, or your settings might revert to the default (4 spaces), instead of the custom XML tab indent size. See bug IDEA-130878, for the latest IntelliJ IDEA 14 (Oct 2014)

Answer-38098, Question-38068
No. Though, groovy, a JVM language, is dynamically typed and would let you write:

Answer-38102, Question-38068
There's the pseudo-typedef antipattern...  Good stuff, drink up! ;-) As the article notes, this technique has some serious issues, primarily that this "typedef" is actually a separate class and thus cannot be used interchangeably with either the type it extends or other similarly defined types.

Answer-38116, Question-38068
In a generic method, you can use a limited form of type inferrence to avoid some repetitions. Example: if you have the function you can use: instead of:

Answer-38118, Question-38068
Use Factory Pattern for creation of Generics: Method Sample:

Answer-39325, Question-38068
The pseudo-typedef antipattern mentioned by Shog9 would work - though it's not recommended to use an ANTIPATTERN - but it does not address your intentions. The goal of pseudo-typedef is to reduce clutter in declaration and improve readability.  What you want is to be able to replace a group of generics declarations by one single trade. I think you have to stop and think: "in witch ways is it valuable?". I mean, I can't think of a scenario where you would need this. Imagine class A: Imagine now that I want to change the 'values' field to a Map. Why would exist many other fields scattered through the code that needs the same change? As for the operations that uses 'values' a simple refactoring would be enough.

Answer-39868, Question-39108
Are you trying to show the JDialog indepentently of the annimation? In order to get the order set properly, you may need to bundle those actions in a runnable that is passed to the EDT at once. eg: It may be best to subclass JDialog so that you can just add the doAnnimation() logic to the setVisible(..) or show() method before calling the superclass implementation.   Finally, I imagine you'll need to set the dimensions of the dalog manually --  I don't remember if Java will know the actual size of the dialog before it is shown, so you may get some useless information for your annimation if you query the size before showing it.

Answer-42959, Question-39108
You may be able to take @rcreswick's answer and expand on it a little to make it work. It's pretty ugly and would have to be invoked in place of the basic showDialog() call, but it should work.

Answer-92610, Question-39108
Maybe you have a look at the SwingWorker Project which is included in JSE 6. (Link to SwingWorker) In the book "Filthy Rich Client" that I am reading at the moment they use this tool a lot. Maybe you can find a hint in the examples on the books website: http://filthyrichclients.org/

Answer-3662376, Question-39108
One possibility is to paint your own dialog on the Glass Pane.  Then you have full control of the dialog and can paint whatever you want.  Here's a tutorial on creating animations on the Glass Pane.

Answer-39309, Question-39194
A Java application bundle on OS X is nothing more than a directory containing your .jars and a number of configuration files.  The SetFile tool sets a custom HFS filesystem property on the directory to tell finder that it is an app, but giving it a ".app" extension serves the same purpose.  I don't think there's anything stopping you from building one on, say, Windows, though of course you have no way of testing that it works, but if you are able to test it at least once on a real Mac, you could then conceivably update the .jars within it on Windows to reflect code changes without too much difficulty. Have a look at the Bundle Programming Guide for more info.

Answer-39356, Question-39194
One way is to generate a zip file with the App using for example Ant. In ant you can specify that the file in Contents/MacOS should have execute-permissions using something like filemode="755".

Answer-39362, Question-39194
Having worked on the Mac port of NITE, I can say that jar packages for other platforms should work equally well on Mac. I would still recommend finding a mac for testing (or even announcing mac support was in beta ;-) ) as we discovered a few mac-only quirks during the port (to go with the windows- and linux- only quirks we'd already discovered)

Answer-40210, Question-39194
Technically, you don't need a Mac.  Applications in OS X just require a specific folder structure and an XML file.  However, the Mac has a really nice tool called Jar Bundler.  In addition to setting up the bundle directories and XML file, it creates a C executable that launches your java application via JNI.  This is nice because the process name matches the application name. I believe that you could have someone generate an application bundle for you once, and then check in the files to your project.  At build time, all you would need to do is copy your jar files to the appropriate locations and maybe update the XML file.

Answer-39431, Question-39391
No it does not. But if the protocol of the URL is HTTP, you'll get a HttpURLConnection as a return object. This class has a setRequestMethod method to specify which HTTP method you want to use.  If you want to do more sophisticated stuff you're probably better off using a library like Jakarta HttpClient.

Answer-39449, Question-39391
If you retrieve the URLConnection object using openConnection() it doesn't actually start communicating with the server.  That doesn't happen until you get the stream from the URLConnection().  When you first get the connection you can add/change headers and other connection properties before actually opening it. URLConnection's life cycle is a bit odd.  It doesn't send the headers to the server until you've gotten one of the streams.  If you just get the input stream then I believe it does a GET, sends the headers, then lets you read the output.  If you get the output stream then I believe it sends it as a POST, as it assumes you'll be writing data to it (You may need to call setDoOutput(true) for the output stream to work).  As soon as you get the input stream the output stream is closed and it waits for the response from the server. For example, this should do a POST: While this would do a GET: URLConnection will also do other weird things.  If the server specifies a content length then URLConnection will keep the underlying input stream open until it receives that much data, even if you explicitly close it.  This caused a lot of problems for us as it made shutting our client down cleanly a bit hard, as the URLConnection would keep the network connection open.  This probably probably exists even if you just use getStream() though.

Answer-40390, Question-40376
Perhaps Runtime#addShutdownHook ?

Answer-41157, Question-40376
The JVM responds to signals on its own. Some will cause the JVM to shutdown gracefully, which includes running shutdown hooks. Other signals will cause the JVM to abort without running shutdown hooks. Shutdown hooks are added using Runtime.addShutdownHook(Thread). I don't think the JDK provides an official way to handle signals within your Java application. However, I did find this IBM article, which describes using some undocumented sun.misc.Signal class to do exactly that. The article dates from 2002 and uses JDK 1.3.1, but I've confirmed that the sun.misc.Signal class still exists in JDK 1.6.0.

Answer-41063, Question-41056
I've used JDEE on several projects.  It handles Code Completion.  I've never used it for debugging or browsing docs, but it's a big step up from a basic text editor.

Answer-2123448, Question-41056
I've had good success with jdibug for debugging Java code with Emacs.

Answer-2535018, Question-41056
For javadoc I found http://javadochelp.sourceforge.net/index.html to be the best.  Exuberant ctags is your best friend when it comes to navigation.

Answer-41241, Question-41233
The wiki lists some more wrappers: Java wrapper (around a SWIG interface): http://tk-software.home.comcast.net/ A good tutorial to use JDBC driver for SQLite. (it works at least !) http://www.ci.uchicago.edu/wiki/bin/view/VDS/VDSDevelopment/UsingSQLite Cross-platform JDBC driver which uses embedded native SQLite libraries on Windows, Linux, OS X, and falls back to pure Java implementation on other OSes: https://github.com/xerial/sqlite-jdbc (formerly zentus) Another Java - SWIG wrapper. It only works on Win32. http://rodolfo_3.tripod.com/index.html sqlite-java-shell: 100% pure Java port of the sqlite3 commandline shell built with NestedVM. (This is not a JDBC driver). SQLite JDBC Driver for Mysaifu JVM: SQLite JDBC Driver for Mysaifu JVM and SQLite JNI Library for Windows (x86) and Linux (i386/PowerPC).

Answer-593137, Question-41233
I found your question while searching for information with SQLite and Java. Just thought I'd add my answer which I also posted on my blog. I have been coding in Java for a while now. I have also known about SQLite but never used it Well I have used it through other applications but never in an app that I coded. So I needed it for a project this week and it's so simple use! I found a Java JDBC driver for SQLite. Just add the JAR file to your classpath and import java.sql.* His test app will create a database file, send some SQL commands to create a table, store some data in the table, and read it back and display on console. It will create the test.db file in the root directory of the project.  You can run this example with java -cp .:sqlitejdbc-v056.jar Test.

Answer-594511, Question-41233
I understand you asked specifically about SQLite, but maybe HSQL database would be a better fit with Java. It is written in Java itself, runs in the JVM, supports in-memory tables etc. and all that features make it quite usable for prototyping and unit-testing.

Answer-1257831, Question-41233
There is a new project SQLJet that is a pure Java implementation of SQLite. It doesn't support all of the SQLite features yet, but may be a very good option for some of the Java projects that work with SQLite databases.

Answer-5360661, Question-41233
Bernie's post is very helpful. Couldn't vote up (don't have enough reputation :( ). But it Helped a lot. Just to reiterate! http://www.zentus.com/sqlitejdbc/ Here you can find the latest SQLite JDBC jar. Just add the jar into you classpath and you are done! :) You can run Bernie's sample code to test if everything is fine.  http://souptonuts.sourceforge.net/readme_sqlite_tutorial.html http://www.sqlite.org/lang.html Here you can find some help on SQL syntax for SQLite. Cheers to SQLite :)

Answer-8797310, Question-41233
The example code leads to a memory leak in Tomcat (after undeploying the webapp, the classloader still remains in memory) which will cause an outofmemory eventually. The way to solve it is to use the sqlite-jdbc-3.7.8.jar; it's a snapshot, so it doesn't appear for maven yet.

Answer-9046459, Question-41233
Typo: java -cp .:sqlitejdbc-v056.jar Test should be: java -cp .:sqlitejdbc-v056.jar; Test notice the semicolon after ".jar" i hope that helps people, could cause a lot of hassle

Answer-9426806, Question-41233
When you compile and run the code, you should set the classpath options value. Just like the following: Please pay attention to "." and the sparate ";"(win, the linux is ":")

Answer-12278261, Question-41233
David Crawshaw project(sqlitejdbc-v056.jar) seems out of date and last update was Jun 20, 2009,  source here I would recomend Xerials fork of Crawshaw sqlite wrapper.  I replaced sqlitejdbc-v056.jar  with Xerials sqlite-jdbc-3.7.2.jar file without any problem.  Uses same syntax as in Bernie's answer and is much faster and with latest sqlite library. What is different from Zentus's SQLite JDBC? The original Zentus's SQLite JDBC driver   http://www.zentus.com/sqlitejdbc/ itself is an excellent utility for   using SQLite databases from Java language, and our SQLiteJDBC library   also relies on its implementation. However, its pure-java version,   which totally translates c/c++ codes of SQLite into Java, is   significantly slower compared to its native version, which uses SQLite   binaries compiled for each OS (win, mac, linux). To use the native version of sqlite-jdbc, user had to set a path to   the native codes (dll, jnilib, so files, which are JNDI C programs) by   using command-line arguments, e.g., -Djava.library.path=(path to the   dll, jnilib, etc.), or -Dorg.sqlite.lib.path, etc. This process was   error-prone and bothersome to tell every user to set these variables.   Our SQLiteJDBC library completely does away these inconveniences. Another difference is that we are keeping this SQLiteJDBC libray   up-to-date to the newest version of SQLite engine, because we are one   of the hottest users of this library. For example, SQLite JDBC is a   core component of UTGB (University of Tokyo Genome Browser) Toolkit,   which is our utility to create personalized genome browsers. EDIT : As usual when you update something, there will be problems in some obscure place in your code(happened to me). Test test test =)

Answer-13273626, Question-41233
sqlitejdbc code can be downloaded using git from https://github.com/crawshaw/sqlitejdbc.  Note: Makefile requires curl binary to download sqlite libraries/deps.

Answer-41694, Question-41686
Are you sure that the image isn't coming through garbled or that you aren't dropping some packets on the way in.

Answer-41748, Question-41686
I don't know what difference it makes, but there seems to be a mismatch of method signatures.  The getBytes() method called in your doPost() method has only one argument: while the method source you included has two arguments: Hope that helps.

Answer-41753, Question-41686
Can you perform a checksum on your original file, and the uploaded file and see if there is any immediate differences? If there are then you can look at performing a diff, to determine the exact part(s) of the file that are missing changed. Things that pop to mind is beginning or end of stream, or endianness.

Answer-42202, Question-41686
One thing I don't like is here in this block from StreamUtils.getBytes(): At line 6, it writes the entire buffer, no matter how many bytes are read in.  I am not convinced this will always be the case.  It would be more correct  like this: Note the 'else' on line 5, along with the two additional parameters (array index start position and length to copy) on line 6. I could imagine that for larger files, like images, the buffer returns before it is filled (maybe it is waiting for more).  That means you'd be unintentionally writing old data that was remaining in the tail end of the buffer.  This is almost certainly happening most of the time at EoF, assuming a buffer > 1 byte, but extra data at EoF is probably not the cause of your corruption...it is just not desirable.

Answer-42470, Question-41686
I'd just use commons io Then you could just do an IOUtils.copy(InputStream, OutputStream); It's got lots of other useful utility methods.

Answer-43549, Question-42828
have you looked into using facelets? It lets you get rid of the whole JSF / JSP differences (it's an alternate and superior view controller). It also supports great design-time semantics with the jsfc tag... gets translated internally to the correct JSF stuff, so you can work with your existing tools.

Answer-63883, Question-42828
To solve this one I'd probably create a JSF fragment that only includes your form, then use a  tag to include it in my JSF page.   That solution is probably a little fragile depending on your environment though. EDIT: See Chris Hall's answer, FacesContext is not available outside the FacesServlet.

Answer-69050, Question-42828
Create a custom JSP tag handler.  You can then retrieve the bean from session scope, and then initialize it on the fly.  See this tutorial for more details.

Answer-71584, Question-42828
Actually, I've resolved this by removing bean from session, so it has to be generated again when jsf page is called. Then I pick up get parameters from a request in constructor.

Answer-72213, Question-42828
You can retrieve a managed bean inside of a tag library using something like this: However, you'd need to use the tag library from one of your JSF pages.  FacesContext.getCurrentInstance() returns null when it's called outside of the FacesServlet.

Answer-43508, Question-43199
Everything seems to be correct. Are you really sure getPassword() returns garbage? Isn't it someone else (your editor, OS, database,...) that doesn't know that it's a unicode string when it displays it to you while the password may be perfectly okay? What exactly makes you think it's a garbage? I would also make sure there's no strange encoding set in the .application config file

Answer-47149, Question-43199
I found the problem. Tomcat was mangling the parameters before Tapestry or my page class even had a crack at it. Creating a servlet filter that enforced the desired character encoding fixed it. CharacterEncodingFilter.java web.xml

Answer-44230, Question-44224
You should be able to do everything related to Oracle using JDBC, so make sure you bone up on that API.  Other than that, it depends on the type of application.  Standalone apps may use Swing (the Java UI toolkit) or in the future JavaFX, which is supposed to make Swing obsolete and may do so in a few years.  Web/enterprisey apps will make use of Java Enterprise Edition, so take a look at the servlet API, and if the app uses Enterprise JavaBeans, look at the Java Persistence API, which you would probably be using instead of JDBC. I haven't used JDeveloper, but I haven't found anything wrong with the free IDEs like Eclipse or Netbeans, and my personal favorite is JetBrains's IntelliJ IDEA.

Answer-44235, Question-44224
There's really nothing specific you need to learn to be an oracle devloper per se.  Obviously you need to learn oracle sql syntax, and all the standard rdbms theory that goes along with database programming in general.  The Java libs for database support are pretty easy to pick up and run with.  I'm sure you can find a tutorial on the web by a quick google search. As for IDE I'd recommend Eclipse.  It's a bit cumbersome at times, but the number of plug-ins available is staggering, and it has great refactoring and code completion support.

Answer-44251, Question-44224
You're question is a little bit too vague in order to give a proper answer...  If you plan to query the Oracle Database from an External Java Program (Either within a Swing Application or an Application Server) then you need to learn 2 core APIs: JDBC (Java Database Connectivity) JPA (Java Persistence API) JDBC is the core API that allows a Java Program to interact with any RDBMS so you should at least know how it works so whenever you have to dig into low-level code, you will actually know what's happening. JPA is the latest Java API for Persistence which basically allows one to map Plain Old Java Object (AKA PoJo) to RDBMS Table Structures. There are multiple known implementation available but I would recommend Hibernate or TopLink as good starting points. After that, you can start to dig into other known frameworks like the Spring Framework for  some other RDBMS related APIs.

Answer-44259, Question-44224
You can use JDeveloper and try to find some tutorials for it (I actually had some from my university). It integrates well with rest of Oracle stack (db and application server). Down site is that although you can download some developers editions to run for personal usage, running Oracle db + Oracle application server + JDeveloper on a machine that has less then 4GB of ram and one core is not really peasant experience.

Answer-44271, Question-44224
Expert Oracle JDBC Programming is a book aimed directly at developers who want to use Java with Oracle.  Before you make even that small monetary investment though, you might want to check out the JDBC tutorial published by Sun.

Answer-45989, Question-44224
To become an Oracle Developer there is a bit more to learn than jdbc. You should take a look at the Oracle web site. It is kind of slow and not very intuitive but has a lot of good information. There are OUGs that have good info as well. If you just want to access Oracle via JAVA then you should use a framework such as Spring. Takes away the pain of jdbc. Lets you write sql and map it to objects. If you don't know PL/SQL it might be good to learn what it is.  My two cents from working with Oracle for the past 7 yrs.

Answer-2010374, Question-44224
Your question is very simple so I have listed a few simple steps to start developing a Java application using Oracle technologies. Install Oracle XE Database. Install [JDeveloper]. Choose the install with Weblogic if you are developing a J2EE application. Build and run a jdbc application using the [sample code] or use the wizard in JDeveloper. Install SQL Developer for writing stored procedures. Steps 3. and 4. are optional. You now have everything you need to build either a proof of concept or an enterprise grade database application, using simple wizards and without re-inventing the wheel. You mentioned developing an Oracle Application. It's best to leave the development of Oracle's packaged Application to Oracle itself but if you want to integrate your custom java application with Oracle's packaged application then use Oracle's SOA Suite. Cheers KB

Answer-44828, Question-44824
Zeroconf and other discovery protocols are similarly referred to as the greatest thing since sliced bread; it's just that the flavor keeps changing.

Answer-44901, Question-44824
Things have definitely quited down for the idea.  Which is strange since you'd think its goals are even more relevant now. http://www.jini.org/wiki/Category:News

Answer-46191, Question-44824
Check out GigaSpaces. It's a quite successful Jini/Javaspaces implementation. I think Jini has a great model, but it is stuck with Java. Web-services is more appealing because it works with standarized protocols, even though Jini service discovery is more natural.

Answer-1041963, Question-44824
The jewel in the crown of Jini was it's JavaSpaces service IMO. Sad that Sun seem to have abandoned it. It still exists as Apache River though, however I'm not sure if it's under active development.

Answer-1041967, Question-44824
My two cents...  Jini was/is nice, but I think it tried to be a Java-centric CORBA back in the day when corporations were beginning to be reluctant regarding paying the big bucks for what CORBA brought to the table.  WS-* specs began to acquire the "accepted-solution" mind-share in the industry.  I think there was a small window where Jini could have grabbed substantial market share, but it never happened.  Sun wanted too much money for what Jini brought to the table compared to other alternatives.  I would love to hear from folks that disagree!  My opinion is that Jini is sound tech, but business-wise has no future in the enterprise.  It may find a niche elsewhere, depending on what Oracle decides to do with it.

Answer-2177820, Question-44824
Jini was an amazing technology. The only reason pushed EJB systems was that it allowed Sun to sell more hardware as EJB ran best on highpowered machines (due to shared state and database access). At the time (1999) Jini allowed much better scalability which ran well on commodity hardware, so it made sense for Sun to not promote Jini. Its a shame as I kept wondering when someone would release an Open Source easy to use Jini server like JBoss did with J2EE. I did however save companies alot of time and money by using the Jini techniques (based on Linda TupleSpaces) and applying them to writing software systems by using Tuple Spaces implemented in other ways.

Answer-9028579, Question-44824
old question, but JINI has been given to Apache and is now the Apache River project: http://river.apache.org

Answer-44916, Question-44912
Short story: no.          Introduction The newest version of the Microsoft Visual J++ development environment   supports a language construct called delegates or bound method   references. This construct, and the new keywords delegate and   multicast introduced to support it, are not a part of the JavaTM   programming language, which is specified by the Java Language   Specification and amended by the Inner Classes Specification included   in the documentation for the JDKTM 1.1 software.  It is unlikely that the Java programming language will ever include   this construct. Sun already carefully considered adopting it in 1996,   to the extent of building and discarding working prototypes. Our   conclusion was that bound method references are unnecessary and   detrimental to the language. This decision was made in consultation   with Borland International, who had previous experience with bound   method references in Delphi Object Pascal.  We believe bound method references are unnecessary because another   design alternative, inner classes, provides equal or superior   functionality. In particular, inner classes fully support the   requirements of user-interface event handling, and have been used to   implement a user-interface API at least as comprehensive as the   Windows Foundation Classes.  We believe bound method references are harmful because they detract   from the simplicity of the Java programming language and the   pervasively object-oriented character of the APIs. Bound method   references also introduce irregularity into the language syntax and   scoping rules. Finally, they dilute the investment in VM technologies   because VMs are required to handle additional and disparate types of   references and method linkage efficiently.

Answer-44928, Question-44912
Not really, no. You may be able to achieve the same effect by using reflection to get Method objects you can then invoke, and the other way is to create an interface with a single 'invoke' or 'execute' method, and then instantiate them to call the method your interested in (i.e. using an anonymous inner class). You might also find this article interesting / useful : A Java Programmer Looks at C# Delegates

Answer-45009, Question-44912
While it is nowhere nearly as clean, but you could implement something like C# delegates using a Java Proxy.

Answer-2075606, Question-44912
I have implemented callback/delegate support in Java using reflection.  Details and working source are available on my website. How It Works There is a principle class named Callback with a nested class named WithParms.  The API which needs the callback will take a Callback object as a parameter and, if neccessary, create a Callback.WithParms as a method variable.  Since a great many of the applications of this object will be recursive, this works very cleanly. With performance still a high priority to me, I didn't want to be required to create a throwaway object array to hold the parameters for every invocation - after all in a large data structure there could be thousands of elements, and in a message processing scenario we could end up processing thousands of data structures a second. In order to be threadsafe the parameter array needs to exist uniquely for each invocation of the API method, and for efficiency the same one should be used for every invocation of the callback; I needed a second object which would be cheap to create in order to bind the callback with a parameter array for invocation.  But, in some scenarios, the invoker would already have a the parameter array for other reasons.  For these two reasons, the parameter array does not belong in the Callback object.  Also the choice of invocation (passing the parameters as an array or as individual objects) belongs in the hands of the API using the callback enabling it to use whichever invocation is  best suited to its inner workings. The WithParms nested class, then, is optional and serves two purposes, it contains the parameter object array needed for the callback invocations, and it provides 10 overloaded invoke() methods (with from 1 to 10 parameters) which load the parameter array and then invoke the callback target. What follows is an example using a callback to process the files in a directory tree.  This is an initial validation pass which just counts the files to process and ensure none exceed a predetermined maximum size. In this case we just create the callback inline with the API invocation.  However, we reflect the target method out as a static value so that the reflection is not done every time. IoUtil.processDirectory(): This example illustrates the beauty of this approach - the application specific logic is abstracted into the callback, and the drudgery of recursively walking a directory tree is tucked nicely away in a completely reusable static utility method.  And we don't have to repeatedly pay the price of defining and implementing an interface for every new use. Of course, the argument for an interface is that it is far more explicit about what to implement (it's enforced, not simply documented) - but in practice I have not found it to be a problem to get the callback definition right. Defining and implementing an interface is not really so bad (unless you're distributing applets, as I am, where avoiding creating extra classes actually matters), but where this really shines is when you have multiple callbacks in a single class. Not only is being forced to push them each into a separate inner class added overhead in the deployed application, but it's downright tedious to program and all that boiler-plate code is really just "noise".

Answer-3953773, Question-44912
Have you read this : Delegates are a useful construct in event-based systems. Essentially   Delegates are objects that encode a method dispatch on a specified   object. This document shows how java inner classes provide a more   generic solution to such problems. What is a Delegate? Really it is very similar to a pointer to member   function as used in C++. But a delegate contains the target object   alongwith the method to be invoked. Ideally it would be nice to be   able to say: obj.registerHandler(ano.methodOne);  ..and that the method methodOne would be called on ano when some specific event was received. This is what the Delegate structure achieves. Java Inner Classes  It has been argued that Java provides this   functionality via inner classes and thus does not need the additional   Delegate construct. At first glance this seems correct but at the same time a nuisance.   Because for many event processing examples the simplicity of the   Delegates syntax is very attractive. General Handler  However, if event-based programming is used in a more   pervasive manner, say, for example, as a part of a general   asynchronous programming environment, there is more at stake. In such a general situation, it is not sufficient to include only the   target method and target object instance. In general there may be   other parameters required, that are determined within the context when   the event handler is registered. In this more general situation, the java approach can provide a very   elegant solution, particularly when combined with use of final   variables: final * final * final  Got your attention? Note that the final variables are accessible from within the anonymous   class method definitions. Be sure to study this code carefully to   understand the ramifications. This is potentially a very powerful   technique. For example, it can be used to good effect when registering   handlers in MiniDOM and in more general situations. By contrast, the Delegate construct does not provide a solution for   this more general requirement, and as such should be rejected as an   idiom on which designs can be based.

Answer-9232634, Question-44912
No, but they're fakeable using proxies and reflection: The nice thing about this idiom is that you can verify that the delegated-to method exists, and has the required signature, at the point where you create the delegator (although not at compile-time, unfortunately, although a FindBugs plug-in might help here), then use it safely to delegate to various instances. See the karg code on github for more tests and implementation.

Answer-9870470, Question-44912
No, but it has similar behavior, internally. In C# delegates are used to creates a separate entry point and they work much like a function pointer. In java there is no thing as function pointer (on a upper look) but internally Java needs to do the same thing in order to achieve these objectives. For example, creating threads in Java requires a class extending Thread or implementing Runnable, because a class object variable can be used a memory location pointer.

Answer-10137808, Question-44912
Depending precisely what you mean, you can achieve a similar effect (passing around a method) using the Strategy Pattern. Instead of a line like this declaring a named method signature: declare an interface: For concrete implementations of the method, define a class that implements the behaviour: Then wherever you would have had a SomeFunction delegate in C#, use an ISomeBehaviour reference instead: With anonymous inner classes, you can even avoid declaring separate named classes and almost treat them like real delegate functions. This should probably only be used when the implementation is very specific to the current context and wouldn't benefit from being reused. And then of course in Java 8, these do become basically lambda expressions:

Answer-13739321, Question-44912
Java doesn't have delegates and is proud of it :). From what I read here I found in essence 2 ways to fake delegates: 1. reflection; 2. inner class Reflections are slooooow! Inner class does not cover the simplest use-case: sort function. Do not want to go into details, but the solution with inner class basically is to create a wrapper class for an array of integers to be sorted in ascending order and an class for an array of integers to be sorted in descending order.

Answer-21788518, Question-44912
I know this post is old, but Java 8 has added lambdas, and the concept of a functional interface, which is any interface with only one method.  Together these offer similar functionality to C# delegates.  See here for more info, or just google Java Lambdas. http://cr.openjdk.java.net/~briangoetz/lambda/lambda-state-final.html

Answer-23485571, Question-44912
Yes & No, but delegate pattern in Java could be thought of this way. This video tutorial is about data exchange between activity - fragments, and it has great essence of delegate sorta pattern using interfaces.

Answer-45089, Question-45086
A quick Google turned up this for a toolkit. While I have never used it, it seems to be quite popular and solid. Not exactly a package, and not really rolling your own, but sort of in the middle.

Answer-45159, Question-45086
Take a look at Apache's Axis project. It's well supported on C++ (and Java) and if you have the good fortune to start with a good WSDL for the target service you'll be home-free.

Answer-45359, Question-45086
I'll vote up darkhelmet since gSoap would also be my recommendation. We're mostly a Java shop but with some C++ bits and gSoap has been our preferred SOAP integration way. It is indeed more work than your typical Java stacks but it seems solid.

Answer-84771, Question-45086
We went with gSOAP rather than Axis to avoid having a dependency on both a JRE and Axis just for building a C++ project.  It's worked ok, which is good since the gSOAP code is horrible and makes it very daunting to fix any bugs in it. A warning about gSOAP linking though: you can never use more than one WSDL in a single link object (executable, dll, shared object).  This is because some of the generated WSDL-specific functions have general names (e.g. soap_getfault()). Worse, with Unix ELF linking, these names will cause cross-linking between shared objects, so a FooService fault might be processed by the soap_getfault() for BarService, corrupting memory if the fault detail structures are different. The workaround for that is to make sure that nothing gSOAP-related is exposed outside the SO they are linked into.  This can be solved by giving gcc these definitions _both when linking the gSOAP library itself and linking your code: I solved it by putting them into a header file and forcing gcc to include that before anything else with -include fixsoaplink.h. A better way if you can take the effort might to change the default ELF visibility to hidden, and only export the symbols you want to (like dllimport/dllexport in VC).

Answer-92256, Question-45086
When I saw the generated code from gSOAP, I about had a heart attack. The fact that the user is required to do all of the memory management for each object just boggled my mind.  So, I sat down and did something probably stupid in the long term, but fairly satisfying in the short term... I wrote a program that wraps the gSOAP code with my own CPP classes that make the interface look more like I'd like it to look. I used Scoped Guards within each service method to hold onto memory, and since I'm dealing with all sorts of different types, I used a std::list<boost::any> to do it.  I have functions that make each object type that I need, and they put the actual memory into my list<any>.  It's had a few problems - mostly just configuration changes.  I'm generating thousands of classes now, talking to dozens of web services. I'm not sure I'd recommend my same path to anyone else...  I should probably bite the bullet and start trying to contribute to gSOAP, rather than maintain my own tool which is dependent on the output of gSOAP...

Answer-148822, Question-45086
Here's another issue with gSOAP we just discovered the hard way: it uses select() for all polling, so once you've got 1024 file descriptors open (64 on Windows?) it will trash the stack.  That results in either spurious errors where it is unable to send messages, to complete crashes of the application. The workaround, unless you're prepared to patch gSOAP itself, is to write your own network code and hook it in with soap->fconnect, ->fsend, ->frecv etc.

Answer-45595, Question-45424
 will do the trick. INPUT constant is defined in Action interface itself. It indicates that action needs more input. By calling page if you meant the page that took you to the action input page, then your input page will have to store HTTP header "Referer" in the request scope for the Action.

Answer-202693, Question-45424
You can use  a dynamic result in struts.xml. For instance: Then in your action, you create a field called next. So to invoke the action you will pass the name of the page that you want to forward to next. The action then returns "next" and struts will know which page to go to. There is a nicer explanation on this post: Stack Overflow

Answer-229951, Question-45424
I prefer the way when you navigating users by particular actions.  http://domain.com/myAction.action You could use some parameter as indicator, that you want to change current design: i.e. http://domain.com/myAction.action?changeDesign=silver_theme So then, you write some struts 2 interceptor, which logic is to check the presence of such parameter 'changeDesign', and this interceptor will do nessesary work of changing design and will control workflow. With interceptor you decouple your actions from crosscutting logic.

Answer-447845, Question-45424
My solution would involve one interface and one interceptor.  You implement the following interface for all actions to which you are likely to want to redirect: The interceptor simply ensures that the target is set, if required: From then on, once these two bits are in place and your actions implement the TargetAware interface (if you expect to have to redirect to them), then you have access to a target parameter in your JSPs whenever you need it.  Pass that parameter on to your VisualizationAction (which might as well implement also the TargetAware interface!), and on SUCCESS, redirect as explained by Vincent Ramdhanie: I did not try every single detail of this strategy.  In particular, beware of the notation surrounding the redirect result type (depending on your specific version of Struts2: 2.0.x and 2.1.x may differ on this...).

Answer-447914, Question-45424
ok, in your class it.___.web.actions.VisualizationAction, you must return a string value containing INPUT, then, on struts.xml you have to set something like this: this will lead you to the page you want. This should work, I've been working on struts2 along 2 months

Answer-45568, Question-45546
can you  use response.setStatus(403) ?

Answer-45585, Question-45546
Quickie If you are using plain JSP views (as is most common), then simply add somewhere in your view file. At the top is a nice place. Detail In MVC, i would always set this in the view, and in most cases with Spring-MVC, use the SimpleMappingExceptionResolver to present the correct view in response to a thrown runtime Exception.  For example: create and throw a PermissionDeniedException in your controller or service layer and have the exception resolver point to a view file permissionDenied.jsp. This view file sets the 403 status and shows the user an appropriate message. In your Spring bean XML file: If you need to implement a user login mechanism, take a look at Spring Security (formerly Acegi Security).

Answer-45620, Question-45546
Using an ExceptionResolver is a great way to go, but if you just want this to be view-independent, you could certainly make a call to response.sendError(HttpServletResponse.SC_FORBIDDEN, "AdditionalInformationIfAvailable"); in your Controller.

Answer-28883524, Question-45546
Create an Exception annotated with @ResponseStatus e.g. like this: Now just throw that Exception in your handler method and the response will have status 403.

Answer-38467064, Question-45546
You can also just throw This returns a 403 in the response header.

Answer-46790, Question-46758
One thing you can try is find a Java bytecode compiler for the language you're talking about (there are JVM compilers for all kinds of languages) and then decompile the bytecode back into Java using a decompiler like Jad.   This is fraught with peril.  The regenerated code will suck and will probably be unreadable.

Answer-48609, Question-46758
Google: ANTLR

Answer-48898, Question-46758
ADA to Java can be done with a find-and-replace!

Answer-790624, Question-46758
If you just want to use some legacy C/Pascal code, you could also use JNI to call it from Java.  If you want to run it in a Java applet or similar constrained environment, and it does not have to be very efficient, you can use NestedVM (which is a MIPS to Java bytecode converter) in conjunction with a gcc cross-compiler that compiles to MIPS). But don't expect to get readably Java code from that.

Answer-790659, Question-46758
The language conversion is fairly simple, but you will find the libraries are different. This is likely to be most of your work.

Answer-1879119, Question-46758
Any of those tools might help only if your non java code is not huge enough.  If its huge non java code and if you want to seriously translate it to java, then few things need to be thought of, its not just hundreds of lines of code, there is a design beneath it, there are few decisions taken by people beneath the code due to which certain problems might have been solved and few things have been working there. and investing time on any good translator won't be worth as it won't exist, it's not just syntax translation from one language to another. If its not so huge code, its better to re write in java, as it has so many APIs packages out of box, it might not be big deal, hiring few interns for this also might help.

Answer-4381267, Question-46758
Source-to-source migrations fall under the umbrella of Program Transformation. Program-Transformation.org tracks a bunch of tools that are useful for language recognition, analysis, and transformation. Here are few that are capable of source-to-source migrations: ASF+SDF Meta-Environment - As noted, there is no new development on this tool. Instead, the developers are focusing on Rascal. Rascal Meta Programming Language Stratego /XT TXL DMS Software Reengineering Toolkit (commercial) If you spend any time with one of the open source tools, you'll notice that even though they include source-to-source migration as a feature, it's hard to find working examples. I imagine this is because there's no such thing as a one-size-fits-all migration. Each project/team makes unique use of a language and can vary by libraries used, type complexity, idioms, style, etc. It makes sense to define some transformations per migration. This means a project must reach some critical mass before automatic migration is worth the effort. A few related documents: An introduction to Rascal - includes a migration between the toy language Pico and Assembly starting at page 94. Cracking the 500 Language Problem An Experiment in Automatic Conversion of Legacy Java Programs to C# (gated) - uses TXL

Answer-47148, Question-47145
Write custom appenders for log4j

Answer-717055, Question-47145
May I suggest that you leverage the existing SyslogAppender?

Answer-47721, Question-47555
We converted a home-grown web framework to stripes in about a week. We're using it in production at this time and it's a great framework. The community is extremely helpful, and the framework doesn't get in your way. It can be extended in many places to change the behavior as you see fit. The url binding feature is awesome as well. We implemented a robust security framework using annotations and interceptors. We're using spring for dependency injection and stripes has excellent support for that. I'd definitely use the new 1.5 release if you're going to use it. I'm a huge fan of the framework. I came from a struts background and it's the exact framework I was looking for. The other developers on our team really enjoy using the stripes framework. I just bought the stripes beta book from the pragmatic programmer's site. It's a great resource on Stripes 1.5.

Answer-58304, Question-47555
We use stripes now on all our production sites, and have been for about a year now. It is an awesome product compared to struts, which we used to use before that. Just the fact that there are literally no XML config files and that you can set it all up with a minimal amount of classes and annotations is awesome. In terms of scaling & speed it actually seems to be better than struts, and my guess would be because there are less layers involved. The code you end up with is a lot cleaner as well, because you don't have to go off to seperate XML files to find out where redirects are going. We use it with an EJB3 backend, and the two seem to work really well together, because you can use your EJB POJO inside your actionBean object, without needing a form object like in struts. In our evaluation we considered an alpha version of struts (that supported annotations) and a lot of other frameworks, but stripes won because of it's superior documentation, stability and clean-ness. Couldn't figure out how to leave a comment: so to answer your second question we haven't encountered a single bug in Stripes that I know of. This is quite impressive for an open source framework. I haven't tried the latest version (1.5) yet, but 1.4.x is very stable.

Answer-234768, Question-47555
I also came from a Struts and JSF background into Stripes.  I went from a large enterprise environment that used mostly struts and JSF on newer projects, to a smaller environment that did all their J2EE in Stripes.   Seems like Stripes gives you what you want in a Web Framework without getting in the way too much.  Not much configuration is necessary, as others have already mentioned.  Very quick development and allows you to focus on presentation etc. instead of hassling with the framework.   If I had to start a fresh new project and I had my say, I would choose either Stripes or JSF.  I might have been scared away from Stripes if I had to make the decision to switch to it, because it kind of looks/feels like a sourceforge basement project instead of a enterprise-grade framework, but it seems to be fairly solid.  We use Stripernate for easy ORM. However, it reminds me of Fruit Stripe gum, which lost its flavor WAY TOO FAST.

Answer-1533261, Question-47555
We have now used Stripes in multiple production projects and so far the experience has been great. Setup time is low and the configuration management issues seem to be fewer. We have webapps running with Stripes/Dojo/Hibernate and others with a mix of Stripes/Spring/JSP/Jquery etc. Adding Stripes to our existing projects was fairly simple thanks to their support for integrating existing Spring configurations. Using Stripes with JSP is fun although sometimes you do feel the need to code in Java and not have to use the JSTL so much.  Note: This is an old question, but given that it pops up pretty fast when you search for Stripes usage, I am adding a response to it.

Answer-2658230, Question-47555
Stripes is yesterdays technology, if you can pick something a little more modern like GWT.

Answer-5212707, Question-47555
We've been using Stripes for about 4 years now. Our stack is Stripes/EJB3/JPA. Many use Stripes plus Stripernate as a single, full stack solution. We don't because we want our business logic within the EJB tier, so we simply rely on JPA Entities as combined Model and DTO. Stripes does the binding to our Entities/DTO and we shove them back in to the EJB tier for work. For most of our CRUD stuff this is very thing and straightforward, making our 80% use case trivial to work with. Yet we have the flexibility to do whatever we want for the edge cases that always come up with complicate applications. We have a very large base Action Bean which encapsulates the bulk of our CRUD operations that makes call backs in to the individual subclasses specific to the entities and forms. We also have a large internal tag file library to manage our pages, security, navigation, tasks, etc. A simple CRUD edit form is little more than a list of field names, and we get all of the chrome and menus and access controls "for free". The beauty of this is that we get to keep the HTTP request based metaphor that we like and we get to choose the individual parts of the system rather than use one fat stack. The Stripes layer is lean and mean, and never gets in our way. We have a bunch of Ajax integrating YUI and JQuery, all working against our Stripes and EJB stack painlessly. I also ported a lighter version of the stack to GAE for a sample project, basically having to do minor work to our EJB tier. So, the entire stack is very nimble and amicable to change. Stripes is a big factor of that since we let it do the few things that it does, and does very well. Then delegate the rest to other parts of the stack. As always there are parts folks would rather have different at times, but Stripes would be the last part to go in our stack, frankly. It could be better at supporting the full HTTP verb set, but I'd rather fix Stripes to do that better than switch over to something else.

Answer-47963, Question-47683
You should implement in your client code javax.jms.ExceptionListener.  You will need a method called onException.  When the client's connection is lost, you should get a JMSException, and this method will be called automatically.  The only thing you have to look out for is if you are intentionally disconnecting from JBossMQ-- that will also throw an exception. Some code might look like this: In your "establishConnection" code, you would then implement a while(!initialized) construct that contains a try/catch inside of it.  Until you are sure you have connected and subscribed properly, stay inside the while loop catching all JMS/Naming/etc. exceptions. We've used this method for years with JBossMQ and it works great. We have never had a problem with our JMS clients not reconnecting after bouncing JBossMQ or losing our network connection.

Answer-72859, Question-47683
I'd highly recommend you use the Spring abstractions for JMS such as the MessageListenerContainer to deal with reconnection, transactions and pooling for you. You just need to supply a MessageListener and configure the MessageListenerContainer with the ConnectionFactory and the container does the rest.

Answer-117503, Question-47683
If you're purely a listener and do no other JMS calls other than connection setup, then the "onException() handler" answer is correct. If you do any JMS calls in your code, just using onException() callback isn't sufficient.  Problems are relayed from the JMS provider to the app either via an exception on a JMS method call or through the onException() callback.  Not both. So if you call any JMS methods from your code, you'll also want to invoke that reconnection logic if you get any exceptions on those calls.

Answer-118477, Question-47683
Piece of advice from personal experience.  Upgrade to JBoss Messaging.  I've seen it in production for 4 months without problems.  It has fully transparent failover - amongst many other features. Also, if you do go with Spring, be very careful with the JmsTemplate.

Answer-48451, Question-48442
About your first question... List, Map and Set serve different purposes. I suggest reading about the Java Collections Framework at http://java.sun.com/docs/books/tutorial/collections/interfaces/index.html. To be a bit more concrete: use List if you need an array-like data structure and you need to iterate over the elements use Map if you need something like a dictionary use a Set if you only need to decide if something belongs to the set or not. About your second question... The main difference between Vector and ArrayList is that the former is synchronized, the latter is not synchronized. You can read more about synchronization in Java Concurrency in Practice. The difference between Hashtable (note that the T is not a capital letter) and HashMap is similiar, the former is synchronized, the latter is not synchronized. I would say that there are no rule of thumb for preferring one implementation or another, it really depends on your needs.

Answer-48452, Question-48442
I've always made those decisions on a case by case basis, depending on the use case, such as: Do I need the ordering to remain? Will I have null key/values?  Dups? Will it be accessed by multiple threads Do I need a key/value pair Will I need random access? And then I break out my handy 5th edition Java in a Nutshell and compare the ~20 or so options.  It has nice little tables in Chapter five to help one figure out what is appropriate.   Ok, maybe if I know off the cuff that a simple ArrayList or HashSet will do the trick I won't look it all up.  ;)  but if there is anything remotely complex about my indended use, you bet I'm in the book.  BTW, I though Vector is supposed to be 'old hat'--I've not used on in years.

Answer-48457, Question-48442
Lists allow duplicate items, while Sets allow only one instance. I'll use a Map whenever I'll need to perform a lookup. For the specific implementations, there are order-preserving variations of Maps and Sets but largely it comes down to speed. I'll tend to use ArrayList for reasonably small Lists and HashSet for reasonably small sets, but there are many implementations (including any that you write yourself). HashMap is pretty common for Maps. Anything more than 'reasonably small' and you have to start worrying about memory so that'll be way more specific algorithmically. This page has lots of animated images along with sample code testing LinkedList vs. ArrayList if you're interested in hard numbers. EDIT: I hope the following links demonstrate how these things are really just items in a toolbox, you just have to think about what your needs are: See Commons-Collections versions of Map, List and Set.

Answer-48462, Question-48442
I'll assume you know the difference between a List, Set and Map from the above answers. Why you would choose between their implementing classes is another thing. For example: List: ArrayList is quick on retrieving, but slow on inserting. It's good for an implementation that reads a lot but doesn't insert/remove a lot. It keeps its data in one continuous block of memory, so every time it needs to expand, it copies the whole array. LinkedList is slow on retrieving, but quick on inserting. It's good for an implementation that inserts/removes a lot but doesn't read a lot. It doesn't keep the entire array in one continuous block of memory. Set: HashSet doesn't guarantee the order of iteration, and therefore is fastest of the sets. It has high overhead and is slower than ArrayList, so you shouldn't use it except for a large amount of data when its hashing speed becomes a factor. TreeSet keeps the data ordered, therefore is slower than HashSet. Map: The performance and behavior of HashMap and TreeMap are parallel to the Set implementations. Vector and Hashtable should not be used. They are synchronized implementations, before the release of the new Collection hierarchy, thus slow. If synchronization is needed, use Collections.synchronizedCollection().

Answer-48471, Question-48442
I found Bruce Eckel's Thinking in Java to be very helpful.  He compares the different collections very well.  I used to keep a diagram he published showing the inheritance heirachy on my cube wall as a quick reference.  One thing I suggest you do is keep in mind thread safety.  Performance usually means not thread safe.

Answer-48520, Question-48442
For non-sorted the best choice, more than nine times out of ten, will be: ArrayList, HashMap, HashSet. Vector and Hashtable are synchronised and therefore might be a bit slower. It's rare that you would want synchronised implementations, and when you do their interfaces are not sufficiently rich for thier synchronisation to be useful. In the case of Map, ConcurrentMap adds extra operations to make the interface useful. ConcurrentHashMap is a good implementation of ConcurrentMap. LinkedList is almost never a good idea. Even if you are doing a lot of insertions and removal, if you are using an index to indicate position then that requires iterating through the list to find the correct node. ArrayList is almost always faster. For Map and Set, the hash variants will be faster than tree/sorted. Hash algortihms tend to have O(1) performance, whereas trees will be O(log n).

Answer-48525, Question-48442
Theoretically there are useful Big-Oh tradeoffs, but in practice these almost never matter. In real-world benchmarks, ArrayList out-performs LinkedList even with big lists and with operations like "lots of insertions near the front."  Academics ignore the fact that real algorithms have constant factors that can overwhelm the asymptotic curve.  For example, linked-lists require an additional object allocation for every node, meaning slower to create a node and vastly worse memory-access characteristics. My rule is: Always start with ArrayList and HashSet and HashMap  (i.e. not LinkedList or TreeMap). Type declarations should always be an interface (i.e. List, Set, Map) so if a profiler or code review proves otherwise you can change the implementation without breaking anything.

Answer-17420706, Question-48442
I really like this cheat sheet from Sergiy Kovalchuk's blog entry: More detailed is Alexander Zagniotov's flowchart from his site.

Answer-44682154, Question-48442
As suggested in other answers, there are different scenarios to use correct collection depending on use case. I am listing few points, ArrayList: Most cases where you just need to store or iterate through a "bunch of things" and later iterate through them. Iterating is faster as its index based. Whenever you create an ArrayList, a fixed amount of memory is allocated to it and once exceeeded,it copies the whole array LinkedList: It uses doubly linked list so insertion and deletion operation will be fast as it will only add or remove a node. Retrieving is slow as it will have to iterate through the nodes. HashSet: Making other yes-no decisions about an item, e.g. "is the item a word of English", "is the item in the database?" , "is the item in this category?" etc. Remembering "which items you've already processed", e.g. when doing a web crawl; HashMap: Used in cases where you need to say "for a given X, what is the Y"? It is often useful for implementing in-memory caches or indexes i.e key value pairs  For example: For a given user ID, what is their cached name/User object?. Always go with HashMap to perform a lookup. Vector and Hashtable are synchronized and therefore bit slower and  If synchronization is needed, use Collections.synchronizedCollection(). Check This for sorted collections. Hope this hepled.

Answer-49920, Question-49900
It is generally considered a bad idea to version control your binaries and I do not recommend doing so. But if you absolutely have to, you can use antcall combined with param to pass parameters and call a target. You can find more information about the antcall task here.

Answer-49935, Question-49900
Also check out the subant task, which lets you call the same target on multiple build files:

Answer-50004, Question-49900
Take a look at Ant macros.  They allow you to define reusable "routines" for Ant builds.  You can find an example here (item 15).

Answer-65947, Question-49900
I would suggest to work with macros over subant/antcall because the main advantage I found with macros is that you're in complete control over the properties that are passed to the macro (especially if you want to add new properties). You simply refactor your Ant script starting with your target: creating a macro (notice the copy/paste and replacement with the @{file}): and invoke the macros with your files: Refactoring, "the Ant way"

Answer-90722, Question-49900
You can use Gant to script your build with groovy to do what you want or have a look at the groovy ant task.

Answer-50406, Question-50398
If it's short, I think you're better off re-writing the code in java.  Downloading one 50Mb runtime is bad enough.

Answer-50408, Question-50398
I would rewrite it if it's not too much trouble. The web service would work, but it seems like that would be a lot of overhead just to reuse a little code.

Answer-50424, Question-50398
You would use the Java Native Interface to call your C# code compiled into a DLL. If its a small amount of C#, it would be much easier to port it to Java. If its a lot, this might be a good way to do it. Here is a highlevel overview of it: http://en.wikipedia.org/wiki/Java_Native_Interface Your other option would be to create a COM assembly from the C# code and use J-Interop to invoke it. http://sourceforge.net/projects/j-interop/

Answer-50440, Question-50398
There is an IL to Java Bytecode compiler GrassHopper which may be of use to you.  I've never tried it though. I'd look at rewriting your code in Java though EDIT: Note that Grasshopper seems to be no longer available.

Answer-50676, Question-50398
If you do not want to rewrite hadle it as an Inter-process communication and choose one of following: Named pipes Sockets  SOAP

Answer-52577, Question-50398
We used JNBridge for this, and it worked great.  It handles Java->.NET and vice versa, all in-proc.

Answer-774505, Question-50398
http://www.infoq.com/articles/in-process-java-net-integration suggests running CLR and JVM in the same process space and passing calls back and forth. It sounds very efficient. I'm going to give it a try and integrate it into Jace if it works well.

Answer-1655173, Question-50398
I am author of jni4net, open source intraprocess bridge between JVM and CLR. It's build on top of JNI and PInvoke. No C/C++ code needed. I hope it will help you.

Answer-2400059, Question-50398
If it is a piece of code that is exposable as a command line utility, I just make the other host language use a system call to execute the utility. If your C# app needs to call Java, compile a special Java main that takes appropriate command line args and returns text output. It the oldest, simplest method.

Answer-50543, Question-50532
From this thread, there are different ways to do this: The DecimalFormat() seems to be the most dynamic way to do it, and it is also very easy to understand when reading others code.

Answer-50544, Question-50532
You and String.format() will be new best friends! http://java.sun.com/j2se/1.5.0/docs/api/java/util/Formatter.html#syntax

Answer-50548, Question-50532
Use DecimalFormat.

Answer-50556, Question-50532
Round numbers, yes. This is the main example source.

Answer-50557, Question-50532
There are two approaches in the standard library. One is to use java.text.DecimalFormat. The other more cryptic methods (String.format, PrintStream.printf, etc) based around java.util.Formatter should keep C programmers happy(ish).

Answer-50892, Question-50532
Be aware that classes that descend from NumberFormat (and most other Format descendants) are not synchronized. It is a common (but dangerous) practice to create format objects and store them in static variables in a util class. In practice, it will pretty much always work until it starts experiencing significant load.

Answer-22401832, Question-50532
As Robert has pointed out in his answer: DecimalFormat is neither synchronized nor does the API guarantee thread safety (it might depend on the JVM version/vendor you are using).  Use Spring's Numberformatter instead, which is thread safe.

Answer-25929742, Question-50532
Try this: Simple and efficient.

Answer-45898683, Question-50532
 For instance, if the double value passed into the formatDouble() method is 345.9372, the following will  be the result: 345.937 Similarly, if the value .7697 is passed to the method, the following will be the result: .770

Answer-51393, Question-51380
1) There might not be that much data available to read (the other end of the pipe might not have sent that much data yet), and the implementing class might be non-blocking (i.e. it will just return what it can, rather than waiting for enough data to fulfil the request). I don't know if any implementations actually behave in this way, however, but the interface is designed to permit it. Another option is simply that the file gets closed part-way through the read. 2) Either readFully() (which will always wait for enough input or else fail) or call skipBytes() in a loop.  I think the former is probably better, unless the array is truly vast.

Answer-51613, Question-51380
Josh Bloch has publicised this recently. It is consistent in that InputStream.read is not guaranteed to read as many bytes as it could. However, it is utterly pointless as an API method. InputStream should probably also have readFully.

Answer-51828, Question-51380
It turns out that readFully() adds more performance overhead than I was willing to put up with. In the end I compromised: I call skipBytes() once, and if that returns fewer than the right number of bytes, I call readFully() for the remaining bytes.

Answer-3977877, Question-51380
I came across this problem today.  It was reading off a network connection on a virtual machine so I imagine there could be a number of reasons for this happening. I solved it by simply forcing the input stream to skip bytes until it had skipped the number of bytes I wanted it to:

Answer-52114, Question-52112
You want SVNKit.  It's dual-licensed, so you have to pay only if you're doing commercial work with it.

Answer-53147, Question-53098
this may be useful: http://code.google.com/p/openid4java

Answer-53188, Question-53098
or pick another option from: http://wiki.openid.net/Libraries they are: openid4java WSO OpenID Library joid NetMesh InfoGrid LID

Answer-53384, Question-53316
LOB/CLOB column may not be large enough.  Hibernate has some default column sizes for LOB/CLOB that are relatively small (may depend on db).  Anyway, try something like this: Adjust the length (in bytes) based on your needs.

Answer-54792, Question-53316
Many JDBC drivers, early versions of Oracle in particular, have problems while inserting LOBs. Did you make sure that the query Hibernate fires, with the same parameters bound works successfully in your JDBC driver?

Answer-53854, Question-53845
Here is an in length discussion about the topic. It includes discussion about different compilers and IDEs.

Answer-53856, Question-53845
To compile the Java program MyJavaProg.java, type: To link it, use the command: and then linking to create an executable mycxxprog.exe

Answer-54614, Question-54567
Show us a small section of your code. Looks like it's a problem with JComboBox being a composite component. I'd suggest avoiding such nasty UI solutions.

Answer-54799, Question-54567
I found a workaround. I thought I'd let the next person with this problem know about.  Basically. Instead of setting the inputVerifier on the ComboBox you set it to it's "Editor Component".

Answer-54696, Question-54686
The only way I can think of doing it is by invoking a command line application that does the job for you and then screenscraping the output (like Linux's ps and Window's tasklist).  Unfortunately, that'll mean you'll have to write some parsing routines to read the data from both.

Answer-54950, Question-54686
This is another approach to parse the the process list from the command "ps -e": If you are using Windows, then you should change the line: "Process p = Runtime.getRun..." etc... (3rd line), for one that looks like this: Hope the info helps!

Answer-55002, Question-54686
There is no platform-neutral way of doing this. In the 1.6 release of Java, a "Desktop" class was added the allows portable ways of browsing, editing, mailing, opening, and printing URI's. It is possible this class may someday be extended to support processes, but I doubt it. If you are only curious in Java processes, you can use the java.lang.management api for getting thread/memory information on the JVM.

Answer-4465630, Question-54686
YAJSW (Yet Another Java Service Wrapper) looks like it has JNA-based implementations of its org.rzo.yajsw.os.TaskList interface for win32, linux, bsd and solaris and is under an LGPL license.  I haven't tried calling this code directly, but YAJSW works really well when I've used it in the past, so you shouldn't have too many worries.

Answer-9463010, Question-54686
On Windows there is an alternative using JNA:

Answer-10638518, Question-54686
Using code to parse ps aux for linux and tasklist for windows are your best options, until something more general comes along. For windows, you can reference: http://www.rgagnon.com/javadetails/java-0593.html Linux can pipe the results of ps aux through grep too, which would make processing/searching quick and easy. I'm sure you can find something similar for windows too.

Answer-16828521, Question-54686
 TASKLIST /v /FI "STATUS eq running" /FO "CSV" /FI "Username eq LHPL002\soft"  /FI "MEMUSAGE gt 10000"  /FI "Windowtitle ne N/A" /NH

Answer-35390242, Question-54686
You can easily retrieve the list of running processes using jProcesses

Answer-41634959, Question-54686
For windows I use following:

Answer-45068036, Question-54686
Finally, with Java 9 it will be possible with ProcessHandle:

Answer-55582, Question-55487
Reporting the value for InitialContext.getEnvironment().get(Context.PROVIDER_URL) might be helpful.

Answer-56701, Question-56687
You can do this with a custom RMI Socket Factory.   The socket factories create the sockets for RMI to use at both the client and server end so if you write your own you've got full control over the ports used.  The client factories are created on the server, Serialized and then sent down to the client which is pretty neat. Here's a guide at Sun telling you how to do it.

Answer-303081, Question-56687
Summary of the long answer below: to solve the problem that I had (restricting server and callback ports at either end of the RMI connection), I needed to create two pairs of client and server socket factories. Longer answer ensues: Our solution to the callback problem had essentially three parts.  The first was the object wrapping which needed the ability to specify that it was being used for a client to server connection vs. being used for a server to client callback.  Using an extension of UnicastRemoteObject gave us the ability to specify the client and server socket factories that we wanted to use.  However, the best place to lock down the socket factories is in the constructor of the remote object.   So, the first argument specifies the part on which the object is expecting requests, whereas the second and third specify the socket factories that will be used at either end of the connection driving this remote object. Since we wanted to restrict the ports used by the connection, we needed to extend the RMI socket factories and lock down the ports.  Here are some sketches of our server and client factories: Note that the server socket factory above ensures that only the port that you previously specified will ever be used by this factory.  The client socket factory has to be paired with the appropriate socket factory (or you'll never connect). So, the only thing remaining to force your two way connection to stay on the same set of ports is some logic to recognize that you are calling back to the client-side.  In that situation, just make sure that your factory method for the remote object calls the RemoteObjectWrapper constructor up top with the callback parameter set to true.

Answer-2455094, Question-56687
I've been having various problems implementing an RMI Server/Client architecture, with Client Callbacks. My scenario is that both Server and Client are behind Firewall/NAT. In the end I got a fully working implementation. Here are the main things that I did: Server Side , Local IP: 192.168.1.10. Public (Internet) IP 80.80.80.10 On the Firewall/Router/Local Server PC open port 6620. On the Firewall/Router/Local Server PC open port 1099. On the Router/NAT redirect incoming connections on port 6620 to 192.168.1.10:6620 On the Router/NAT redirect incoming connections on port 1099 to 192.168.1.10:1099 In the actual program: Client Side, Local IP: 10.0.1.123 Public (Internet) IP 70.70.70.20 On the Firewall/Router/Local Server PC open port 1999. On the Router/NAT redirect incoming connections on port 1999 to 10.0.1.123:1999 In the actual program: Hope this helps. Iraklis

Answer-37356737, Question-56687
You don't need socket factories for this, or even multiple ports. If you're starting the Registry from your server JVM you can use port 1099 for everything, and indeed that is what will happen by default. If you're not starting the registry at all, as in a client callback object, you can provide port 1099 when exporting it. The part of your question about 'the client connections back to the server resulting from callbacks' doesn't make sense. They are no different from the original client connections to the server, and they will use the same server port(s).

Answer-56866, Question-56843
Doesn't the Criteria API do it for you? It looks almost exactly like what you're asking for.

Answer-56883, Question-56843
It looks like you want to use the Criteria query API built into Hibernate. To do your above query it would look like this: If you don't have access to the Hibernate Session yet, you can used 'DetachedCriteria' like so: If you wanted to get all Stock that have a Bonus with a specific ID you could do the following: For more infromation check out Criteria Queries from the Hibernate docs

Answer-56937, Question-56843
@Sbastien Rocca-Serra  That's just a join. Hibernate does it automatically, if and only if you've got the mapping between Stock and Bonus setup and if bonus is a property of Stock. Criteria.list() will return Stock objects and you just call stock.getBonus(). Note, if you want to do anything like You need to use Criteria.createAlias(). It'd be something like

Answer-57100, Question-56843
@Sbastien Rocca-Serra Now we're getting somewhere concrete. The sort of join you're trying to do isn't really possible through the Criteria API, but a sub-query should accomplish the same thing. First you create a DetachedCriteria for the bonus table, then use the IN operator for someValue. This is equivalent to The only downside would be if you have references to different tables in someValue and your ID's are not unique across all tables. But your query would suffer from the same flaw.

Answer-57141, Question-56843
Criteria API does not provide all functionality avaiable in HQL. For example, you cannot do more than one join over the same column. Why don't you use NAMED QUERIES? The look much more clean:

Answer-586392, Question-56843
I wrote a GPL'd solution for OMERO which you could easily build suited to your situation. Source: QueryBuilder.java Test: QueryBuilderMockTest Usage: It functions as a state machine "select->from->join->where->order", etc. and keeps up with optional parameters. There were several queries which the Criteria API could not perform (see HHH-879), so in the end it was simpler to write this small class to wrap StringBuilder. (Note: there is a ticket HHH-2407 describing a Hibernate branch which should unify the two. After that, it would probably make sense to re-visit the Criteria API)

Answer-1835464, Question-56843
Take a look at the search package available from the hibernate-generic-dao project.  This is a pretty decent HQL Builder implementation.

Answer-1880122, Question-56843
I know this thread is pretty old, but I also was looking for a HqlBuilder And I found this "screensaver" project  It is NOT a Windows screensaver, it's a  "Lab Information Management System (LIMS) for high-throughput screening (HTS) facilities that perform small molecule and RNAi screens." It contains an HQLBuilder that is looking quite good. Here is a sample list of available methods:

Answer-2044946, Question-56843
For a type-safe approach to your problem, consider Querydsl. The example query becomes Querydsl uses APT for code generation like JPA2 and supports JPA/Hibernate, JDO, SQL and Java collections. I am the maintainer of Querydsl, so this answer is biased.

Answer-8372768, Question-56843
For another type-safe query dsl, I recommend http://www.torpedoquery.org.  The library is still young but it provides type safety by directly using your entity's classes.  This means early compiler errors when the query no longer applies before of refactoring or redesign. I also provided you with an example.  I think from your posts that you where trying to do a subquery restriction, so I based the exemple on that:

Answer-17545743, Question-56843
Now are also available the standard JPA Type Safe query and an less standard but also good Object Query Examples: JPA Type Safe Object Query

Answer-59008, Question-58988
ISP states that: Clients should not be forced to depend   on methods that they do not use. ISP relates to important characteristics - cohesion and coupling. Ideally your components must be highly tailored. It improves code robustness and maintainability.  Enforcing ISP gives you following bonuses: High cohesion - better understandability, robustness Low coupling - better maintainability, high resistance to changes If you want to learn more about software design principles, get a copy of Agile Software Development, Principles, Patterns, and Practices book.

Answer-59012, Question-58988
It simplifies the interface that any one client will use and removes dependencies that they might otherwise develop on parts of the interface that they don't need.

Answer-59529, Question-58988
One reason is that having many interfaces with a minimal amount of methods for each one makes it easier to implement each interface and to implement them correctly. A large interface can be unruly. Also, using a focused interface in a scenario makes the code more maintanable because you can see which facet of the object is being used (e.g., an IComparable interface lets you know that the object is only being used for comparisons in the given scenario).

Answer-17999321, Question-58988
The interface segregation is the I on the SOLID principle, before digging too deep with the first, lets explain whats does the latter mean.  SOLID can be considered a set of best practices and recommendations made by experts (meaning they have  been proved before) in order to provide a reliable foundation in how we design applications. These practices strive to  make easier to maintain, extend, adapt and scale our applications. Why should I care about SOLID programming? First of all, you have to realize you are not going to be forever where you are. If we use standards and well known architectures, we can be sure that our code will be easy to maintain by other developers that come after us, and Im sure you wouldnt want to deal with the task of fixing a code that didnt applied any known methodology as it would be very hard to understand it. The interface segregation principle. Know that we know what the SOLID principles are we can get into more detail about the Interface Segregation principle, but what exactly does the interface segregation says? Clients should not be forced to implement unnecessary methods which   they will not use This means that sometimes we tend to make interfaces with a lot of methods, which can be good to an extent, however this can easily abused, and we can end up with classes that implement empty or useless methods which of course adds extra code and burden to our apps. Imagine you are declaring a lot of methods in single interface, if you like visual aids a class that is implementing an interface but that is really needing a couple of methods of it would look like this: In the other hand, if you properly apply the interface segregation and split your interface in smaller subsets you can me sure to implement those that are only needed: See! Is way better!  Enforcing this principle will allow you to have low coupling which aids to  a better maintainability and high  resistance to changes. So you can really leverage the usage of interfaces and implementing the methods when you really should. Now lets review a less abstract example, say you declared an interface called Reportable And you have a client that will only to export some data in Excel format, you can implement the interface, but would you only have to implement the excel method? The answer is no, you will have to code the implementation for all the methods even if you are not going to use them, this can cause a lot of junk code hence making the code hard to maintain.. Remember keep it simple and dont repeat yourself and you will find that you are already using this principle without knowing.

Answer-20562950, Question-58988
This principle primarily serves twin purposes To make the code more readable and manageable. Promotes single responsibility for classes ( high cohesion ). Ofcourse why should a class have a method that has no behavioural impact ? Why not just remove it. Thats what ISP is about There are few questions that a designer must ask with concerns to ISP What does one achieve with ISP How to I analyse an already existing code for any ISP violations To take this discussion further, I must also add that this principle isn't a 'principle' in the strictest sense, because under certain circumstances, applying ISP to the design, instead of promoting readability, might make the object structure unreadable and cluttered with unnecessary code.  You may well observe this in the java.awt.event package More at my blog: http://design-principle-pattern.blogspot.in/2013/12/interface-segregation-principle.html

Answer-33517269, Question-58988
To avoid regression efforts, when just one client-specific or one behavior-specific changes. If you have combined all your behavior methods all in one BIG interface, just think about how you will end up testing all the pieces of code where all you have referred to is that interface, even when only small changes happened. For a more detailed explanation, refer to Interface segregation principle article

Answer-40042398, Question-58988
ISP is important. Basic idea of ISP : Client should not be forced to depend on methods it does not use. This principle seems to be more logical. Ideally client should not implement the methods, which are not used by the client.  Refer to below SE question for code example: Interface Segregation Principle- Program to an interface Advantages: Flexibility : In absence of ISP, you have one Generic FAT interface and many classes implementing it. Assume that you had 1 interface and 50 classes. If there is a change in interface, all 50 classes have to change their implementation. With ISP, you will divide generic FAT interface into fine granular small interfaces. If there is a change in small granular interface, only the classes implementing that interface  will be affected. Maintainability and Ease of use:  Since changes are limited to fine granular interface instead of generic FACT interface, code maintenance is easier. Unrelated code is no longer part of implementation classes.

Answer-59119, Question-59107
Change it to: You need that because Comparable is itself a generic type.

Answer-85739, Question-59107
Here's an odd case: Luckily code like the one above is rare, but nullCompare() will not support comparison of Bs unless it is stated that Comparable may apply to T or any superclass thereof: Even though most people will never benefit from the above tweak, it may come in handy when designing APIs for exported libraries.

Answer-115539, Question-59107
I'm not sure that genericizing this method makes sense. Currently the method works on any kind of Comparable; if you genericize it you will have to implement it (with exactly the same code) multiple times. Sometimes it is possible to compare two objects that don't have a common ancestor, and any generic version won't allow this. By adding generics you won't add any safety to the code; any problems of safety will occur in the call to compareTo. What I would suggest is simply suppressing the warning. It's not really warning you about anything useful.

Answer-227885, Question-59107
Cannot edit so I have to post my answer. You need to declare nested type parameter since Comparable is generic. Please note that Comparable< ? super T >, which makes more flexible.  You will see the same method definition on Collections.sort

Answer-815484, Question-59107
To make it even more general, you could even allow it to work for two different types. =P

Answer-61222, Question-61191
Honestly it sounds like bad CLASSPATH. One thing I suggest to start debugging this problem is copying the jtds package to same path as your main packages/classes and see if it works. This way you can assure the Classpath manifest is or isn't the problem. The Spring/Hibernate relies on the lib directory, so it will always be on classpath because it's main structure. Use the lib directory also to test. Hope this guidelines will help. Also send more information, like paths, classpath and manifest files.

Answer-106590, Question-61191
This issue occurred because our jdbc.url had invalid url. This was because maven treats jdbc.url property as a special property and while profiling, instead of url defined in the filter.properties. And that is the reason "No Suitable Driver" exception. The question should have been more clear.  Anyways to fix that we had to rename jdbc.url properties to jdbc.url.somename. This fixed our issue with maven profiling. We also had a similar maven profiling issue for a property called "server.name" This filter property was also confusing maven profiling . We had to change the name of that property as well. Thanks again Fernando.

Answer-1389015, Question-61191
It is a Maven bug http://jira.codehaus.org/browse/MNG-3563

Answer-61617, Question-61615
I'd say it entirely depends on who's working on the codebase. If you have a small group of developers who all share a common language and you don't ever plan needing anyone who doesn't speak the language to work on the code then go ahead and use whatever characters you want. If you need to have people of varying cultures and languages working on the code then it's probably best to stick with English since it's the common denominator for just about everyone in the world.

Answer-61619, Question-61615
Part of the problem is that the Java/C# language and its libraries are based on English words like if and toString(). I personally would not like to switch between non-English language and English while reading code. However, if your database, UI, business logics (including metaphors) are already in some non-English language, there's no need to translate every method names and variables into English.

Answer-61620, Question-61615
It depends: Does your team conform to any existing standards that require your using ASCII? Is your code ever going to be feasibly reused or read by someone who doesn't speak your native language? Do you envision a scenario where you'll need to ask for help online and will therefore not be able to copy-paste your code sample in as-is? Are you certain your entire suite of tools support code encoding? If you answered 'yes' to any of the above, stay ASCII only.  If not, go forward at your own risk.

Answer-61624, Question-61615
I would stick to english, simply because you usually never know who is working on that code, and because some third-party tools used in the build/testing/bugtracking progress may have problems. Typing  on a Non-German Keyboard is simply a PITA, and I simply believe that anyone involved in software development should speak english, but maybe that's just my arrogance as a non-native-english speaker. What you call "American arrogance" is not whether or not your program uses international variable names, it's when your program thinks "Whrung" and "Wahrung" are the same words.

Answer-61630, Question-61615
IF you get past the other prerequisites you then have one extra (IMHO more important) one - How difficult is the symbol to type. On my regular en-us keyboard, the only way I know of to type the letter  is to hold alt, and hit 0227 on the numeric keypad, or copy and paste. This would be a HUGE big roadblock in the way of typing quickly. You don't want to slow your coding down with trivial stuff like this if you aren't forced to. International keyboards may alleviate this, but then what happens if you have to code on your laptop which doesn't have an international keyboard, etc?

Answer-61641, Question-61615
I used to work in a development team that happily wiped their asses with any naming (and for that matter any other coding) conventions. Believe it or not, having to cope with 's and 's in the code was a contributing factor of me resigning. Though I'm Finnish, I prefer writing code with US keyboard settings because curly and square brackets are a pain to write in a Finnish keyboard (try right alt and 7 and 0 for curlies). So I say stick with the ascii characters.

Answer-69791, Question-61615
If your business are non-English speakers, and you think Domain Driven Design has something to it, then there is another aspect: How do we, as developers, use the same domain language as our business without any translation overhead? That does not only mean translations between languages, say English and Norwegian, but also between different words. We should use the exact same words as our business for our entity classes and services. I have found it easier to just give in and use my native language. Now that my code use the same words, it's easier to have a conversation with my domain experts. And after a while you get used to it, just like how you got used to code without Hungarian notation.

Answer-1413778, Question-61615
Here's an example of where I've used non-ASCII identifiers, because I found it more readable than replacing the greek letters with their English names.  Even though I don't have  or  on my keyboard (I relied on copy-and-paste.) However these are all local variables.  I would keep non-ASCII identifiers out of public interfaces.

Answer-3185293, Question-61615
I would stick to ASCII characters because if anyone in your development team is using an SDK that only supports ASCII or you wanted to make your code open source, alot of problems could arise. Personally, I would not do it even if you are not planning on bringing anyone who doesn't speak the language in on the project, because you are running a business and it seems to me that one running a business would want his business to expand, which in this day and age means transcending national borders. My opinion is that English is the language of the realm, and even if you name your variables in a different language, there is little to no point to use any non-ASCII characters in your programming. Leave it up to the language to deal with it if you are handling data that is UTF8: my iPhone program (which involves tons of user data going in between the phone and server) has full UTF8 support, but has no UTF8 in the source code. It just seems to open such a large can of worms for almost no benefit.

Answer-18499554, Question-61615
There is another hazzard to using non-ASCII characters, though it will probably only bite in obscure cases.  The allowed characters are defined in terms of the methods Character.isJavaIdentifierStart(int) and Character.isJavaIdentifierPart(int), which are defined in terms of Unicode.  However, the exact version of Unicode used depends on the version Java platform, as specified in the documentation for java.lang.Character. Since character properties change slightly from one Unicode version to the next, it's possible (but probably very unlikely) you could have identifiers that are valid in one version of Java, but not in the next.

Answer-62324, Question-62276
One tool which does this is the software tomograph http://www.software-tomography.com/ it is commercial and the ui sucks :o

Answer-62329, Question-62276
There is also structure 101 which should do this http://www.headwaysoftware.com/

Answer-62332, Question-62276
And you can use the open source tool CAP which is an eclipse plugin. http://cap.xore.de/ CAP has a graphical package view which will show you the lines to the classes so after some clicks (depending on the size of the circle) you will find the culprit.

Answer-62335, Question-62276
Findbugs can detect circular class dependencies, and has an eclipse plugin too. http://findbugs.sourceforge.net/

Answer-62344, Question-62276
There are some commercial tools: Structure101 & Lattix which can be used for this purpose.

Answer-66059, Question-62276
a first possible answer is... not pretty. But it does begin to do what I am after. (a better solution is presented below) Dependency Finder! Download it, unzip it. Not the most modern or active project ever, but if you edit [Dependency Finder]/bin/DependencyFinder.bat, add its path for DEFAULT_DEPENDENCYFINDER_HOME, set a JAVA_HOME, you can launch it. Then you click on the 'Extract' button (CTRL-E - first button), enter your classes path, and let it scan away. The tricky part is to click exactly the right set of 'programming elements' and 'closures' items, in order to not been swamped by the level of details in the result. Select only 'classes' in the left side ('programming elements') Select only 'classes' in the right side ('closures') add "/javax?./,/org./,/sun./" as exclusion pattern (for both programming elements and closures) Click on the wheels (last button - Compute all - CTRL A) And here you go. Whenever you see '<->', you have got yourself a nice cyclic dependency. (If you select 'features' on the 'closure' side, you can even know what function does trigger the cycle - awesome -) I am ready to test any other suggestions.

Answer-71610, Question-62276
Well... after testing DepFinder presented above, it turns out it is great for a quick detection of simple dependencies, but it does not scale well with the number of classes... So the REAL ACTUAL ANSWER is: CDA - Class Dependency Analyzer It is fast, up-to-date, easy to use and provides with graphical representation of classes and their circular dependencies. A dream come true ;) You have to create a workset in which you enter only the directory of your classes (.class) (no need to have a complete classpath) The option "Detect circular dependencies - ALT-C" works as advertise and does not take 100% of the CPU for hours to analyze my 468 classes. Note: to refresh a workspace, you need to open it again(!), in order to trigger a new scan of your classes.

Answer-3415537, Question-62276
We use Sonar to detect package cycles. It draws a nice graph of the dependencies and shows which ones go in the wrong direction. You can even navigate to the source where the dependency is used. See http://www.sonarsource.org/fight-back-design-erosion-by-breaking-cycles-with-sonar/

Answer-18217016, Question-62276
Highwheel detects class and package cycles and reports the source of the dependences down to the class/method/field level indicating the type of the relationship (inheritance, composition, part of a method signature etc).  It also breaks large cycles down into their sub-elements which can be understood/tackled individually. https://github.com/hcoles/highwheel The output is html with embedded svg that requires a modern browser.

Answer-62460, Question-62423
I would recommend using VTD-XML http://vtd-xml.sourceforge.net/ From their FAQ ( http://vtd-xml.sourceforge.net/faq.html ): Why should I use VTD-XML for large XML files? For numerous reasons summarized below: Performance: The performance of VTD-XML is far better than SAX Ease to use: Random access combined with XPath makes application easy to write Better maintainability: App code is shorter and simpler to understand. Incremental update: Occasional, small changes become very efficient. Indexing: Pre-parsed form of XML will further boost processing performance. Other features: Cut, paste, split and assemble XML documents is only possible with VTD-XML. In order to take advantage of VTD-XML, we recommended that developers split their ultra large XML documents into smaller, more manageable  chucks (<2GB).

Answer-62480, Question-62423
You have a few options here, but none of them are good.  Since XML Objects aren't broken into distinct parts, you'll either have to use some filesystem level modification with regex pattern matching (sed is a good start), OR you should break your xml into smaller parts for manageability.

Answer-62541, Question-62423
If possible, serialize the XML and use diff/patch/apply Linux tools (or equivalent tools in your platform) . This way, you don't have to deal with parsing, writing.

Answer-64650, Question-62423
If your XML file is so large that updating it is a performance bottleneck, you should consider moving away from XML to a more efficient disk format (or a real database).   If, however, you just feel like it might be a problem, remember the rules of optimization: Don't do it (experts only) Don't do it, yet.

Answer-114810, Question-62423
Process Large XML Files with XQuery Works with Gigabyte Size XML Files http://www.xquery.com XQuery is a query language that was designed as a native XML query language. Because most types of data can be represented as XML, XQuery can also be used to query other types of data. For example, XQuery can be used to query relational data using an XML view of a relational database. This is important because many Internet applications need to integrate information from multiple sources, including data found in web messages, relational data, and various XML sources. XQuery was specifically designed for this kind of data integration. For example, suppose your company is a financial institution that needs to produce reports of stock holdings for each client. A client requests a report with a Simple Object Access Protocol (SOAP) message, which is represented in XML. In most businesses, the stock holdings data is stored in multiple relational databases, such as Oracle, Microsoft SQL Server, or DB2. XQuery can query both the SOAP message and the relational databases, creating a report in XML. XQuery is based on the structure of XML and leverages that structure to make it possible to perform queries on any type of data that can be represented as XML, including relational data. In addition, XQuery API for Java (XQJ) lets your queries run in any environment that supports the J2EE platform.

Answer-65401, Question-63030
The AWT code in IKVM is fairly easy to read and edit.  I'd recommend you look for the methods that you are using that throw that exception, and then implement them.  I've done this several times before with IKVM's AWT implementation and found it easy to do for background/server related functions.  Its much less usable if your app is a desktop app, however.

Answer-63066, Question-63042
Here's a good place to start: http://java.sun.com/j2se/1.5.0/docs/relnotes/features.html http://java.sun.com/developer/technicalArticles/releases/j2se15/

Answer-63077, Question-63042
Java 5 new features Java 6 new features The real meat is in Java 5. Generics, Autoboxing, Annotations.

Answer-63112, Question-63042
I can recommend Bruce Eckel's "Thinking in Java" 4th edition.  He goes over a bunch of basic stuff you can skip, but his treatment of new 1.5 features is very thorough, especially the chapter on generics.  And it is a good Java reference to own.

Answer-63113, Question-63042
Dietel : How to program Java This book is highly recommended.  Teaches everything, does it well.  Starts with simple Hello World and ends up in you writing your own BASIC compiler.  handles databases as well.  Does everything, uml, design.  Just can't say enough about it. And it is also beautiful book, I mean in design and color and it is not dry.

Answer-63634, Question-63042
I would thoroughly recommend Java Concurrency in Practice by Brian Goetz, Tim Peierls, Joshua Bloch, and Joseph Bowbeer. It focusses solely on good concurrency coding, but includes excellent guidance on the new concurrency features in the Java 5 and 6 libraries. Of course, it is no help at all on the other features, but if you ever deal with threads (and if you have a GUI, then you have threads), then this book is indispensable.

Answer-11788474, Question-63042
Java 5 introduced several major updates, such as language improvements (i.e. Annotations, Generics, Autoboxing, and improved syntax for looping) among many others. Annotation is a mechanism for tagging classes with metadata so that, they can be used by metadata-aware programs. Generics is a mechanism of specifying types for objects belonging to collections, such as Arraylists, so that type safety is guaranteed at compile time. Autoboxing allows the automatic conversions between primitive types (e.g. int) and wrapper types (e.g. Integer). Improved syntax for looping includes the enhancements for each loop for going through the items of array or collections comparatively easily. Java 6 focuses on new specifications and APIs including XML, Web Services, JDBC version 4.0, programming based on Annotations, APIs for Java compiler and Application client GUI.With new compiler API added with Java 6, the java compiler can now receive and/or send output to an abstraction of the file system (programs can specify/process compiler output). Furthermore, Java 6 added enhancements to the applications GUI capabilities in AWT (faster splash screens and support for system tray) and SWING (better drag-and-drop, support for customizing layouts, multithreading enhancements and ability to write GIF images).

Answer-64058, Question-64038
You could call during init or whatever Locale.setDefault() or -Duser.language=, -Duser.country=, and -Duser.variant= at the command line. Here's something on Sun's site.

Answer-64064, Question-64038
I believe java gleans this from the environment variables in which it was launched, so you'll need to make sure your LANG and LC_* environment variables are set appropriately. The locale manpage has full info on said environment variables.

Answer-64070, Question-64038
One way to control the locale settings is to set the java system properties user.language and user.region.

Answer-64096, Question-64038
With the user.language, user.country and user.variant properties. Example: java -Duser.language=th -Duser.country=TH -Duser.variant=TH SomeClass

Answer-2936833, Question-64038
If you are on Mac, simply using System Preferences -> Languages and dragging the language to test to top (before English) will make sure the next time you open the App, the right locale is tried!!

Answer-9894836, Question-64038
I had to control this in a script that ran on a machine with French locale, but a specific Java program had to run with en_US. As already pointed out, the following works: Alternatively,  I prefer the latter.

Answer-10646947, Question-64038
You can change on the console:

Answer-24987464, Question-64038
For tools like jarsigner which is implemented in Java.

Answer-29652154, Question-64038
On linux, create file in /etc/default/locale with the following contents and then use the source command to export this variable by running The source command sets the variable permanently.

Answer-64162, Question-64148
I think your best bet is to use an ORM-tool that includes database migration like SubSonic: http://subsonicproject.com/2-1-pakala/subsonic-using-migrations/

Answer-64165, Question-64148
We ended up making update scripts each time we changed the database.  So there's a script from version 10 to 11, from 11 to 12, etc.. Then we can run any consecutive set of scripts to skip from some existing version to the new version.  We stored the existing version in the database so we could detect this upon startup. Yes this involved database-specific code!  One of the main problems with Hibernate!

Answer-64225, Question-64148
When working with Hibernate, I use an installer class that runs from the command-line and has options for creating database schema, inserting base data, and dynamically updating the database schema using SchemaUpdate. I find it to be extremely useful. It also gives me a place to put one-off scripts that will be run when a new version is launched to, for example, populate a new field in an existing DB table.

Answer-64312, Question-64148
I don't see why ORM generated schemas are any different to other DB schemas - the problem is the same. Assuming your ORM will spit out a generation script, you can use an external tool to do the diff I've not tried it but google came back with SQLCompare as one option - I'm sure there are others.

Answer-64647, Question-64148
LiquiBase is an interesting open source library for handling database refactorings (upgrades).  I have not used it, but will definitely give it a try on my next project where I need to upgrade a db schema.

Answer-162933, Question-64148
We hand code SQL update scripts and we tear down the schema and rebuild it applying the update scripts as part of our continuous build process. If any hibernate mappings do not match the schema, the build will fail.

Answer-5132384, Question-64148
DbMaintain can also help here.

Answer-28055590, Question-64148
You can check this feature comparison of some database schema upgrade tools. A comparison of the number of questions in SOW of some of those tools: mybatis (1049 questions tagged) Liquibase (663 questions tagged) Flyway (400 questions tagged) DBDeploy (24 questions tagged).

Answer-64229, Question-64213
 Apache Commons Log4j Google collections

Answer-64231, Question-64213
The Google Collections API is pretty handy if you use lots of, well, Collections...

Answer-64294, Question-64213
It might be worth saying that the first thing to do is get to know the libraries in the newer versions of Java. A lot of ideas have worked their way back into java - java.util.concurrent, java.nio, and javax.xml

Answer-64295, Question-64213
Apache's Jakarta Commons.

Answer-64318, Question-64213
The Spring framework is surprisingly general purpose. I started by just using it as a configuration management tool, but then realized how helpful dependency injection is when doing test-driven development. Then I slowly discovered many useful modules hidden in the corners of Spring.

Answer-64837, Question-64213
JXL for Excel workbook creation/edition. I work in a bank and the multipurpose report tool for diary work is Excel. Whatever appliction we do must import/export from/to Excel. The only fail it's that it has memory problems with large workbooks and formating it's a little obscure

Answer-66003, Question-64213
Functional Java offers first-class function values, immutable lists/arrays, lazy/infinite streams, tuple types, either types, optional values (type-safe alternative to null). Works well in conjunction with Google Collections or the java.util collections. It also provides handy concurrency abstractions like parallel strategies, parallel list/array functors, actor concurrency, and composable light-weight processes.

Answer-66427, Question-64213
Take a look at jmate project. It contains really helpful methods for strings, collections and IO operations (for now).  Look some examples here.

Answer-912941, Question-64213
lambdaj is a thread safe library of static methods that provides an internal DSL to manipulate collections in a pseudo-functional and statically typed way without explicitly iterating on them. It eliminates the burden to write (often poorly readable) loops while iterating over collections.

Answer-913028, Question-64213
Here is a good start. http://java-sources.org/

Answer-3707905, Question-64213
Google Collections migrated to great Guava Libraries . It contains some common utilities, string matcher, splitter, joiner, IO utils etc.

Answer-23703968, Question-64213
Lately I was trying to find answer to this question. I made some data analysis for this, you can find results here and here.

Answer-65828, Question-64781
What is your measure of "requests/sec"?  In other words, what happens for the 31st request?  What resource is being blocked?  If it is the front-end/servlet/web portion, can you run em.persist() in another thread and return immediately? Also, are you creating transactions each time?  Are you creating EntityManagerFactory objects with each request?

Answer-66373, Question-64781
You should decouple from the JPA interface and use the bare TopLink API. You can probably chuck the objects you're persisting into a UnitOfWork and commit the UnitOfWork on your schedule (sync or async). Note that one of the costs of em.persist() is the implicit clone that happens of the whole object graph. TopLink will work rather better if you uow.registerObject() your two user objects yourself, saving itself the identity tests it has to otherwise do. So you'll end up with: This is very old school TopLink btw ;) Note that the batch will help a lot, because batch writing and more especially batch writing with parameter binding will kick in which for this simple example will probably have a very large impact on your performance. Other things to look for: your sequencing size. A lot of the time spent writing objects in TopLink is actually spent reading sequencing information from the database, especially with the small defaults (I would probably have several hundred or even more as my sequence size).

Answer-64913, Question-64833
It has been a while, but I seem to remember having trouble with the slight differences in how default namespaces were handled between .Net and Java web services. Double check the generated c# proxy class and any namespaces declared within (especially the defaults xmlns=""), against what the Java service is expecting.  There will be probably be very subtle differences which you will have to recreate. If this is the case then you will to provide more namespace declarations in the c# attributes.

Answer-66296, Question-64833
From your question, it looks like you had the client working at one point, and then the service was changed to return an array. Make sure you re-generate the proxy so the returned SOAP message is deserialized on the client. It wasn't clear you had done this - just making sure.

Answer-66834, Question-64833
Thanks to Xian, I have a solution. The wsdl for the service included a line The soap that the client sent to the server had the following attribute on all data elements: But the xml payload of the response (from the service back to the client) did not have this namespace included.  By tinkering with the HTTP response (which I obtained with WireShark), I observed that the .NET proxy class correctly picked up the MyResponse values if I forced the xmlns attribute on every returned data element. Short of changing the service, which I don't control, the workaround is to edit the VS generated proxy class (eg Reference.cs) and look for lines like this: and comment out the XmlType attribute line.  This will tell the CLR to look for response elements in the default namespace rather than the one specied in the wsdl.  You have to redo this whenever you update the reference, but at least it works.

Answer-65065, Question-65010
FWIW: I have never had good luck with GCJ, I have had a lot of problems using it and have had some obscure issues pop up that took forever to diagnose to GCJ rather than me (I am always very very reluctant to blame things on external libraries). I will openly admit this happened several years ago and I have never wanted to go near GCJ again. To give that more substance this was while I was in school and was working on a mostly trivial program so on an "enterprise level" I have had a healthy fear of GCJ.

Answer-65069, Question-65010
I don't think that a large application like yours will compile to machine code. Remember that java is not only java syntax (might compile to machine code) but also a virtual machine which is more like an application / process environment. I would suggest making an uberjar or like that instead.

Answer-65109, Question-65010
I don't know about GCJ, but my company uses Excelsior JET with success.  We haven't done it with a webapp (yet) but it should be able to handle anything that the Sun JRE can.  In fact JET is a Sun-certified Java implementation.

Answer-65733, Question-65010
Having one executable has a few downsides: You can't patch it as easy (i.e. replace one class file) I don't think it can be called a webapp -- I assume it won't run in Tomcat. It is non-standard so that increases your maintenance costs. It is non-standard so tool support is reduced. If he wants something simple maybe a war or ear would be better.  I can't see any benefit to doing this -- I would think this might be beneficial it it was a standalone application that you distributed so that people can just double-click on it.

Answer-72914, Question-65010
Perhaps your boss just needs a demo as to how easy it is to distribute and deploy a war file for your customers on their own app servers.  Every file is "binary", so you might be too-literal in thinking he means an executable on the command-line.

Answer-3734522, Question-65010
I've only used GCJ very briefly, and quickly moved to Sun's JDK. The main problems I saw was that GCJ seems to always lag a little behind the latest version of Sun's JDK and that there were weird mysterious bugs caused by subtle differences with Sun's JDK. In version 1.5 (which is supposd to be compatible with Sun's v1.5), I had problems compiling using generics, and finally gave up and moved to Sun's JDK. I must say, any difference in performance was negligible (for my purposes, YMMV) and really the solution for installation issues is to create an installer for your app. Reverse engineering a binary isn't really all that harder than reverse engineering bytecode. Use an obfuscator if it is that important. Overall, I think the compatibility problems involved in using GCJ greatly outweighs any gains (which I think questionable at best) you might possible derive from it. Try compiling parts of your app in gcj and see how it goes though. If it works out fine, otherwise you get something solid to pitch to your boss.

Answer-3734683, Question-65010
I'll play devils advocate a bit, though I know little about GCJ.  Compiling to native code may give your application a performance boost and use less memory, so if it can be made to work, there are advantages for the business in terms of competition. Being able to support an application better is also a good for business.  So perhaps it is worth investigating baring in mind that nothing can lose a customer faster than an application that doesn't work. You need proper project time to try this out and a customer, that knows what they are getting into, that is willing to give it whirl (harder to find).

Answer-3740493, Question-65010
Excelsior JET is the definitive answer

Answer-65204, Question-65128
Formally SysInternal's, now Microsoft's Process Explorer http://technet.microsoft.com/en-us/sysinternals/bb896653.aspx "Find" menu item -> "Find Handle or DLL..."

Answer-65280, Question-65128
SysInternals may not help with Java class IO.  Try getting a thread dump of the JVM (e.g., kill -3) while these logs are being written to.  You should be able to catch a thread red handed with java.io packages near the top of the stack trace.

Answer-65618, Question-65128
Try placing a breakpoint in the File class' constructors and the mkdir and createNewFile methods.  Generally, code will use the File class to create its files or directories.  You should have the Java source code for these classes included with your JVM.

Answer-65957, Question-65128
Add -Dlog4j.debug to the command line and there will be extra info in standard output about how it is configured.

Answer-65191, Question-65150
I'll cast a vote for jBPM.  We used it on a larg-ish ETL platform in-house and it seemed to work quite well.  I don't have anything to compare it to, however.

Answer-65245, Question-65150
Here's an article that compares kBPM, OpenWFE, and Enhydra Shark that looks like it has some good, thorough info.

Answer-66675, Question-65150
It depends what kind of initial investment you want to make.  jBPM is the best in terms of features and flexibility, but OSWorkflow is a  more lightweight, easier to get up and running and has with a smaller learning curve.

Answer-253770, Question-65150
YAWL - Yet another workflow Language http://en.wikipedia.org/wiki/YAWL

Answer-1869322, Question-65150
Drools Flow is the best workflow solution that I came across recently. It has a luxury to be better than other solutions, since it is built and designed recently, and based on lessons learned from other long existing, somewhat over engineered frameworks. Drools Flow comes as a community project along with an official Drools 5 release that besides Flow includes: Guvnor, Expert and Fusion. Unfortunately Drools Flow does not have an official Red Hat support contract yet, and that is a stopper for some big corporations to consider it. One might think the support is not there for political reasons due to the jBPM project living under same support roof.

Answer-65575, Question-65310
This is what my code looks like. It seems to work fine. Are you using a service locator or just creating your service?

Answer-179414, Question-65310
Just a guess, but it looks like that error message is reporting that you've left the service name blank.  I imagine the code that generates that error message looks like this:

Answer-200824, Question-65310
I don't know what version of Axis you're using but I'm using Axis2 for both server and client and the Java2WSDL create a default endpoint for the service on localhost. If you create the client stub with WSDL2Java, the default constructor of the stub will then point to localhost. If the service is on other endpoint you must use the constructor with the endpoint as parameter... Maybe the problem is not that at all but as said on other answers, without the WSDL you're using as WSDL2Java input it's hard to say.

Answer-6496124, Question-65310
It is an exception used by Axis' control flow.  http://wiki.apache.org/ws/FrontPage/Axis/DealingWithCommonExceptions --> org.apache.axis.ConfigurationException: No service named XXX is available

Answer-17981199, Question-65310
According to the documentation linked to by @arnonym, this exception is somewhat misleading. In the first attempt to find the service a ConfigurationException is thrown and caught. It is logged at DEBUG level by the ConfigurationException class. Then another attempt is made using a different method to find the service that may then succeed. The workaround for this is to just change the log level on the ConfigurationException class to INFO in your log4j.properties:

Answer-66697, Question-66528
The Hibernate annotations docs (http://www.hibernate.org/hib_docs/annotations/reference/en/html_single/) suggest that this should be a class-level annotation rather than occurring inline within your code.  And indeed when I paste that code into my IDE and move it around, the compile errors are present when the annotation is inline, but vanish when I put it in above the class declaration: ...obviously I have no evidence that the above code will actually work.  I have only verified that it doesn't cause compile errors.

Answer-66825, Question-66528
Your example comes straight out of the API docs which are unfortunately poorly presented. Your annotation should be placed on some class, probably the one in which you will be creating the query to use the result set mapping. However, it actually doesn't matter where this annotation is placed. Your JPA provider will actually maintain a global list of all these mappings, so no matter where you define it, you will be able to use it anywhere. This is a shortcoming of the annotation method (as opposed to specifying things in XML.) Many other things in the JPA (i.e. named queries) are defined this same way. It makes it seem like there's some kind of connection between the thing being defined and the class on which it is annotated, when it's not.

Answer-66771, Question-66750
You may not want to change the capitalization -- different cultures capitalize different words (for example, in German you capitalize every noun, not just proper nouns).

Answer-66784, Question-66750
Not all languages share english capitalization rules. I guess you'd need to alter the data used by the API, but your non-english clients might not appreciate it... about.com on french capitalization

Answer-66794, Question-66750
Capitalisation rules are different for different languages.  In French, month names should not be capitalised.

Answer-15253817, Question-66750
I'm having a problem now where a sentence begins with "dimanche 07 mars", which wouldn't matter if it were not at the beginning of a sentence. I guess this cannot be changed, unless I do manual string manipulation on the first character of the string.

Answer-45249515, Question-66750
tl;dr lundi Localize As others stated, you should not be forcing your own parochial (US English?) notions of capitalization. Use a decent date-time library, and let it automatically localize for you. java.time You are using terrible old date-time classes that are now legacy, supplanted by the java.time classes. The LocalDate class represents a date-only value without time-of-day and without time zone. A time zone is crucial in determining a date. For any given moment, the date varies around the globe by zone. For example, a few minutes after midnight in Paris France is a new day while still yesterday in Montral Qubec. Specify a proper time zone name in the format of continent/region, such as America/Montreal, Africa/Casablanca, or Pacific/Auckland. Never use the 3-4 letter abbreviation such as EST or IST as they are not true time zones, not standardized, and not even unique(!).  To work with a month, extract a Month enum object. Ditto for day-of-week, DayOfWeek.  Call getDisplayName. Pass a TextStyle for abbreviation. Pass a Locale to determine (a) the human language for translation of name of day, name of month, and such, and (b) the cultural norms deciding issues of abbreviation, capitalization, punctuation, separators, and such. Note that Locale has nothing to do with time zone. and About java.time The java.time framework is built into Java 8 and later. These classes supplant the troublesome old legacy date-time classes such as java.util.Date, Calendar, & SimpleDateFormat. The Joda-Time project, now in maintenance mode, advises migration to the java.time classes. To learn more, see the Oracle Tutorial. And search Stack Overflow for many examples and explanations. Specification is JSR 310. Where to obtain the java.time classes?  Java SE 8, Java SE 9, and later Built-in.  Part of the standard Java API with a bundled implementation. Java 9 adds some minor features and fixes. Java SE 6 and Java SE 7 Much of the java.time functionality is back-ported to Java 6 & 7 in ThreeTen-Backport. Android The ThreeTenABP project adapts ThreeTen-Backport (mentioned above) for Android specifically. See How to use ThreeTenABP. The ThreeTen-Extra project extends java.time with additional classes. This project is a proving ground for possible future additions to java.time. You may find some useful classes here such as Interval, YearWeek, YearQuarter, and more.

Answer-66792, Question-66767
Is there a valid <servlet-mapping> for 'MyServlet' in your web.xml?  That's been my number one culprit in the past

Answer-66821, Question-66767
A 404 error means that the requested resource was not found.  As pkaeding suggests, it is probably due to the servlet mapping not being correct (or not being present) in the web.xml file.  Servlets must be specified in the web.xml file, and not only that, but they must be mapped to particular paths (an "url-mapping").  If the "MyServlet" servlet exists, but is not mapped to a path that may resolve with "/MyServlet/MyServlet" based on the application context root, and nothing else (i.e. another servlet, etc) resolves with this path, the application server will throw a 404 stating that nothing is mapped to the given path.

Answer-196427, Question-66767
I just spent about an hour pulling my hair out on this very problem. Tomcat 5.5.27 on OSX was working just fine until I'd added another servlet and servlet-mapping at which point everything was returning a 404. I hadn't realized it, but when I'd added a new servlet/servlet-mapping pair I'd put the servlet-mapping before the servlet entry. It's an easy mistake to make and, although knee-capping the entire application without giving anything resembling a sensible error message seems a little extreme, it makes perfect sense in retrospect.

Answer-1261685, Question-66767
Servlet mapping is a common problem.  But if you have any fitlers in your web.xml those can be the culprit as well.  One thing to realize is filters always execute the code before the doFilter before any servlets starts executing.  (Technically filters execute code after the doFilter)  In our code we created filters that would return 404 under certain situations.  Sometimes removing some or all filters-mapping will help discover if it is related to filter-mappings.

Answer-66943, Question-66840
I would try using DWR to integrate JavaScript with your Java app. It makes Java to JavaScript communication transparent and only requires one servlet + configuration of what to expose. I havent done this with JMS, but it should work the same. There are three technologies that together solve all my integration problems, Spring, Mule, and DWR.

Answer-67047, Question-66840
You'll find some references to the Dojo/Bayeux approach here http://www.pathf.com/blogs/2006/08/bayeux_a_json_p/ If you're using WebSphere 6.0 or higher then the Web 2.0 Feature Pack includes an implementation.

Answer-68981, Question-66840
I think this is your answer.  Looks like it is baked in to ActieMQ.  I tried the examples and they seem to work. http://activemq.apache.org/ajax.html

Answer-101508, Question-66840
The Seam framework supports subscription to JMS topics from a JavaScript based client: http://docs.jboss.com/seam/2.0.2.GA/reference/en-US/html/remoting.html#d0e14169

Answer-66942, Question-66875
Unfortunately, different versions of the Java Plug-In have different caching behaviors. Setting your Cache-Control and Last-Modified HTTP headers is the ideal solution, but it only works under the most recent versions of the JRE.  The only solution GUARANTEED to work is to rename your application jars when their versions change (we've seen strange caching behavior when trying other tricks like adding query strings based on file dates). This isn't so difficult to do if you have a properly automated deployment system.

Answer-9582242, Question-66875
As per this link  , same jar file should not be listed int "archive" and "cache_archive" params. In that case, the JAR file is cached using the native browser cache.

Answer-15113420, Question-66875
You can remove applet from Java cache using Java Control Panel. For example, on Win XP

Answer-67870, Question-67183
The document in question discusses the Solaris threading model and how the VM maps to it. This has nothing to do with Linux. Also, the document discusses performance only. The program's overall behaviour should not change no matter what you choose. Java's exposed threading model is the same on every platform and defined in the Java specifications. To a Java application, the underlying OS should be completely transparent even for threading. If you have to know, though ... The Sun JVM maps its threads 1:1 to Windows threads. It doesn't use multiple processes or fibers.

Answer-67909, Question-67183
That document is a little more about Solaris threading than the Java threading model. All JVMs call the native thread API of the OS they're written for so there is always one Java thread for an OS thread. The diagram in the document shows that it's not until the threads are in the OS space that they change. Each OS can handle threads in different ways for Windows specific documentation here is a good place to start: MSDN About Processes and Threads. For a long time various flavours of *nix have implemented their threads with processes rather than actual threads it seems that those specific tuning parameters where there to sort of ease the transition to a newer threading model in Solaris. Which made the older model and those JVM options obsolete. For a list of JVM options for the HotSpot JVM you can look at: HotSpot VM Options. A lot of these are useful for tuning long running applications but you can also get into trouble with them if you don't understand what they're doing. Also keep in mind that each implementation of the JVM can have a different set of options you won't find some of them on IBM's VM or BEA's.

Answer-68903, Question-67183
It really depends on the specific JVM implementation. I assume you're wondering about Sun's Windows JVM, and I can tell you with certainty that the Sun JVM maps a Java thread to an OS thread. You could try spawning up a couple of threads from Java code, open up Task Manager and see what happened.

Answer-255653, Question-67183
To answer you question most directly, precise semantics on how threads are implemented are deliberately left undefined by the JVM specification. FWIW, Sebastion's statement that "Java's exposed threading model is the same on every platform and defined in the Java specifications. To a Java application, the underlying OS should be completely transparent even for threading", is inaccurate. I have found significant empirical differences between threading under Windows and Linux regarding thread starvation using wait/notify.  Linux is significantly more prone to starvation when the many threads contend for a single lock - to the extent that I had to load up 3 orders of magnitude more threads in Windows to cause starvation than in Linux.  For heavily contended locks the Java concurrency locks with a fair modifier become critical. To illustrate numbers, I had problems under Linux with one lock heavily contended by 31 threads, where the same code under Windows required 10,000 (yes, that's 10 thousand) threads to begin to demonstrate starvation problems. To make matters worse, there have been 3 different threading models under Linux, each of which have different characteristics. Mostly, threading has been transparent in my experience, but issues of contention deserve careful consideration.

Answer-67367, Question-67275
Check if the input stream is positioned in the begging. Otherwise, as implementation: I do not think that you need to write to the result stream while you are reading, unless you process this exact stream in another thread. Just create a byte array, read the input stream, then create the output stream.

Answer-67377, Question-67275
It is unclear how you got the zipStream. It should work when you get it like this:

Answer-67403, Question-67275
I'd use IOUtils from the commons io project.

Answer-67719, Question-67275
You probably tried reading from a FileInputStream like this: This wont work since a zip archive can contain multiple files and you need to specify which file to read. You could use java.util.zip.ZipFile and a library such as IOUtils from Apache Commons IO or ByteStreams from Guava that assist you in copying the stream. Example:

Answer-67765, Question-67275
t is unclear how you got the zipStream. It should work when you get it like this: If you are obtaining the ZipInputStream from a ZipFile you can get one stream for the 3d party library, let it use it, and you obtain another input stream using the code before. Remember, an inputstream is a cursor. If you have the entire data (like a ZipFile) you can ask for N cursors over it. A diferent case is if you only have an "GZip" inputstream, only an zipped byte stream. In that case you ByteArrayOutputStream buffer makes all sense.

Answer-68187, Question-67275
I would call getNextEntry() on the ZipInputStream until it is at the entry you want (use ZipEntry.getName() etc.).  Calling getNextEntry() will advance the "cursor" to the beginning of the entry that it returns.  Then, use ZipEntry.getSize() to determine how many bytes you should read using zipInputStream.read().

Answer-69177, Question-67275
You could implement your own wrapper around the ZipInputStream that ignores close() and hand that off to the third-party library.

Answer-69489, Question-67275
Your loop looks valid - what does the following code (just on it's own) return? if it's returning -1, then the zipStream is closed before you get it, and all bets are off.  It's time to use your debugger and make sure what's being passed to you is actually valid. When you call getNextEntry(), does it return a value, and is the data in the entry meaningful (i.e. does getCompressedSize() return a valid value)?  IF you are just reading a Zip file that doesn't have read-ahead zip entries embedded, then ZipInputStream isn't going to work for you. Some useful tidbits about the Zip format: Each file embedded in a zip file has a header.  This header can contain useful information (such as the compressed length of the stream, it's offset in the file, CRC) - or it can contain some magic values that basically say 'The information isn't in the stream header, you have to check the Zip post-amble'. Each zip file then has a table that is attached to the end of the file that contains all of the zip entries, along with the real data.  The table at the end is mandatory, and the values in it must be correct.  In contrast, the values embedded in the stream do not have to be provided. If you use ZipFile, it reads the table at the end of the zip.  If you use ZipInputStream, I suspect that getNextEntry() attempts to use the entries embedded in the stream.  If those values aren't specified, then ZipInputStream has no idea how long the stream might be.  The inflate algorithm is self terminating (you actually don't need to know the uncompressed length of the output stream in order to fully recover the output), but it's possible that the Java version of this reader doesn't handle this situation very well. I will say that it's fairly unusual to have a servlet returning a ZipInputStream (it's much more common to receive an inflatorInputStream if you are going to be receiving compressed content.

Answer-2093184, Question-67275
Please try code bellow

Answer-9999404, Question-67275
You're missing call ZipEntry entry = (ZipEntry) zipStream.getNextEntry();  to position the first byte decompressed of the first entry.

Answer-68124, Question-68109
I'm not aware of any virtual machine in widespread use that saves statistical usage data between program invocations -- but it certainly is an interesting possibility for future research. What you're seeing is almost certainly due to disk caching.

Answer-68131, Question-68109
Java JVM (actually might change from different implementations of the JVM) when first started out will interpret the byte code. Once it detects that the code will be running enough number of times JITs it to native machine language so it runs faster.

Answer-68199, Question-68109
Okay, I found where I read that.  This is all from "Learning Java" (O'Reilly 2005): The problem with a traditional JIT compilation is that optimizing code takes time. So a JIT compiler can produce decent results but may suffer a significant latency when the application starts up. This is generally not a problem for long-running server-side applications but is a serious problem for client-side software and applications run on smaller devices with limited capabilities. To address this, Sun's compiler technology, called HotSpot, uses a trick called adaptive compilation. If you look at what programs actually spend their time doing, it turns out that they spend almost all their time executing a relatively small part of the code again and again. The chunk of code that is executed repeatedly may be only a small fraction of the total program, but its behavior determines the program's overall performance. Adaptive compilation also allows the Java runtime to take advantage of new kinds of optimizations that simply can't be done in a statically compiled language, hence the claim that Java code can run faster than C/C++ in some cases. To take advantage of this fact, HotSpot starts out as a normal Java bytecode interpreter, but with a difference: it measures (profiles) the code as it is executing to see what parts are being executed repeatedly. Once it knows which parts of the code are crucial to performance, HotSpot compiles those sections into optimal native machine code. Since it compiles only a small portion of the program into machine code, it can afford to take the time necessary to optimize those portions. The rest of the program may not need to be compiled at alljust interpretedsaving memory and time. In fact, Sun's default Java VM can run in one of two modes: client and server, which tell it whether to emphasize quick startup time and memory conservation or flat out performance. A natural question to ask at this point is, Why throw away all this good profiling information each time an application shuts down? Well, Sun has partially broached this topic with the release of Java 5.0 through the use of shared, read-only classes that are stored persistently in an optimized form. This significantly reduces both the startup time and overhead of running many Java applications on a given machine. The technology for doing this is complex, but the idea is simple: optimize the parts of the program that need to go fast, and don't worry about the rest. I'm kind of wondering how far Sun has gotten with it since Java 5.0.

Answer-68602, Question-68109
I agree that it's likely the result of disk caching. FYI, the IBM Java 6 VM does contain an ahead-of-time compiler (AOT).  The code isn't quite as optimized as what the JIT would produce, but it is stored across VMs, I believe in some sort of persistent shared memory.  Its primary benefit is to improve startup performance.  The IBM VM by default JITs a method after it's been called 1000 times.  If it knows that a method is going to be called 1000 times just during the VM startup (think a commonly-used method like java.lang.String.equals(...) ), then it's beneficial for it to store that in the AOT cache so that it never has to waste time compiling at runtime.

Answer-69412, Question-68109
I agree that the performance difference seen by the poster is most likely caused by disk latency bringing the JRE into memory.  The Just In Time compiler (JIT) would not have an impact on performance of a little application. Java 1.6u10 (http://download.java.net/jdk6/) touches the runtime JARs in a background process (even if Java isn't running) to keep the data in the disk cache.  This significantly decreases startup times (Which is a huge benefit to desktop apps, but probably of marginal value to server side apps). On large, long running applications, the JIT makes a big difference over time - but the amount of time required for the JIT to accumulate sufficient statistics to kick in and optimize (5-10 seconds) is very, very short compared to the overall life of the application (most run for months and months).  While storing and restoring the JIT results is an interesting academic exercise, the practical improvement is not very large (Which is why the JIT team has been more focused on things like GC strategies for minimizing memory cache misses, etc...). The pre-compilation of the runtime classes does help desktop applications quite a bit (as does the aforementioned 6u10 disk cache pre-loading).

Answer-75199, Question-68109
You should describe how your Benchmark was done. Especially at which point you start to measure the time.  If you include the JVM startup time (which is useful for Benchmarking the User experience but not so useful to optimize Java code) then it might be a filesystem caching effect or it can be caused by a feature called "Java Class Data Sharing": For Sun: http://java.sun.com/j2se/1.5.0/docs/guide/vm/class-data-sharing.html This is an option where the JVM saves a prepared image of the runtime classes to a file, to allow quicker loading (and sharing) of those at the next start. You can control this with -Xshare:on or -Xshare:off with a Sun JVM. The default is -Xshare:auto which will load the shared classes image if present, and if not present it will write it at first startup if the directory is write able. With IBM Java 5 this is BTW even more powerful: http://www.ibm.com/developerworks/java/library/j-ibmjava4/ I don't know of any mainstream JVM which is saving JIT statistics.