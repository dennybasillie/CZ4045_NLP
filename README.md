# CZ4045 NLP: Online Forum Data Processing

## 0. Project Structure
```
├── Readme.txt
├── Report.PDF
└── SourceCode
    ├── annotated-dataset - contains the ground truth annotated posts.
    ├── annotation-util - contains the code to automate the annotation process of non-irregular tokens.
    ├── application - contains the negation detection application + script for extracting dependency structure
    ├── crf_tokenizer - contains the code to generate CRF models for the CRF-based tokenizer
    ├── dataset - contains the code to extract the 500 threads from the original Stack Exchange Data Dump XML file, along with the dataset it retrieves, code to visualize the dataset statistics as pie charts.
    ├── further_analysis - contains the code to complete ‘Further Analysis’ part, which are to get the top 20 most frequent irregular sentences from the tokenized dataset and do POS-tagging on 10 randomly selected sentences which contain irregular tokens.
    ├── pos-tagging - contains the code to do the initial POS-tagging on 10 randomly selected sentences from the dataset.
    ├── preprocessing - contains the code to clean HTML tags and code segments from the extracted dataset.
    ├── randomizer - contains the code to retrieve 100 random posts from the cleaned dataset.
    ├── requirements.txt - contains the third-party library dependencies of the project, along with their version numbers.
    ├── stanford-ner-java - contains the jar files of Stanford NER package. Initially empty because including third-party libraries in the submission is not allowed.
    ├── stemming - contains the code to do the initial stemming of the dataset and print out the top 20 most frequent stemmed words.
    ├── tokenizer - contains the code for the regex-based tokenizer.
    ├── validation - contains the code to validate the result generated by the regex-based tokenizer with the ground truth annotated posts.
```

All the paths used in the following steps are relative to SourceCode/ directory. The directory should be referred as project folder or root project. All terminal command run at root project by default.

## 1. Requirements
This project requires to be executed in Linux (tested in Ubuntu 16.04) or MacOS because of the existence of some bash file. Windows is not preferred since it does not have the package to run bash script. However, it is possible to run bash file by using CygWin or using git bash terminal.
CygWin: https://www.cygwin.com/
Git: https://git-scm.com/

This project use 2 programming languages:
a. Python 2.7
b. Java 8 

## 2. Third-Party Library/Package Installation
List of python packages can be seen in the requirements.txt file. To install all python packages, run the following command in terminal. 
```
pip install -r requirements.txt 
```
Stanford NER java library may be downloaded from https://nlp.stanford.edu/software/CRF-NER.shtml.

Unzip the file and copy stanford-ner.jar to the stanford-ner-java folder inside project folder.

## 3. Dataset Download
The 500 threads used as the dataset for this project can be found in these 2 links:
https://github.com/michjk/CZ4045_NLP/blob/master/preprocessing/questions-cleaned.txt
https://github.com/michjk/CZ4045_NLP/blob/master/preprocessing/answers-cleaned.txt

The first link redirects to the 500 question posts, while the second link redirects to the answers for the 500 question posts. All the posts have been cleaned of every HTML tags and code segments.


The annotated posts used as the ground truth dataset can be found in this link: https://github.com/michjk/CZ4045_NLP/tree/master/annotated_dataset

In the directory, 3 types of files can be found; .txt, .ann, and .conll. The .txt files contain the original posts. The .ann files are generated from brat annotation tool. They contain the annotation results. The .conll files are the modified .ann files used to train the CRF-based tokenizer. All these 3 types of files are segmented into 4 files (Post 1 until 4).

## 4. Data Collection

The entire stackoverflow-posts data dump is first downloaded from StackExchange. Unzipping the data dump results in a posts.xml file. At this point, run the extractor.py script through terminal and pass in three arguments as follows: 
```
cd dataset
python extractor.py posts.xml questions.txt answers.txt
```
The second and third arguments represent the names of the files to which the question posts and answer posts should be extracted.

500 forum threads related to Java are now available in the two files passed in as arguments earlier on. They should reside in the same directory as the script.

The countTokens.py script is used for obtaining statistics about the distribution of posts in our dataset.


## 5. Stemming
Inside the project directory, go to the ‘stemming’ directory and run stemming.py.
```
python stemming.py
```
The script goes through the cleaned question and answer posts, use NLTK Porter Stemmer to stem each word and keep track of the count of every stemmed words, excluding any stop words. Then it prints out the top 20 most frequent stemmed words met in the dataset.

## 6. POS-tagging
Inside the project directory, go to the ‘pos-tagging’ directory and run main.py.
```
python main.py
```
This will fetch the preprocessed posts and randomly select 10 sentences to do POS tagging on. POS-tagging is done on the sentences with NLTK POS Tagger and an output file ‘postagged-sents.txt’ is generated containing the POS-tagging result.


## 7. Annotation
Brat is used to annotate 100 posts as ground truth for tokenizer. To install brat, clone the repo by running the following command in terminal:
```
git clone https://github.com/nlplab/brat 
```
After that, go inside brat folder and run following command in terminal:
```
./install.sh
```
To generate 100 posts, a randomizer script is used. To run the scripts, run the following commands inside annotation_util folder:
```
python randomizer-posts.py
```
The randomizer ensure 50 questions and 50 answers are selected randomly. After that, 100 posts are divided into 4 manually. Each file contains 25 posts and the files must be named as Post-1.txt, Post-2.txt, Post-3.txt, and Post-4.txt. The example of a file that contains 2 posts
```
Post-1, Question-57145
This is just me wondering if there isn't a better way to do things.

Post-2, Question-25637
Is there a way to shutdown a computer using a built-in Java method?
```
Next, Post-1 until Post-4 is copied to brat/data/ folder. The folder is used by Brat as directory for annotating dataset.

To run server, run the following commands inside brat folder:
```
python standalone.py
```
Brat is run in localhost:8001. Go to the url and open the post files to start annotating (label Token and label Irregular). Brat automatically create .ann file with same names as annotated txt file (Post-1.ann, etc) that contains list of tokens and their label. The example:
```
T9	Token 67 73	myself
T10	Token 74 82	creating
T11	Irregular 83 97	instance-level
T12	Token 98 109	collections
```
After annotating, run anntoconll.py inside brat/tools/ to generate conll file from ann file. The example command is:
```
python anntoconll.py ../data/Post-1.txt
```
Run the commands also for Post-2 until Post-4.
The conll file contains BIO labelling for the posts. B token means beginning token. I token means next token that follow I token or B token. O token means other words. In this experiment, O token is not used and will be erased in later part. B-Token and I-Token will also be replaced by Token label. The example of conll files:
```
I-Token	67	73	myself
I-Token	74	82	creating
B-Irregular	83	91	instance
I-Irregular	91	92	-
I-Irregular	92	97	level
B-Token	98	109	collections
```
For more information, read: http://brat.nlplab.org


## 8. Tokenizer
### a. Regex Tokenizer
To run the regex tokenizer, from the project directory, go into the ‘tokenizer’ directory and run ‘tokenizer_re.py’ along with a .txt file name (without extension) which contains text to be tokenized.
```
python tokenizer_re.py Post-1
```
The script will load the Post-1.txt and go through each line. With the definitions of tokens as its baseline, the script will tokenize the text accordingly and generate a .ann file containing the tokenization result.

### b. CRF Tokenizer
Before training CRF, 4 fold train and test data set need to be generated. Run the command inside crf_tokenizer to generate the set:
```
./k_fold_preprocessor.sh
```
The bash script run python program to generate train-1 to train-4 and test-1 to test-4. The files contain 2 columns: word token and its label. The example of the files:
```
I	B-Irregular
'	I-Irregular
ve	I-Irregular
recently	Token
inherited	Token
a	Token
internationalized	 Token
```
To start training, run the command inside crf_tokenizer:
```
./train.sh
```
The bash script run Stanford NER java for training CRF model. The training generate serialized model tokenizer-1 to tokenizer-4. To do testing, run the command inside crf_tokenizer:
```
./test.sh
```
The bash script run Stanford command for testing. It generates scores-1 to scores-4 that contains Precision, Recall, and F1 scores for each fold. The example of scores file:
```
Invoked on Thu Nov 02 04:49:12 SGT 2017 with arguments: -loadClassifier tokenizer-1.ser.gz -testFile test-1.txt
testFile=test-1.txt
loadClassifier=tokenizer-1.ser.gz
Loading classifier from tokenizer-1.ser.gz ... done [0.2 sec].
CRFClassifier tagged 3131 words in 1 documents at 8069.59 words per second.
         Entity	P	R	F1	TP	FP	FN
      Irregular	0.7200	0.7100	0.7150	306	119	125
          Token	0.6942	0.6976	0.6959	286	126	124
         Totals	0.7073	0.7039	0.7056	592	245	249
```
## 9. Further Analysis
### a. Irregular tokens
In this part, the top 20 most frequent irregular tokens, or in this case, words which are non-standard English words, from the dataset shall be found. 

From the home project directory, go into ‘further-analysis’ directory. Run irregular_analyzer.py along with the .ann file path which contains tokenized form of the dataset.
```
python irregular_pos_tag.py Post-1.ann
```
The script irregular_tokenizer.py will scan through even token in Post-1.ann. The count value of every irregular token is recorded. Finally, print out the top 20 most frequent irregular tokens found in the .ann file.


### b. POS Tagging
In this part, 10 more sentences which contain irregular tokens shall be selected and POS-tagged. Another script is written for this.

From the home project directory, go into ‘further-analysis’ directory. Run irregular_pos_tag.py along with the fileName which contains the dataset.
```
python irregular_pos_tag.py preprocessed_dataset/posts-cleaned.txt
```
The script irregular_pos_tag.py will look for irregular tokens in randomly selected sentences from the dataset, until there are 10 sentences. Then, the sentences will be tokenized and POS-tagged. An output file named ‘irregular_pos_tag_result.txt’ which contains the POS-tagged 10 sentences shall be created.

## 10. Application (Negation Expression Detector)

The application is in the form of a script, ‘negation.py’. It is able to accept a single sentence or a file containing multiple sentences as input and extract all sentences containing negation expressions. These sentences are then output to an external file for ease of subsequent retrieval and further processing. 

There are two main functions in the application: ‘negationSingle’ and ‘negationBatch’. As their names suggest, the former accepts one sentence as input, and the latter processes a file. The ‘negationSingle’ function is usable in the Python IDLE shell environment. By importing the ‘negation’ module and passing a sentence to the ‘negationSingle’ function, a user would be informed whether the sentence contains any negation expression. 

The example below utilises the function in a Python session:
```
>>> from negation import negationSingle
>>> sentence = “I have never received an A grade in my course so far.”
>>> negationSingle(sentence)
The sentence contains negation expression.
It has been written to 'negativeExpSent.txt' in the same folder.
```

The ‘negationBatch’ function performs in a similar fashion, but instead of informing whether every sentence in a file has negation expression, it displays the total number of sentences containing negation expression. This function could be called directly and passed a file for processing in a Python interactive session, just like in the example above, or be accessed through executing the application script file in a command line interface. 

To do the latter, perform the following steps in terminal (or bash):
```
cd application
python negation.py test.txt
```

